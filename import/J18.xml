<volume id='J18'>

  <paper id='1000'>
    <title>Computational Linguistics, Volume 44, Issue 1 - April 2018</title>
    <month>March</month>
    <year>2018</year>
  </paper>

  <paper id="1001">
    <title>Smart Enough to Talk With Us? Foundations and Challenges for Dialogue Capable AI Systems</title>
    <author><first>Barbara J.</first><last>Grosz</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>1–15</pages>
    <doi>10.1162/COLI_a_00313</doi>
    <url>http://www.aclweb.org/anthology/J18-1001</url>
  </paper>

  <paper id="1002">
    <title>On the Derivational Entropy of Left-to-Right Probabilistic Finite-State Automata and Hidden Markov Models</title>
    <author><first>Joan Andreu</first><last>Sánchez</last></author>
    <author><first>Martha Alicia</first><last>Rocha</last></author>
    <author><first>Verónica</first><last>Romero</last></author>
    <author><first>Mauricio</first><last>Villegas</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>Probabilistic finite-state automata are a formalism that is widely used in many problems of automatic speech recognition and natural language processing. Probabilistic finite-state automata are closely related to other finite-state models as weighted finite-state automata, word lattices, and hidden Markov models. Therefore, they share many similar properties and problems. Entropy measures of finite-state models have been investigated in the past in order to study the information capacity of these models. The derivational entropy quantifies the uncertainty that the model has about the probability distribution it represents. The derivational entropy in a finite- state automaton is computed from the probability that is accumulated in all of its individual state sequences. The computation of the entropy from a weighted finite-state automaton requires a normalized model. This article studies an efficient computation of the derivational entropy of left-to-right probabilistic finite-state automata, and it introduces an efficient algorithm for normalizing weighted finite-state automata. The efficient computation of the derivational entropy is also extended to continuous hidden Markov models.</abstract>
    <pages>17–37</pages>
    <doi>10.1162/COLI_a_00306</doi>
    <url>http://www.aclweb.org/anthology/J18-1002</url>
  </paper>

  <paper id="1003">
    <title>A Notion of Semantic Coherence for Underspecified Semantic Representation</title>
    <author><first>Mehdi</first><last>Manshadi</last></author>
    <author><first>Daniel</first><last>Gildea</last></author>
    <author><first>James F.</first><last>Allen</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>The general problem of finding satisfying solutions to constraint-based underspecified representations of quantifier scope is NP-complete. Existing frameworks, including Dominance Graphs, Minimal Recursion Semantics, and Hole Semantics, have struggled to balance expressivity and tractability in order to cover real natural language sentences with efficient algorithms. We address this trade-off with a general principle of coherence, which requires that every variable introduced in the domain of discourse must contribute to the overall semantics of the sentence. We show that every underspecified representation meeting this criterion can be efficiently processed, and that our set of representations subsumes all previously identified tractable sets.</abstract>
    <pages>39–83</pages>
    <doi>10.1162/COLI_a_00307</doi>
    <url>http://www.aclweb.org/anthology/J18-1003</url>
  </paper>

  <paper id="1004">
    <title>Cache Transition Systems for Graph Parsing</title>
    <author><first>Daniel</first><last>Gildea</last></author>
    <author><first>Giorgio</first><last>Satta</last></author>
    <author><first>Xiaochang</first><last>Peng</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>Motivated by the task of semantic parsing, we describe a transition system that generalizes standard transition-based dependency parsing techniques to generate a graph rather than a tree. Our system includes a cache with fixed size m, and we characterize the relationship between the parameter m and the class of graphs that can be produced through the graph-theoretic concept of tree decomposition. We find empirically that small cache sizes cover a high percentage of sentences in existing semantic corpora.</abstract>
    <pages>85-118</pages>
    <doi>10.1162/COLI_a_00308</doi>
    <url>http://www.aclweb.org/anthology/J18-1004</url>
  </paper>

  <paper id="1005">
    <title>Weighted DAG Automata for Semantic Graphs</title>
    <author><first>David</first><last>Chiang</last></author>
    <author><first>Frank</first><last>Drewes</last></author>
    <author><first>Daniel</first><last>Gildea</last></author>
    <author><first>Adam</first><last>Lopez</last></author>
    <author><first>Giorgio</first><last>Satta</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>Graphs have a variety of uses in natural language processing, particularly as representations of linguistic meaning. A deficit in this area of research is a formal framework for creating, combining, and using models involving graphs that parallels the frameworks of finite automata for strings and finite tree automata for trees. A possible starting point for such a framework is the formalism of directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki and extended by Quernheim and Knight. In this article, we study the latter in depth, demonstrating several new results, including a practical recognition algorithm that can be used for inference and learning with models defined on DAG automata. We also propose an extension to graphs with unbounded node degree and show that our results carry over to the extended formalism.</abstract>
    <pages>119-186</pages>
    <doi>10.1162/COLI_a_00309</doi>
    <url>http://www.aclweb.org/anthology/J18-1005</url>
  </paper>

  <paper id="1006">
    <title>Book Review: Bayesian Analysis in Natural Language Processing by Shay Cohen</title>
    <author><first>Kevin</first><last>Duh</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>187-189</pages>
    <doi>10.1162/COLI_r_00310</doi>
    <url>http://www.aclweb.org/anthology/J18-1006</url>
  </paper>

  <paper id="1007">
    <title>Metaphor: A Computational Perspective by Tony Veale, Ekaterina Shutova and Beata Beigman Klebanov</title>
    <author><first>Carlo</first><last>Strapparava</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>191–192</pages>
    <doi>10.1162/COLI_r_00311</doi>
    <url>http://www.aclweb.org/anthology/J18-1007</url>
  </paper>

  <paper id="1008">
    <title>Neural Network Methods for Natural Language Processing by Yoav Goldberg</title>
    <author><first>Yang</first><last>Liu</last></author>
    <author><first>Meng</first><last>Zhang</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>193–195</pages>
    <doi>10.1162/</doi>
    <url>http://www.aclweb.org/anthology/J18-100</url>
  </paper>

</volume>
