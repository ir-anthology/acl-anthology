<volume id='J18'>

  <paper id='1000'>
    <title>Computational Linguistics, Volume 44, Issue 1 - April 2018</title>
    <month>March</month>
    <year>2018</year>
  </paper>

  <paper id="1001">
    <title>Smart Enough to Talk With Us? Foundations and Challenges for Dialogue Capable AI Systems</title>
    <author><first>Barbara J.</first><last>Grosz</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>1–15</pages>
    <doi>10.1162/COLI_a_00313</doi>
    <url>http://www.aclweb.org/anthology/J18-1001</url>
  </paper>

  <paper id="1002">
    <title>On the Derivational Entropy of Left-to-Right Probabilistic Finite-State Automata and Hidden Markov Models</title>
    <author><first>Joan Andreu</first><last>Sánchez</last></author>
    <author><first>Martha Alicia</first><last>Rocha</last></author>
    <author><first>Verónica</first><last>Romero</last></author>
    <author><first>Mauricio</first><last>Villegas</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>Probabilistic finite-state automata are a formalism that is widely used in many problems of automatic speech recognition and natural language processing. Probabilistic finite-state automata are closely related to other finite-state models as weighted finite-state automata, word lattices, and hidden Markov models. Therefore, they share many similar properties and problems. Entropy measures of finite-state models have been investigated in the past in order to study the information capacity of these models. The derivational entropy quantifies the uncertainty that the model has about the probability distribution it represents. The derivational entropy in a finite- state automaton is computed from the probability that is accumulated in all of its individual state sequences. The computation of the entropy from a weighted finite-state automaton requires a normalized model. This article studies an efficient computation of the derivational entropy of left-to-right probabilistic finite-state automata, and it introduces an efficient algorithm for normalizing weighted finite-state automata. The efficient computation of the derivational entropy is also extended to continuous hidden Markov models.</abstract>
    <pages>17–37</pages>
    <doi>10.1162/COLI_a_00306</doi>
    <url>http://www.aclweb.org/anthology/J18-1002</url>
  </paper>

  <paper id="1003">
    <title>A Notion of Semantic Coherence for Underspecified Semantic Representation</title>
    <author><first>Mehdi</first><last>Manshadi</last></author>
    <author><first>Daniel</first><last>Gildea</last></author>
    <author><first>James F.</first><last>Allen</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>The general problem of finding satisfying solutions to constraint-based underspecified representations of quantifier scope is NP-complete. Existing frameworks, including Dominance Graphs, Minimal Recursion Semantics, and Hole Semantics, have struggled to balance expressivity and tractability in order to cover real natural language sentences with efficient algorithms. We address this trade-off with a general principle of coherence, which requires that every variable introduced in the domain of discourse must contribute to the overall semantics of the sentence. We show that every underspecified representation meeting this criterion can be efficiently processed, and that our set of representations subsumes all previously identified tractable sets.</abstract>
    <pages>39–83</pages>
    <doi>10.1162/COLI_a_00307</doi>
    <url>http://www.aclweb.org/anthology/J18-1003</url>
  </paper>

  <paper id="1004">
    <title>Cache Transition Systems for Graph Parsing</title>
    <author><first>Daniel</first><last>Gildea</last></author>
    <author><first>Giorgio</first><last>Satta</last></author>
    <author><first>Xiaochang</first><last>Peng</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>Motivated by the task of semantic parsing, we describe a transition system that generalizes standard transition-based dependency parsing techniques to generate a graph rather than a tree. Our system includes a cache with fixed size m, and we characterize the relationship between the parameter m and the class of graphs that can be produced through the graph-theoretic concept of tree decomposition. We find empirically that small cache sizes cover a high percentage of sentences in existing semantic corpora.</abstract>
    <pages>85-118</pages>
    <doi>10.1162/COLI_a_00308</doi>
    <url>http://www.aclweb.org/anthology/J18-1004</url>
  </paper>

  <paper id="1005">
    <title>Weighted DAG Automata for Semantic Graphs</title>
    <author><first>David</first><last>Chiang</last></author>
    <author><first>Frank</first><last>Drewes</last></author>
    <author><first>Daniel</first><last>Gildea</last></author>
    <author><first>Adam</first><last>Lopez</last></author>
    <author><first>Giorgio</first><last>Satta</last></author>
    <month>March</month>
    <year>2018</year>
    <abstract>Graphs have a variety of uses in natural language processing, particularly as representations of linguistic meaning. A deficit in this area of research is a formal framework for creating, combining, and using models involving graphs that parallels the frameworks of finite automata for strings and finite tree automata for trees. A possible starting point for such a framework is the formalism of directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki and extended by Quernheim and Knight. In this article, we study the latter in depth, demonstrating several new results, including a practical recognition algorithm that can be used for inference and learning with models defined on DAG automata. We also propose an extension to graphs with unbounded node degree and show that our results carry over to the extended formalism.</abstract>
    <pages>119-186</pages>
    <doi>10.1162/COLI_a_00309</doi>
    <url>http://www.aclweb.org/anthology/J18-1005</url>
  </paper>

  <paper id="1006">
    <title>Book Review: Bayesian Analysis in Natural Language Processing by Shay Cohen</title>
    <author><first>Kevin</first><last>Duh</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>187-189</pages>
    <doi>10.1162/COLI_r_00310</doi>
    <url>http://www.aclweb.org/anthology/J18-1006</url>
  </paper>

  <paper id="1007">
    <title>Metaphor: A Computational Perspective by Tony Veale, Ekaterina Shutova and Beata Beigman Klebanov</title>
    <author><first>Carlo</first><last>Strapparava</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>191–192</pages>
    <doi>10.1162/COLI_r_00311</doi>
    <url>http://www.aclweb.org/anthology/J18-1007</url>
  </paper>

  <paper id="1008">
    <title>Neural Network Methods for Natural Language Processing by Yoav Goldberg</title>
    <author><first>Yang</first><last>Liu</last></author>
    <author><first>Meng</first><last>Zhang</last></author>
    <month>March</month>
    <year>2018</year>
    <pages>193–195</pages>
    <doi>10.1162/</doi>
    <url>http://www.aclweb.org/anthology/J18-100</url>
  </paper>

  <paper id='2000'>
    <title>Computational Linguistics, Volume 44, Issue 2 - June 2018</title>
    <month>June</month>
    <year>2018</year>
  </paper>

  <paper id="2001">
    <title>A Dependency Perspective on RST Discourse Parsing and Evaluation</title>
    <author><first>Mathieu</first><last>Morey</last></author>
    <author><first>Philippe</first><last>Muller</last></author>
    <author><first>Nicholas</first><last>Asher</last></author>
    <month>June</month>
    <year>2018</year>
    <abstract>Computational text-level discourse analysis mostly happens within Rhetorical Structure Theory (RST), whose structures have classically been presented as constituency trees, and relies on data from the RST Discourse Treebank (RST-DT); as a result, the RST discourse parsing community has largely borrowed from the syntactic constituency parsing community. The standard evaluation procedure for RST discourse parsers is thus a simplified variant of PARSEVAL, and most RST discourse parsers use techniques that originated in syntactic constituency parsing. In this article, we isolate a number of conceptual and computational problems with the constituency hypothesis.  We then examine the consequences, for the implementation and evaluation of RST discourse parsers, of adopting a dependency perspective on RST structures, a view advocated so far only by a few approaches to discourse parsing. While doing that, we show the importance of the notion of headedness of RST structures. We analyze RST discourse parsing as dependency parsing by adapting to RST a recent proposal in syntactic parsing that relies on head-ordered dependency trees, a representation isomorphic to headed constituency trees. We show how to convert the original trees from the RST corpus, RST-DT, and their binarized versions used by all existing RST parsers to head-ordered dependency trees. We also propose a way to convert existing simple dependency parser output to constituent trees. This allows us to evaluate and to compare approaches from both constituent-based and dependency-based perspectives in a unified framework, using constituency and dependency metrics.  We thus propose an evaluation framework to compare extant approaches easily and uniformly, something the RST parsing community has lacked up to now. We can also compare parsers’ predictions to each other across frameworks. This allows us to characterize families of parsing strategies across the different frameworks, in particular with respect to the notion of headedness. Our experiments provide evidence for the conceptual similarities between dependency parsers and shift-reduce constituency parsers, and confirm that dependency parsing constitutes a viable approach to RST discourse parsing.</abstract>
    <pages>197–235</pages>
    <doi>10.1162/COLI_a_00314</doi>
    <url>http://www.aclweb.org/anthology/J18-2001</url>
  </paper>

  <paper id="2002">
    <title>Unrestricted Bridging Resolution</title>
    <author><first>Yufang</first><last>Hou</last></author>
    <author><first>Katja</first><last>Markert</last></author>
    <author><first>Michael</first><last>Strube</last></author>
    <month>June</month>
    <year>2018</year>
    <abstract>In contrast to identity anaphors, which indicate coreference between a noun phrase and its antecedent, bridging anaphors link to their antecedent(s) via lexico-semantic, frame, or encyclopedic relations. Bridging resolution involves recognizing bridging anaphors and finding links to antecedents. In contrast to most prior work, we tackle both problems. Our work also follows a more wide-ranging definition of bridging than most previous work and does not impose any restrictions on the type of bridging anaphora or relations between anaphor and antecedent.  We create a corpus (ISNotes) annotated for information status (IS), bridging being one of the IS subcategories. The annotations reach high reliability for all categories and marginal reliability for the bridging subcategory. We use a two-stage statistical global inference method for bridging resolution. Given all mentions in a document, the first stage, bridging anaphora recognition, recognizes bridging anaphors as a subtask of learning fine-grained IS. We use a cascading collective classification method where (i) collective classification allows us to investigate relations among several mentions and autocorrelation among IS classes and (ii) cascaded classification allows us to tackle class imbalance, important for minority classes such as bridging. We show that our method outperforms current methods both for IS recognition overall as well as for bridging, specifically. The second stage, bridging antecedent selection, finds the antecedents for all predicted bridging anaphors. We investigate the phenomenon of semantically or syntactically related bridging anaphors that share the same antecedent, a phenomenon we call sibling anaphors. We show that taking sibling anaphors into account in a joint inference model improves antecedent selection performance. In addition, we develop semantic and salience features for antecedent selection and suggest a novel method to build the candidate antecedent list for an anaphor, using the discourse scope of the anaphor. Our model outperforms previous work significantly.</abstract>
    <pages>237–284</pages>
    <doi>10.1162/COLI_a_00315</doi>
    <url>http://www.aclweb.org/anthology/J18-2002</url>
  </paper>

  <paper id="2003">
    <title>Spurious Ambiguity and Focalization</title>
    <author><first>Glyn</first><last>Morrill</last></author>
    <author><first>Oriol</first><last>Valentín</last></author>
    <month>June</month>
    <year>2018</year>
    <abstract>Spurious ambiguity is the phenomenon whereby distinct derivations in grammar may assign the same structural reading, resulting in redundancy in the parse search space and inefficiency in parsing. Understanding the problem depends on identifying the essential mathematical structure of derivations. This is trivial in the case of context free grammar, where the parse structures are ordered trees; in the case of type logical categorial grammar, the parse structures are proof nets. However, with respect to multiplicatives, intrinsic proof nets have not yet been given for displacement calculus, and proof nets for additives, which have applications to polymorphism, are not easy to characterize. In this context we approach here multiplicative-additive spurious ambiguity by means of the proof-theoretic technique of focalization.</abstract>
    <pages>285–327</pages>
    <doi>10.1162/COLI_a_00316</doi>
    <url>http://www.aclweb.org/anthology/J18-2003</url>
  </paper>

  <paper id="2004">
    <title>The Influence of Context on the Learning of Metrical Stress Systems Using Finite-State Machines</title>
    <author><first>Cesko</first><last>Voeten</last></author>
    <author><first>Menno</first><last>van Zaanen</last></author>
    <month>June</month>
    <year>2018</year>
    <abstract>Languages vary in the way stress is assigned to syllables within words. This article investigates the learnability of stress systems in a wide range of languages. The stress systems can be described using finite-state automata with symbols indicating levels of stress (primary, secondary, or no stress). Finite-state automata have been the focus of research in the area of grammatical inference for some time now. It has been shown that finite-state machines are learnable from examples using state-merging. One such approach, which aims to learn k-testable languages, has been applied to stress systems with some success. The family of k-testable languages has been shown to be efficiently learnable (in polynomial time). Here, we extend this approach to k, l-local languages by taking not only left context, but also right context, into account. We consider empirical results testing the performance of our learner using various amounts of context (corresponding to varying definitions of phonological locality). Our results show that our approach of learning stress patterns using state-merging is more reliant on left context than on right context. Additionally, some stress systems fail to be learned by our learner using either the left-context k-testable or the left-and-right-context k, l-local learning system. A more complex merging strategy, and hence grammar representation, is required for these stress systems.</abstract>
    <pages>329–348</pages>
    <doi>10.1162/COLI_a_00317</doi>
    <url>http://www.aclweb.org/anthology/J18-2004</url>
  </paper>

  <paper id="2005">
    <title>Tree Structured Dirichlet Processes for Hierarchical Morphological Segmentation</title>
    <author><first>Burcu</first><last>Can</last></author>
    <author><first>Suresh</first><last>Manandhar</last></author>
    <month>June</month>
    <year>2018</year>
    <abstract>This article presents a probabilistic hierarchical clustering model for morphological segmentation. In contrast to existing approaches to morphology learning, our method allows learning hierarchical organization of word morphology as a collection of tree structured paradigms. The model is fully unsupervised and based on the hierarchical Dirichlet process. Tree hierarchies are learned along with the corresponding morphological paradigms simultaneously. Our model is evaluated on Morpho Challenge and shows competitive performance when compared to state-of-the-art unsupervised morphological segmentation systems. Although we apply this model for morphological segmentation, the model itself can also be used for hierarchical clustering of other types of data.</abstract>
    <pages>349–374</pages>
    <doi>10.1162/COLI_a_00318</doi>
    <url>http://www.aclweb.org/anthology/J18-2005</url>
  </paper>

  <paper id="2006">
    <title>Domain-Sensitive Temporal Tagging By Jannik Strötgen, Michael Gertz</title>
    <author><first>Ruihong</first><last>Huang</last></author>
    <month>June</month>
    <year>2018</year>
    <pages>375–377</pages>
    <doi>10.1162/COLI_r_00319</doi>
    <url>http://www.aclweb.org/anthology/J18-2006</url>
  </paper>

  <paper id="2007">
    <title>Festina Lente: A Farewell from the Editor</title>
    <author><first>Paola</first><last>Merlo</last></author>
    <month>June</month>
    <year>2018</year>
    <pages>379-385</pages>
    <doi>10.1162/COLI_e_00320</doi>
    <url>http://www.aclweb.org/anthology/J18-2007</url>
  </paper>

</volume>
