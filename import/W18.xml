<?xml version='1.0' encoding='UTF-8'?>
<volume id='W18'>
  <paper id='0100'>
    <title>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </title>
    <editor>
      <first>Asad</first>
      <last>Sayeed</last>
    </editor>
    <editor>
      <first>Cassandra</first>
      <last>Jacobs</last>
    </editor>
    <editor>
      <first>Tal</first>
      <last>Linzen</last>
    </editor>
    <editor>
      <first>Marten</first>
      <last>van Schijndel</last>
    </editor>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-01</url>
    <doi>10.18653/v1/W18-01</doi>
    <bibtype>book</bibtype>
    <bibkey>CMCL:2018</bibkey>
  </paper>
  <paper id='0101'>
    <title>Coreference and Focus in Reading Times</title>
    <author>
      <first>Evan</first>
      <last>Jaffe</last>
    </author>
    <author>
      <first>Cory</first>
      <last>Shain</last>
    </author>
    <author>
      <first>William</first>
      <last>Schuler</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W18-0101</url>
    <doi>10.18653/v1/W18-0101</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jaffe-shain-schuler:2018:CMCL</bibkey>
  </paper>
  <paper id='0102'>
    <title>
      Predictive power of word surprisal for reading times is a linear function
      of language model quality
    </title>
    <author>
      <first>Adam</first>
      <last>Goodkind</last>
    </author>
    <author>
      <first>Klinton</first>
      <last>Bicknell</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–18</pages>
    <url>http://www.aclweb.org/anthology/W18-0102</url>
    <doi>10.18653/v1/W18-0102</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>goodkind-bicknell:2018:CMCL</bibkey>
  </paper>
  <paper id='0103'>
    <title>Dynamic encoding of structural uncertainty in gradient symbols</title>
    <author>
      <first>Pyeong Whan</first>
      <last>Cho</last>
    </author>
    <author>
      <first>Matthew</first>
      <last>Goldrick</last>
    </author>
    <author>
      <first>Richard L.</first>
      <last>Lewis</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Smolensky</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>19–28</pages>
    <url>http://www.aclweb.org/anthology/W18-0103</url>
    <doi>10.18653/v1/W18-0103</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>cho-EtAl:2018:CMCL</bibkey>
  </paper>
  <paper id='0104'>
    <title>Phonological (un)certainty weights lexical activation</title>
    <author>
      <first>Laura</first>
      <last>Gwilliams</last>
    </author>
    <author>
      <first>David</first>
      <last>Poeppel</last>
    </author>
    <author>
      <first>Alec</first>
      <last>Marantz</last>
    </author>
    <author>
      <first>Tal</first>
      <last>Linzen</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29–34</pages>
    <url>http://www.aclweb.org/anthology/W18-0104</url>
    <doi>10.18653/v1/W18-0104</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gwilliams-EtAl:2018:CMCL</bibkey>
  </paper>
  <paper id='0105'>
    <title>Predicting and Explaining Human Semantic Search in a Cognitive Model</title>
    <author>
      <first>Filip</first>
      <last>Miscevic</last>
    </author>
    <author>
      <first>Aida</first>
      <last>Nematzadeh</last>
    </author>
    <author>
      <first>Suzanne</first>
      <last>Stevenson</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–45</pages>
    <url>http://www.aclweb.org/anthology/W18-0105</url>
    <doi>10.18653/v1/W18-0105</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>miscevic-nematzadeh-stevenson:2018:CMCL</bibkey>
  </paper>
  <paper id='0106'>
    <title>Modeling bilingual word associations as connected monolingual networks</title>
    <author>
      <first>Yevgen</first>
      <last>Matusevych</last>
    </author>
    <author>
      <first>Amir Ardalan</first>
      <last>Kalantari Dehaghi</last>
    </author>
    <author>
      <first>Suzanne</first>
      <last>Stevenson</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–56</pages>
    <url>http://www.aclweb.org/anthology/W18-0106</url>
    <doi>10.18653/v1/W18-0106</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>matusevych-kalantaridehaghi-stevenson:2018:CMCL</bibkey>
  </paper>
  <paper id='0107'>
    <title>
      Experiential, Distributional and Dependency-based Word Embeddings have
      Complementary Roles in Decoding Brain Activity
    </title>
    <author>
      <first>Samira</first>
      <last>Abnar</last>
    </author>
    <author>
      <first>Rasyan</first>
      <last>Ahmed</last>
    </author>
    <author>
      <first>Max</first>
      <last>Mijnheer</last>
    </author>
    <author>
      <first>Willem</first>
      <last>Zuidema</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–66</pages>
    <url>http://www.aclweb.org/anthology/W18-0107</url>
    <doi>10.18653/v1/W18-0107</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>abnar-EtAl:2018:CMCL</bibkey>
  </paper>
  <paper id='0108'>
    <title>
      Exactly two things to learn from modeling scope ambiguity resolution:
      Developmental continuity and numeral semantics
    </title>
    <author>
      <first>K.J.</first>
      <last>Savinelli</last>
    </author>
    <author>
      <first>Greg</first>
      <last>Scontras</last>
    </author>
    <author>
      <first>Lisa</first>
      <last>Pearl</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–75</pages>
    <url>http://www.aclweb.org/anthology/W18-0108</url>
    <doi>10.18653/v1/W18-0108</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>savinelli-scontras-pearl:2018:CMCL</bibkey>
  </paper>
  <paper id='0200'>
    <title>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </title>
    <editor>Tommi A Pirinen</editor>
    <editor>Michael Rießler</editor>
    <editor>Jack Rueter</editor>
    <editor>Trond Trosterud</editor>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-02</url>
    <doi>10.18653/v1/W18-02</doi>
    <bibtype>book</bibtype>
    <bibkey>IWCLUL2018:2018</bibkey>
  </paper>
  <paper id='0201'>
    <title>
      Dependency Parsing of Code-Switching Data with Cross-Lingual Feature
      Representations
    </title>
    <author>
      <first>Niko</first>
      <last>Partanen</last>
    </author>
    <author>
      <first>Kyungtae</first>
      <last>Lim</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Rießler</last>
    </author>
    <author>
      <first>Thierry</first>
      <last>Poibeau</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–17</pages>
    <url>http://www.aclweb.org/anthology/W18-0201</url>
    <doi>10.18653/v1/W18-0201</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>partanen-EtAl:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0202'>
    <title>Building a Finnish SOM-based ontology concept tagger and harvester</title>
    <author>
      <first>Seppo</first>
      <last>Nyrkkö</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18–25</pages>
    <url>http://www.aclweb.org/anthology/W18-0202</url>
    <doi>10.18653/v1/W18-0202</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>nyrkko:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0203'>
    <title>Sound-aligned corpus of Udmurt dialectal texts</title>
    <author>
      <first>Timofey</first>
      <last>Arkhangelskiy</last>
    </author>
    <author>
      <first>Ekaterina</first>
      <last>Georgieva</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26–38</pages>
    <url>http://www.aclweb.org/anthology/W18-0203</url>
    <doi>10.18653/v1/W18-0203</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>arkhangelskiy-georgieva:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0204'>
    <title>Automatic Generation of Wiktionary Entries for Finno-Ugric Minority Languages</title>
    <author>
      <first>Zsanett</first>
      <last>Ferenczi</last>
    </author>
    <author>
      <first>Iván</first>
      <last>Mittelholcz</last>
    </author>
    <author>
      <first>Eszter</first>
      <last>Simon</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–50</pages>
    <url>http://www.aclweb.org/anthology/W18-0204</url>
    <doi>10.18653/v1/W18-0204</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ferenczi-mittelholcz-simon:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0205'>
    <title>Development of an Open Source Natural Language Generation Tool for Finnish</title>
    <author>
      <first>Mika</first>
      <last>Hämäläinen</last>
    </author>
    <author>
      <first>Jack</first>
      <last>Rueter</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>51–58</pages>
    <url>http://www.aclweb.org/anthology/W18-0205</url>
    <doi>10.18653/v1/W18-0205</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>hamalainen-rueter:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0206'>
    <title>Guessing lexicon entries using finite-state methods</title>
    <author>
      <first>Kimmo</first>
      <last>Koskenniemi</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–75</pages>
    <url>http://www.aclweb.org/anthology/W18-0206</url>
    <doi>10.18653/v1/W18-0206</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>koskenniemi:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0207'>
    <title>
      Tracking Typological Traits of Uralic Languages in Distributed Language
      Representations
    </title>
    <author>
      <first>Johannes</first>
      <last>Bjerva</last>
    </author>
    <author>
      <first>Isabelle</first>
      <last>Augenstein</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–86</pages>
    <url>http://www.aclweb.org/anthology/W18-0207</url>
    <doi>10.18653/v1/W18-0207</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bjerva-augenstein:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0208'>
    <title>New Baseline in Automatic Speech Recognition for Northern Sámi</title>
    <author>
      <first>Juho</first>
      <last>Leinonen</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Smit</last>
    </author>
    <author>
      <first>Sami</first>
      <last>Virpioja</last>
    </author>
    <author>
      <first>Mikko</first>
      <last>Kurimo</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–97</pages>
    <url>http://www.aclweb.org/anthology/W18-0208</url>
    <doi>10.18653/v1/W18-0208</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>leinonen-EtAl:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0209'>
    <title>Initial Experiments in Data-Driven Morphological Analysis for Finnish</title>
    <author>
      <first>Miikka</first>
      <last>Silfverberg</last>
    </author>
    <author>
      <first>Mans</first>
      <last>Hulden</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>98–105</pages>
    <url>http://www.aclweb.org/anthology/W18-0209</url>
    <doi>10.18653/v1/W18-0209</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>silfverberg-hulden:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0210'>
    <title>Towards an open-source universal-dependency treebank for Erzya</title>
    <author>
      <first>Jack</first>
      <last>Rueter</last>
    </author>
    <author>
      <first>Francis</first>
      <last>Tyers</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>106–118</pages>
    <url>http://www.aclweb.org/anthology/W18-0210</url>
    <doi>10.18653/v1/W18-0210</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rueter-tyers:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0211'>
    <title>
      Utilization of Nganasan digital resources: a statistical approach to vowel
      harmony
    </title>
    <author>
      <first>László</first>
      <last>Fejes</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119–138</pages>
    <url>http://www.aclweb.org/anthology/W18-0211</url>
    <doi>10.18653/v1/W18-0211</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>fejes:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0212'>
    <title>Parallel Forms in Estonian Finite State Morphology</title>
    <author>
      <first>Heiki-Jaan</first>
      <last>Kaalep</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>139–153</pages>
    <url>http://www.aclweb.org/anthology/W18-0212</url>
    <doi>10.18653/v1/W18-0212</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kaalep:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0213'>
    <title>
      Extracting inflectional class assignment in Pite Saami: Nouns, verbs and
      those pesky adjectives
    </title>
    <author>
      <first>Joshua</first>
      <last>Wilbur</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>154–168</pages>
    <url>http://www.aclweb.org/anthology/W18-0213</url>
    <doi>10.18653/v1/W18-0213</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wilbur:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0214'>
    <title>Analysing Finnish with word lists: the DDI approach to morphology revisited</title>
    <author>
      <first>Atro</first>
      <last>Voutilainen</last>
    </author>
    <author>
      <first>Maria</first>
      <last>Palolahti</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>169–179</pages>
    <url>http://www.aclweb.org/anthology/W18-0214</url>
    <doi>10.18653/v1/W18-0214</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>voutilainen-palolahti:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id='0300'>
    <title>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</title>
    <editor>Gaja Jarosz</editor>
    <editor>Brendan O'Connor</editor>
    <editor>Joe Pater</editor>
    <doi>10.7275/R5GF0RQW</doi>
  </paper>
  <paper id='0301'>
    <title>
      Statistical Learning Theory and Linguistic Typology: a Learnability
      Perspective on OT's Strict Domination
    </title>
    <author>Émile Enguehard</author>
    <author>Edward Flemming</author>
    <author>Giorgio Magri</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>1-11</pages>
    <attachment type='note'>W18-0301.Notes.pdf</attachment>
    <dataset>W18-0301.Datasets.zip</dataset>
    <doi>10.7275/R5BP010Z</doi>
  </paper>
  <paper id='0302'>
    <title>
      Detecting Language Impairments in Autism: A Computational Analysis of
      Semi-structured Conversations with Vector Semantics
    </title>
    <author>Adam Goodkind</author>
    <author>Michelle Lee</author>
    <author>Gary E. Martin</author>
    <author>Molly Losh</author>
    <author>Klinton Bicknell</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>12-22</pages>
    <doi>10.7275/R56W988P</doi>
  </paper>
  <paper id='0303'>
    <title>Grammar Size and Quantitative Restrictions on Movement</title>
    <author>Thomas Graf</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>23-33</pages>
    <doi>10.7275/R5348HJ8</doi>
  </paper>
  <paper id='0304'>
    <title>Modeling the Decline in English Passivization</title>
    <author>Liwen Hou</author>
    <author>David Smith</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>34-43</pages>
    <doi>10.7275/R5ZC812C</doi>
  </paper>
  <paper id='0305'>
    <title>Syntactic Category Learning as Iterative Prototype-Driven Clustering</title>
    <author>Jordan Kodner</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>44-54</pages>
    <doi>10.7275/R5TQ5ZQ4</doi>
  </paper>
  <paper id='0306'>
    <title>A bidirectional mapping between English and CNF-based reasoners</title>
    <author>Steven Abney</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>55-63</pages>
    <doi>10.7275/R5PZ571N</doi>
  </paper>
  <paper id='0307'>
    <title>Formal Restrictions On Multiple Tiers</title>
    <author>Alena Aksenova</author>
    <author>Sanket Deshmukh</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>64-73</pages>
    <doi>10.7275/R5K64G8S</doi>
  </paper>
  <paper id='0308'>
    <title>Differentiating Phrase Structure Parsing and Memory Retrieval in the Brain</title>
    <author>Shohini Bhattasali</author>
    <author>John Hale</author>
    <author>Christophe Pallier</author>
    <author>Jonathan Brennan</author>
    <author>Wen-Ming Luh</author>
    <author>R. Nathan Spreng</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>74-80</pages>
    <doi>10.7275/R5FF3QJ2</doi>
  </paper>
  <paper id='0309'>
    <title>Modeling the Complexity and Descriptive Adequacy of Construction Grammars</title>
    <author>Jonathan Dunn</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>81-90</pages>
    <doi>10.7275/R59P2ZTB</doi>
  </paper>
  <paper id='0310'>
    <title>Decomposing phonological transformations in serial derivations</title>
    <author>Andrew Lamont</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>91-101</pages>
    <doi>10.7275/R55X273D</doi>
  </paper>
  <paper id='0311'>
    <title>
      Phonologically Informed Edit Distance Algorithms for Word Alignment with
      Low-Resource Languages
    </title>
    <author>Richard T. McCoy</author>
    <author>Robert Frank</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>102-112</pages>
    <doi>10.7275/R5251GC0</doi>
  </paper>
  <paper id='0312'>
    <title>Conditions on abruptness in a gradient-ascent Maximum Entropy learner</title>
    <author>Elliott Moreton</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>113-124</pages>
    <doi>10.7275/R5XG9PBX</doi>
  </paper>
  <paper id='0313'>
    <title>Using Rhetorical Topics for Automatic Summarization</title>
    <author>Natalie M. Schrimpf</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>125-135</pages>
    <doi>10.7275/R5SQ8XM6</doi>
  </paper>
  <paper id='0314'>
    <title>Sound Analogies with Phoneme Embeddings</title>
    <author>Miikka P. Silfverberg</author>
    <author>Lingshuang Mao</author>
    <author>Mans Hulden</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>136-144</pages>
    <doi>10.7275/R5NZ85VD</doi>
  </paper>
  <paper id='0315'>
    <title>Imdlawn Tashlhiyt Berber Syllabification is Quantifier-Free</title>
    <author>Kristina Strother-Garcia</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>145-153</pages>
    <doi>10.7275/R5J67F4D</doi>
  </paper>
  <paper id='0316'>
    <title>Towards a Formal Description of NPI-licensing Patterns</title>
    <author>Mai Ha Vu</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>154-163</pages>
    <doi>10.7275/R5DF6PDP</doi>
  </paper>
  <paper id='0317'>
    <title>The Organization of Lexicons: a Cross-Linguistic Analysis of Monosyllabic Words</title>
    <author>Shiying Yang</author>
    <author>Chelsea Sanker</author>
    <author>Uriel Cohen Priva</author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>164-173</pages>
    <doi>10.7275/R58P5XPZ</doi>
  </paper>
  <paper id='0500'>
    <title>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </title>
    <editor>
      <first>Joel</first>
      <last>Tetreault</last>
    </editor>
    <editor>
      <first>Jill</first>
      <last>Burstein</last>
    </editor>
    <editor>
      <first>Ekaterina</first>
      <last>Kochmar</last>
    </editor>
    <editor>
      <first>Claudia</first>
      <last>Leacock</last>
    </editor>
    <editor>
      <first>Helen</first>
      <last>Yannakoudakis</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-05</url>
    <doi>10.18653/v1/W18-05</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-05:2018</bibkey>
  </paper>
  <paper id='0501'>
    <title>
      Using exemplar responses for training and evaluating automated speech
      scoring systems
    </title>
    <author>
      <first>Anastassia</first>
      <last>Loukina</last>
    </author>
    <author>
      <first>Klaus</first>
      <last>Zechner</last>
    </author>
    <author>
      <first>James</first>
      <last>Bruno</last>
    </author>
    <author>
      <first>Beata</first>
      <last>Beigman Klebanov</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–12</pages>
    <abstract>
      Automated scoring engines are usually trained and evaluated against human
      scores and compared to the benchmark of human-human agreement. In this
      paper we compare the performance of an automated speech scoring engine
      using two corpora: a corpus of almost 700,000 randomly sampled spoken
      responses with scores assigned by one or two raters during operational
      scoring, and a corpus of 16,500 exemplar responses with scores reviewed by
      multiple expert raters. We show that the choice of corpus used for model
      evaluation has a major effect on estimates of system performance with r
      varying between 0.64 and 0.80. Surprisingly, this is not the case for the
      choice of corpus for model training: when the training corpus is
      sufficiently large, the systems trained on different corpora showed almost
      identical performance when evaluated on the same corpus. We show that this
      effect is consistent across several learning algorithms. We conclude that
      evaluating the model on a corpus of exemplar responses if one is available
      provides additional evidence about system validity; at the same time,
      investing effort into creating a corpus of exemplar responses for model
      training is unlikely to lead to a substantial gain in model performance.
      Author3Affiliation
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0501</url>
    <doi>10.18653/v1/W18-0501</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>loukina-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0502'>
    <title>
      Using Paraphrasing and Memory-Augmented Models to Combat Data Sparsity in
      Question Interpretation with a Virtual Patient Dialogue System
    </title>
    <author>
      <first>Lifeng</first>
      <last>Jin</last>
    </author>
    <author>
      <first>David</first>
      <last>King</last>
    </author>
    <author>
      <first>Amad</first>
      <last>Hussein</last>
    </author>
    <author>
      <first>Michael</first>
      <last>White</last>
    </author>
    <author>
      <first>Douglas</first>
      <last>Danforth</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–23</pages>
    <abstract>
      When interpreting questions in a virtual patient dialogue system one must
      inevitably tackle the challenge of a long tail of relatively infrequently
      asked questions. To make progress on this challenge, we investigate the
      use of paraphrasing for data augmentation and neural memory-based
      classification, finding that the two methods work best in combination. In
      particular, we find that the neural memory-based approach not only
      outperforms a straight CNN classifier on low frequency questions, but also
      takes better advantage of the augmented data created by paraphrasing,
      together yielding a nearly 10\% absolute improvement in accuracy on the
      least frequently asked questions.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0502</url>
    <doi>10.18653/v1/W18-0502</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jin-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0503'>
    <title>Predicting misreadings from gaze in children with reading difficulties</title>
    <author>
      <first>Joachim</first>
      <last>Bingel</last>
    </author>
    <author>
      <first>Maria</first>
      <last>Barrett</last>
    </author>
    <author>
      <first>Sigrid</first>
      <last>Klerke</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–34</pages>
    <abstract>
      We present the first work on predicting reading mistakes in children with
      reading difficulties based on eye-tracking data from real-world reading
      teaching. Our approach employs several linguistic and gaze-based features
      to inform an ensemble of different classifiers, including multi-task
      learning models that let us transfer knowledge about individual readers to
      attain better predictions. Notably, the data we use in this work stems
      from noisy readings in the wild, outside of controlled lab conditions. Our
      experiments show that despite the noise and despite the small fraction of
      misreadings, gaze data improves the performance more than any other
      feature group and our models achieve good performance. We further show
      that gaze patterns for misread words do not fully generalize across
      readers, but that we can transfer some knowledge between readers using
      multitask learning at least in some cases. Applications of our models
      include partial automation of reading assessment as well as personalized
      text simplification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0503</url>
    <doi>10.18653/v1/W18-0503</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bingel-barrett-klerke:2018:W18-05</bibkey>
  </paper>
  <paper id='0504'>
    <title>
      Automatic Input Enrichment for Selecting Reading Material: An Online Study
      with English Teachers
    </title>
    <author>
      <first>Maria</first>
      <last>Chinkina</last>
    </author>
    <author>
      <first>Ankita</first>
      <last>Oswal</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–44</pages>
    <abstract>
      Input material at the appropriate level is crucial for language
      acquisition. Automating the search for such material can systematically
      and efficiently support teachers in their pedagogical practice. This is
      the goal of the computational linguistic task of automatic input
      enrichment (Chinkina & Meurers, 2016): It analyzes and re-ranks a
      collection of texts in order to prioritize those containing target
      linguistic forms. In the online study described in the paper, we collected
      240 responses from English teachers in order to investigate whether they
      preferred automatic input enrichment over web search when selecting
      reading material for class. Participants demonstrated a general preference
      for the material provided by an automatic input enrichment system. It was
      also rated significantly higher than the texts retrieved by a standard web
      search engine with regard to the representation of linguistic forms and
      equivalent with regard to the relevance of the content to the topic. We
      discuss the implications of the results for language teaching and consider
      the potential strands of future research.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0504</url>
    <doi>10.18653/v1/W18-0504</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chinkina-oswal-meurers:2018:W18-05</bibkey>
  </paper>
  <paper id='0505'>
    <title>Estimating Linguistic Complexity for Science Texts</title>
    <author>
      <first>Farah</first>
      <last>Nadeem</last>
    </author>
    <author>
      <first>Mari</first>
      <last>Ostendorf</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45–55</pages>
    <abstract>
      Evaluation of text difficulty is important both for downstream tasks like
      text simplification, and for supporting educators in classrooms. Existing
      work on automated text complexity analysis uses linear models with
      engineered knowledge-driven features as inputs. While this offers
      interpretability, these models have lower accuracy for shorter texts.
      Traditional readability metrics have the additional drawback of not
      generalizing to informational texts such as science. We propose a neural
      approach, training on science and other informational texts, to mitigate
      both problems. Our results show that neural methods outperform
      knowledge-based linear models for short texts, and have the capacity to
      generalize to genres not present in the training data.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0505</url>
    <doi>10.18653/v1/W18-0505</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>nadeem-ostendorf:2018:W18-05</bibkey>
  </paper>
  <paper id='0506'>
    <title>Second Language Acquisition Modeling</title>
    <author>
      <first>Burr</first>
      <last>Settles</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Brust</last>
    </author>
    <author>
      <first>Erin</first>
      <last>Gustafson</last>
    </author>
    <author>
      <first>Masato</first>
      <last>Hagiwara</last>
    </author>
    <author>
      <first>Nitin</first>
      <last>Madnani</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–65</pages>
    <abstract>
      We present the task of 
      <i>second language acquisition (SLA) modeling</i>
      . Given a history of errors made by learners of a second language, the
      task is to predict errors that they are likely to make at arbitrary points
      in the future. We describe a large corpus of more than 7M words produced
      by more than 6k learners of English, Spanish, and French using Duolingo, a
      popular online language-learning app. Then we report on the results of a
      shared task challenge aimed studying the SLA task via this corpus, which
      attracted 15 teams and synthesized work from various fields including
      cognitive science, linguistics, and machine learning.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0506</url>
    <doi>10.18653/v1/W18-0506</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>settles-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0507'>
    <title>A Report on the Complex Word Identification Shared Task 2018</title>
    <author>
      <first>Seid Muhie</first>
      <last>Yimam</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Biemann</last>
    </author>
    <author>
      <first>Shervin</first>
      <last>Malmasi</last>
    </author>
    <author>
      <first>Gustavo</first>
      <last>Paetzold</last>
    </author>
    <author>
      <first>Lucia</first>
      <last>Specia</last>
    </author>
    <author>
      <first>Sanja</first>
      <last>Štajner</last>
    </author>
    <author>
      <first>Anaïs</first>
      <last>Tack</last>
    </author>
    <author>
      <first>Marcos</first>
      <last>Zampieri</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–78</pages>
    <abstract>
      We report the findings of the second Complex Word Identification (CWI)
      shared task organized as part of the BEA workshop co-located with
      NAACL-HLT’2018. The second CWI shared task featured multilingual and
      multi-genre datasets divided into four tracks: English monolingual, German
      monolingual, Spanish monolingual, and a multilingual track with a French
      test set, and two tasks: binary classification and probabilistic
      classification. A total of 12 teams submitted their results in different
      task/track combinations and 11 of them wrote system description papers
      that are referred to in this report and appear in the BEA workshop
      proceedings.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0507</url>
    <doi>10.18653/v1/W18-0507</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>yimam-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0508'>
    <title>Towards Single Word Lexical Complexity Prediction</title>
    <author>
      <first>David</first>
      <last>Alfter</last>
    </author>
    <author>
      <first>Elena</first>
      <last>Volodina</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–88</pages>
    <abstract>
      In this paper we present work-in-progress where we investigate the
      usefulness of previously created word lists to the task of single-word
      lexical complexity analysis and prediction of the complexity level for
      learners of Swedish as a second language. The word lists used map each
      word to a single CEFR level, and the task consists of predicting CEFR
      levels for unseen words. In contrast to previous work on word-level
      lexical complexity, we experiment with topics as additional features and
      show that linking words to topics significantly increases accuracy of
      classification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0508</url>
    <doi>10.18653/v1/W18-0508</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>alfter-volodina:2018:W18-05</bibkey>
  </paper>
  <paper id='0509'>
    <title>
      COAST - Customizable Online Syllable Enhancement in Texts. A flexible
      framework for automatically enhancing reading materials
    </title>
    <author>
      <first>Heiko</first>
      <last>Holz</last>
    </author>
    <author>
      <first>Zarah</first>
      <last>Weiss</last>
    </author>
    <author>
      <first>Oliver</first>
      <last>Brehm</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>89–100</pages>
    <abstract>
      This paper presents COAST, a web-based application to easily and
      automatically enhance syllable structure, word stress, and spacing in
      texts, that was designed in close collaboration with learning therapists
      to ensure its practical relevance. Such syllable-enhanced texts are
      commonly used in learning therapy or private tuition to promote the
      recognition of syllables in order to improve reading and writing skills.
      In a state of the art solutions for automatic syllable enhancement, we put
      special emphasis on syllable stress and support specific marking of the
      primary syllable stress in words. Core features of our tool are i) a
      highly customizable text enhancement and template functionality, and ii) a
      novel crowd-sourcing mechanism that we employ to address the issue of data
      sparsity in language resources. We successfully tested COAST with
      real-life practitioners in a series of user tests validating the concept
      of our framework.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0509</url>
    <doi>10.18653/v1/W18-0509</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>holz-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0510'>
    <title>Annotating picture description task responses for content analysis</title>
    <author>
      <first>Levi</first>
      <last>King</last>
    </author>
    <author>
      <first>Markus</first>
      <last>Dickinson</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>101–109</pages>
    <abstract>
      Given that all users of a language can be creative in their language
      usage, the overarching goal of this work is to investigate issues of
      variability and acceptability in written text, for both non-native
      speakers (NNSs) and native speakers (NSs). We control for meaning by
      collecting a dataset of picture description task (PDT) responses from a
      number of NSs and NNSs, and we define and annotate a handful of features
      pertaining to form and meaning, to capture the multi-dimensional ways in
      which responses can vary and can be acceptable. By examining the decisions
      made in this corpus development, we highlight the questions facing anyone
      working with learner language properties like variability, acceptability
      and native-likeness. We find reliable inter-annotator agreement, though
      disagreements point to difficult areas for establishing a link between
      form and meaning.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0510</url>
    <doi>10.18653/v1/W18-0510</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>king-dickinson:2018:W18-05</bibkey>
  </paper>
  <paper id='0511'>
    <title>Annotating Student Talk in Text-based Classroom Discussions</title>
    <author>
      <first>Luca</first>
      <last>Lugini</last>
    </author>
    <author>
      <first>Diane</first>
      <last>Litman</last>
    </author>
    <author>
      <first>Amanda</first>
      <last>Godley</last>
    </author>
    <author>
      <first>Christopher</first>
      <last>Olshefski</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110–116</pages>
    <abstract>
      Classroom discussions in English Language Arts have a positive effect on
      students’ reading, writing and reasoning skills. Although prior work has
      largely focused on teacher talk and student-teacher interactions, we focus
      on three theoretically-motivated aspects of high-quality student talk:
      argumentation, specificity, and knowledge domain. We introduce an
      annotation scheme, then show that the scheme can be used to produce
      reliable annotations and that the annotations are predictive of discussion
      quality. We also highlight opportunities provided by our scheme for
      education and natural language processing research.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0511</url>
    <doi>10.18653/v1/W18-0511</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>lugini-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0512'>
    <title>
      Toward Automatically Measuring Learner Ability from Human-Machine Dialog
      Interactions using Novel Psychometric Models
    </title>
    <author>
      <first>Vikram</first>
      <last>Ramanarayanan</last>
    </author>
    <author>
      <first>Michelle</first>
      <last>LaMar</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>117–126</pages>
    <abstract>
      While dialog systems have been widely deployed for computer-assisted
      language learning (CALL) and formative assessment systems in recent years,
      relatively limited work has been done with respect to the psychometrics
      and validity of these technologies in evaluating and providing feedback
      regarding student learning and conversational ability. This paper
      formulates a Markov decision process based measurement model, and applies
      it to text chat data collected from crowdsourced native and non-native
      English language speakers interacting with an automated dialog agent. We
      investigate how well the model measures speaker conversational ability,
      and find that it effectively captures the differences in how native and
      non-native speakers of English accomplish the dialog task. Such models
      could have important implications for CALL systems of the future that
      effectively combine dialog management with measurement of learner
      conversational ability in real-time.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0512</url>
    <doi>10.18653/v1/W18-0512</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ramanarayanan-lamar:2018:W18-05</bibkey>
  </paper>
  <paper id='0513'>
    <title>Generating Feedback for English Foreign Language Exercises</title>
    <author>
      <first>Björn</first>
      <last>Rudzewitz</last>
    </author>
    <author>
      <first>Ramon</first>
      <last>Ziai</last>
    </author>
    <author>
      <first>Kordula</first>
      <last>De Kuthy</last>
    </author>
    <author>
      <first>Verena</first>
      <last>Möller</last>
    </author>
    <author>
      <first>Florian</first>
      <last>Nuxoll</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>127–136</pages>
    <abstract>
      While immediate feedback on learner language is often discussed in the
      Second Language Acquisition literature (e.g., Mackey 2006), few systems
      used in real-life educational settings provide helpful, metalinguistic
      feedback to learners. In this paper, we present a novel approach
      leveraging task information to generate the expected range of well-formed
      and ill-formed variability in learner answers along with the required
      diagnosis and feedback. We combine this offline generation approach with
      an online component that matches the actual student answers against the
      pre-computed hypotheses. The results obtained for a set of 33 thousand
      answers of 7th grade German high school students learning English show
      that the approach successfully covers frequent answer patterns. At the
      same time, paraphrases and content errors require a more flexible
      alignment approach, for which we are planning to complement the method
      with the CoMiC approach successfully used for the analysis of reading
      comprehension answers (Meurers et al., 2011).
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0513</url>
    <doi>10.18653/v1/W18-0513</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rudzewitz-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0514'>
    <title>
      NT2Lex: A CEFR-Graded Lexical Resource for Dutch as a Foreign Language
      Linked to Open Dutch WordNet
    </title>
    <author>
      <first>Anaïs</first>
      <last>Tack</last>
    </author>
    <author>
      <first>Thomas</first>
      <last>François</last>
    </author>
    <author>
      <first>Piet</first>
      <last>Desmet</last>
    </author>
    <author>
      <first>Cédrick</first>
      <last>Fairon</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>137–146</pages>
    <abstract>
      In this paper, we introduce NT2Lex, a novel lexical resource for Dutch as
      a foreign language (NT2) which includes frequency distributions of 17,743
      words and expressions attested in expert-written textbook texts and
      readers graded along the scale of the Common European Framework of
      Reference (CEFR). In essence, the lexicon informs us about what kind of
      vocabulary should be understood when reading Dutch as a non-native reader
      at a particular proficiency level. The main novelty of the resource with
      respect to the previously developed CEFR-graded lexicons concerns the
      introduction of corpus-based evidence for L2 word sense complexity through
      the linkage to Open Dutch WordNet (Postma et al., 2016). The resource thus
      contains, on top of the lemmatised and part-of-speech tagged lexical
      entries, a total of 11,999 unique word senses and 8,934 distinct synsets.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0514</url>
    <doi>10.18653/v1/W18-0514</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>tack-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0515'>
    <title>Experiments with Universal CEFR Classification</title>
    <author>
      <first>Sowmya</first>
      <last>Vajjala</last>
    </author>
    <author>
      <first>Taraka</first>
      <last>Rama</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>147–153</pages>
    <abstract>
      The Common European Framework of Reference (CEFR) guidelines describe
      language proficiency of learners on a scale of 6 levels. While the
      description of CEFR guidelines is generic across languages, the
      development of automated proficiency classification systems for different
      languages follow different approaches. In this paper, we explore universal
      CEFR classification using domain-specific and domain-agnostic,
      theory-guided as well as data-driven features. We report the results of
      our preliminary experiments in monolingual, cross-lingual, and
      multilingual classification with three languages: German, Czech, and
      Italian. Our results show that both monolingual and multilingual models
      achieve similar performance, and cross-lingual classification yields
      lower, but comparable results to monolingual classification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0515</url>
    <doi>10.18653/v1/W18-0515</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>vajjala-rama:2018:W18-05</bibkey>
  </paper>
  <paper id='0516'>
    <title>Chengyu Cloze Test</title>
    <author>
      <first>Zhiying</first>
      <last>Jiang</last>
    </author>
    <author>
      <first>Boliang</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Lifu</first>
      <last>Huang</last>
    </author>
    <author>
      <first>Heng</first>
      <last>Ji</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>154–158</pages>
    <abstract>
      We present a neural recommendation model for Chengyu, which is a special
      type of Chinese idiom. Given a query, which is a sentence with an empty
      slot where the Chengyu is taken out, our model will recommend the best
      Chengyu candidate that best fits the slot context. The main challenge lies
      in that the literal meaning of a Chengyu is usually very different from
      it’s figurative meaning. We propose a new neural approach to leverage the
      definition of each Chengyu and incorporate it as background knowledge.
      Experiments on both Chengyu cloze test and coherence checking in college
      entrance exams show that our system achieves 89.5% accuracy on cloze test
      and outperforms human subjects who attended competitive universities in
      China. We will make all of our data sets and resources publicly available
      as a new benchmark for research purposes.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0516</url>
    <doi>10.18653/v1/W18-0516</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jiang-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0517'>
    <title>LaSTUS/TALN at Complex Word Identification (CWI) 2018 Shared Task</title>
    <author>
      <first>Ahmed</first>
      <last>AbuRa’ed</last>
    </author>
    <author>
      <first>Horacio</first>
      <last>Saggion</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>159–165</pages>
    <abstract>
      This paper presents the participation of the LaSTUS/TALN team in the
      Complex Word Identification (CWI) Shared Task 2018 in the English
      monolingual track . The purpose of the task was to determine if a word in
      a given sentence can be judged as complex or not by a certain target
      audience. For the English track, task organizers provided a training and a
      development datasets of 27,299 and 3,328 words respectively together with
      the sentence in which each word occurs. The words were judged as complex
      or not by 20 human evaluators; ten of whom are natives. We submitted two
      systems: one system modeled each word to evaluate as a numeric vector
      populated with a set of lexical, semantic and contextual features while
      the other system relies on a word embedding representation and a distance
      metric. We trained two separate classifiers to automatically decide if
      each word is complex or not. We submitted six runs, two for each of the
      three subsets of the English monolingual CWI track.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0517</url>
    <doi>10.18653/v1/W18-0517</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>aburaed-saggion:2018:W18-05</bibkey>
  </paper>
  <paper id='0518'>
    <title>Cross-lingual complex word identification with multitask learning</title>
    <author>
      <first>Joachim</first>
      <last>Bingel</last>
    </author>
    <author>
      <first>Johannes</first>
      <last>Bjerva</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>166–174</pages>
    <abstract>
      We approach the 2018 Shared Task on Complex Word Identification by
      leveraging a cross-lingual multitask learning approach. Our method is
      highly language agnostic, as evidenced by the ability of our system to
      generalize across languages, including languages for which we have no
      training data. In the shared task, this is the case for French, for which
      our system achieves the best performance. We further provide a qualitative
      and quantitative analysis of which words pose problems for our system.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0518</url>
    <doi>10.18653/v1/W18-0518</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bingel-bjerva:2018:W18-05</bibkey>
  </paper>
  <paper id='0519'>
    <title>UnibucKernel: A kernel-based learning method for complex word identification</title>
    <author>
      <first>Andrei</first>
      <last>Butnaru</last>
    </author>
    <author>
      <first>Radu Tudor</first>
      <last>Ionescu</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>175–183</pages>
    <abstract>
      In this paper, we present a kernel-based learning approach for the 2018
      Complex Word Identification (CWI) Shared Task. Our approach is based on
      combining multiple low-level features, such as character n-grams, with
      high-level semantic features that are either automatically learned using
      word embeddings or extracted from a lexical knowledge base, namely
      WordNet. After feature extraction, we employ a kernel method for the
      learning phase. The feature matrix is first transformed into a normalized
      kernel matrix. For the binary classification task (simple versus complex),
      we employ Support Vector Machines. For the regression task, in which we
      have to predict the complexity level of a word (a word is more complex if
      it is labeled as complex by more annotators), we employ v-Support Vector
      Regression. We applied our approach only on the three English data sets
      containing documents from Wikipedia, WikiNews and News domains. Our best
      result during the competition was the third place on the English Wikipedia
      data set. However, in this paper, we also report better post-competition
      results.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0519</url>
    <doi>10.18653/v1/W18-0519</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>butnaru-ionescu:2018:W18-05</bibkey>
  </paper>
  <paper id='0520'>
    <title>
      CAMB at CWI Shared Task 2018: Complex Word Identification with
      Ensemble-Based Voting
    </title>
    <author>
      <first>Sian</first>
      <last>Gooding</last>
    </author>
    <author>
      <first>Ekaterina</first>
      <last>Kochmar</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>184–194</pages>
    <abstract>
      This paper presents the winning systems we submitted to the Complex Word
      Identifica- tion Shared Task 2018. We describe our best performing
      systems’ implementations and discuss our key findings from this research.
      Our best-performing systems achieve an F1 score of 0.8792 on the NEWS,
      0.8430 on the WIKINEWS and 0.8115 on the WIKIPEDIA test sets in the
      monolingual English binary classification track, and a mean absolute error
      of 0.0558 on the NEWS, 0.0674 on the WIKINEWS and 0.0739 on the WIKIPEDIA
      test sets in the probabilistic track.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0520</url>
    <doi>10.18653/v1/W18-0520</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gooding-kochmar:2018:W18-05</bibkey>
  </paper>
  <paper id='0521'>
    <title>Complex Word Identification Based on Frequency in a Learner Corpus</title>
    <author>
      <first>Tomoyuki</first>
      <last>Kajiwara</last>
    </author>
    <author>
      <first>Mamoru</first>
      <last>Komachi</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>195–199</pages>
    <abstract>
      We introduce the TMU systems for the Complex Word Identification (CWI)
      Shared Task 2018. TMU systems use random forest classifiers and regressors
      whose features are the number of characters, the number of words, and the
      frequency of target words in various corpora. Our simple systems performed
      best on 5 tracks out of 12 tracks. Our ablation analysis revealed the
      usefulness of a learner corpus for CWI task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0521</url>
    <doi>10.18653/v1/W18-0521</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kajiwara-komachi:2018:W18-05</bibkey>
  </paper>
  <paper id='0522'>
    <title>
      The Whole is Greater than the Sum of its Parts: Towards the Effectiveness
      of Voting Ensemble Classifiers for Complex Word Identification
    </title>
    <author>
      <first>Nikhil</first>
      <last>Wani</last>
    </author>
    <author>
      <first>Sandeep</first>
      <last>Mathias</last>
    </author>
    <author>
      <first>Jayashree Aanand</first>
      <last>Gajjam</last>
    </author>
    <author>
      <first>Pushpak</first>
      <last>Bhattacharyya</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200–205</pages>
    <abstract>
      In this paper, we present an effective system using voting ensemble
      classifiers to detect contextually complex words for non-native English
      speakers. To make the final decision, we channel a set of eight calibrated
      classifiers based on lexical, size and vocabulary features and train our
      model with annotated datasets collected from a mixture of native and
      non-native speakers. Thereafter, we test our system on three datasets
      namely News, WikiNews, and Wikipedia and report competitive results with
      an F1-Score ranging between 0.777 to 0.855 for each of the datasets. Our
      system outperforms multiple other models and falls within 0.042 to 0.026
      percent of the best-performing model’s score in the shared task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0522</url>
    <doi>10.18653/v1/W18-0522</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wani-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0523'>
    <title>
      Grotoco@SLAM: Second Language Acquisition Modeling with Simple Features,
      Learners and Task-wise Models
    </title>
    <author>
      <first>Sigrid</first>
      <last>Klerke</last>
    </author>
    <author>
      <first>Héctor</first>
      <last>Martínez Alonso</last>
    </author>
    <author>
      <first>Barbara</first>
      <last>Plank</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>206–211</pages>
    <abstract>
      We present our submission to the 2018 Duolingo Shared Task on Second
      Language Acquisition Modeling (SLAM). We focus on evaluating a range of
      features for the task, including user-derived measures, while examining
      how far we can get with a simple linear classifier. Our analysis reveals
      that errors differ per exercise format, which motivates our final and
      best-performing system: a task-wise (per exercise-format) model.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0523</url>
    <doi>10.18653/v1/W18-0523</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>klerke-martnezalonso-plank:2018:W18-05</bibkey>
  </paper>
  <paper id='0524'>
    <title>Context Based Approach for Second Language Acquisition</title>
    <author>
      <first>Nihal V.</first>
      <last>Nayak</last>
    </author>
    <author>
      <first>Arjun R.</first>
      <last>Rao</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>212–216</pages>
    <abstract>
      SLAM 2018 focuses on predicting a student’s mistake while using the
      Duolingo application. In this paper, we describe the system we developed
      for this shared task. Our system uses a logistic regression model to
      predict the likelihood of a student making a mistake while answering an
      exercise on Duolingo in all three language tracks - English/Spanish
      (en/es), Spanish/English (es/en) and French/English (fr/en). We conduct an
      ablation study with several features during the development of this system
      and discover that context based features plays a major role in language
      acquisition modeling. Our model beats Duolingo’s baseline scores in all
      three language tracks (AUROC scores for en/es = 0.821, es/en = 0.790 and
      fr/en = 0.812). Our work makes a case for providing favourable textual
      context for students while learning second language.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0524</url>
    <doi>10.18653/v1/W18-0524</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>nayak-rao:2018:W18-05</bibkey>
  </paper>
  <paper id='0525'>
    <title>Second Language Acquisition Modeling: An Ensemble Approach</title>
    <author>
      <first>Anton</first>
      <last>Osika</last>
    </author>
    <author>
      <first>Susanna</first>
      <last>Nilsson</last>
    </author>
    <author>
      <first>Andrii</first>
      <last>Sydorchuk</last>
    </author>
    <author>
      <first>Faruk</first>
      <last>Sahin</last>
    </author>
    <author>
      <first>Anders</first>
      <last>Huss</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>217–222</pages>
    <abstract>
      Accurate prediction of students’ knowledge is a fundamental building block
      of personalized learning systems. Here, we propose an ensemble model to
      predict student knowledge gaps. Applying our approach to student trace
      data from the online educational platform Duolingo we achieved highest
      score on all three datasets in the 2018 Shared Task on Second Language
      Acquisition Modeling. We describe our model and discuss relevance of the
      task compared to how it would be setup in a production environment for
      personalized education.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0525</url>
    <doi>10.18653/v1/W18-0525</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>osika-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0526'>
    <title>Modeling Second-Language Learning from a Psychological Perspective</title>
    <author>
      <first>Alexander</first>
      <last>Rich</last>
    </author>
    <author>
      <first>Pamela</first>
      <last>Osborn Popp</last>
    </author>
    <author>
      <first>David</first>
      <last>Halpern</last>
    </author>
    <author>
      <first>Anselm</first>
      <last>Rothe</last>
    </author>
    <author>
      <first>Todd</first>
      <last>Gureckis</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>223–230</pages>
    <abstract>
      Psychological research on learning and memory has tended to emphasize
      small-scale laboratory studies. However, large datasets of people using
      educational software provide opportunities to explore these issues from a
      new perspective. In this paper we describe our approach to the Duolingo
      Second Language Acquisition Modeling (SLAM) competition which was run in
      early 2018. We used a well-known class of algorithms (gradient boosted
      decision trees), with features partially informed by theories from the
      psychological literature. After detailing our modeling approach and a
      number of supplementary simulations, we reflect on the degree to which
      psychological theory aided the model, and the potential for cognitive
      science and predictive modeling competitions to gain from each other.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0526</url>
    <doi>10.18653/v1/W18-0526</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rich-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0527'>
    <title>
      A Memory-Sensitive Classification Model of Errors in Early Second Language
      Learning
    </title>
    <author>
      <first>Brendan</first>
      <last>Tomoschuk</last>
    </author>
    <author>
      <first>Jarrett</first>
      <last>Lovelett</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>231–239</pages>
    <abstract>
      In this paper, we explore a variety of lin- guistic and cognitive features
      to better un- derstand second language acquisition in early users of the
      language learning app Duolingo. With these features, we trained a random
      forest classifier to predict errors in early learners of French, Spanish,
      and English. Of particular note was our finding that mean and variance in
      error for each user and token can be a memory efficient replacement for
      their respective dummy- encoded categorical variables. At test, the- se
      models improved over the baseline model with AUROC values of 0.803 for
      English, 0.823 for French, and 0.829 for Spanish.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0527</url>
    <doi>10.18653/v1/W18-0527</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>tomoschuk-lovelett:2018:W18-05</bibkey>
  </paper>
  <paper id='0528'>
    <title>Annotation and Classification of Sentence-level Revision Improvement</title>
    <author>
      <first>Tazin</first>
      <last>Afrin</last>
    </author>
    <author>
      <first>Diane</first>
      <last>Litman</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>240–246</pages>
    <abstract>
      Studies of writing revisions rarely focus on revision quality. To address
      this issue, we introduce a corpus of between-draft revisions of student
      argumentative essays, annotated as to whether each revision improves essay
      quality. We demonstrate a potential usage of our annotations by developing
      a machine learning model to predict revision improvement. With the goal of
      expanding training data, we also extract revisions from a dataset edited
      by expert proofreaders. Our results indicate that blending expert and
      non-expert revisions increases model performance, with expert data
      particularly important for predicting low-quality revisions.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0528</url>
    <doi>10.18653/v1/W18-0528</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>afrin-litman:2018:W18-05</bibkey>
  </paper>
  <paper id='0529'>
    <title>
      Language Model Based Grammatical Error Correction without Annotated
      Training Data
    </title>
    <author>
      <first>Christopher</first>
      <last>Bryant</last>
    </author>
    <author>
      <first>Ted</first>
      <last>Briscoe</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>247–253</pages>
    <abstract>
      Since the end of the CoNLL-2014 shared task on grammatical error
      correction (GEC), research into language model (LM) based approaches to
      GEC has largely stagnated. In this paper, we re-examine LMs in GEC and
      show that it is entirely possible to build a simple system that not only
      requires minimal annotated data ($$1000 sentences), but is also fairly
      competitive with several state-of-the-art systems. This approach should be
      of particular interest for languages where very little annotated training
      data exists, although we also hope to use it as a baseline to motivate
      future research.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0529</url>
    <doi>10.18653/v1/W18-0529</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bryant-briscoe:2018:W18-05</bibkey>
  </paper>
  <paper id='0530'>
    <title>A Semantic Role-based Approach to Open-Domain Automatic Question Generation</title>
    <author>
      <first>Michael</first>
      <last>Flor</last>
    </author>
    <author>
      <first>Brian</first>
      <last>Riordan</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>254–263</pages>
    <abstract>
      We present a novel rule-based system for automatic generation of factual
      questions from sentences, using semantic role labeling (SRL) as the main
      form of text analysis. The system is capable of generating both
      wh-questions and yes/no questions from the same semantic analysis. We
      present an extensive evaluation of the system and compare it to a recent
      neural network architecture for question generation. The SRL-based system
      outperforms the neural system in both average quality and variety of
      generated questions.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0530</url>
    <doi>10.18653/v1/W18-0530</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>flor-riordan:2018:W18-05</bibkey>
  </paper>
  <paper id='0531'>
    <title>Automated Content Analysis: A Case Study of Computer Science Student Summaries</title>
    <author>
      <first>Yanjun</first>
      <last>Gao</last>
    </author>
    <author>
      <first>Patricia</first>
      <last>M.Davies</last>
    </author>
    <author>
      <first>Rebecca J.</first>
      <last>Passonneau</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>264–272</pages>
    <abstract>
      Technology is transforming Higher Education learning and teaching. This
      paper reports on a project to examine how and why automated content
      analysis could be used to assess precis writing by university students. We
      examine the case of one hundred and twenty-two summaries written by
      computer science freshmen. The texts, which had been hand scored using a
      teacher-designed rubric, were autoscored using the Natural Language
      Processing software, PyrEval. Pearson’s correlation coefficient and
      Spearman rank correlation were used to analyze the relationship between
      the teacher score and the PyrEval score for each summary. Three content
      models automatically constructed by PyrEval from different sets of human
      reference summaries led to consistent correlations, showing that the
      approach is reliable. Also observed was that, in cases where the focus of
      student assessment centers on formative feedback, categorizing the PyrEval
      scores by examining the average and standard deviations could lead to
      novel interpretations of their relationships. It is suggested that this
      project has implications for the ways in which automated content analysis
      could be used to help university students improve their summarization
      skills.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0531</url>
    <doi>10.18653/v1/W18-0531</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gao-mdavies-passonneau:2018:W18-05</bibkey>
  </paper>
  <paper id='0532'>
    <title>
      Toward Data-Driven Tutorial Question Answering with Deep Learning
      Conversational Models
    </title>
    <author>
      <first>Mayank</first>
      <last>Kulkarni</last>
    </author>
    <author>
      <first>Kristy</first>
      <last>Boyer</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>273–283</pages>
    <abstract>
      There has been an increase in popularity of data-driven question answering
      systems given their recent success. This pa-per explores the possibility
      of building a tutorial question answering system for Java programming from
      data sampled from a community-based question answering forum. This paper
      reports on the creation of a dataset that could support building such a
      tutorial question answering system and discusses the methodology to create
      the 106,386 question strong dataset. We investigate how retrieval-based
      and generative models perform on the given dataset. The work also
      investigates the usefulness of using hybrid approaches such as combining
      retrieval-based and generative models. The results indicate that building
      data-driven tutorial systems using community-based question answering
      forums holds significant promise.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0532</url>
    <doi>10.18653/v1/W18-0532</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kulkarni-boyer:2018:W18-05</bibkey>
  </paper>
  <paper id='0533'>
    <title>Distractor Generation for Multiple Choice Questions Using Learning to Rank</title>
    <author>
      <first>Chen</first>
      <last>Liang</last>
    </author>
    <author>
      <first>Xiao</first>
      <last>Yang</last>
    </author>
    <author>
      <first>Neisarg</first>
      <last>Dave</last>
    </author>
    <author>
      <first>Drew</first>
      <last>Wham</last>
    </author>
    <author>
      <first>Bart</first>
      <last>Pursel</last>
    </author>
    <author>
      <first>C Lee</first>
      <last>Giles</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>284–290</pages>
    <abstract>
      We investigate how machine learning models, specifically ranking models,
      can be used to select useful distractors for multiple choice questions.
      Our proposed models can learn to select distractors that resemble those in
      actual exam questions, which is different from most existing unsupervised
      ontology-based and similarity-based methods. We empirically study
      feature-based and neural net (NN) based ranking models with experiments on
      the recently released SciQ dataset and our MCQL dataset. Experimental
      results show that feature-based ensemble learning methods (random forest
      and LambdaMART) outperform both the NN-based method and unsupervised
      baselines. These two datasets can also be used as benchmarks for
      distractor generation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0533</url>
    <doi>10.18653/v1/W18-0533</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>liang-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0534'>
    <title>A Portuguese Native Language Identification Dataset</title>
    <author>
      <first>Iria</first>
      <von>del</von>
      <last>Río Gayo</last>
    </author>
    <author>
      <first>Marcos</first>
      <last>Zampieri</last>
    </author>
    <author>
      <first>Shervin</first>
      <last>Malmasi</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>291–296</pages>
    <abstract>
      In this paper we present NLI-PT, the first Portuguese dataset compiled for
      Native Language Identification (NLI), the task of identifying an author’s
      first language based on their second language writing. The dataset
      includes 1,868 student essays written by learners of European Portuguese,
      native speakers of the following L1s: Chinese, English, Spanish, German,
      Russian, French, Japanese, Italian, Dutch, Tetum, Arabic, Polish, Korean,
      Romanian, and Swedish. NLI-PT includes the original student text and four
      different types of annotation: POS, fine-grained POS, constituency parses,
      and dependency parses. NLI-PT can be used not only in NLI but also in
      research on several topics in the field of Second Language Acquisition and
      educational NLP. We discuss possible applications of this dataset and
      present the results obtained for the first lexical baseline system for
      Portuguese NLI.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0534</url>
    <doi>10.18653/v1/W18-0534</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>delrogayo-zampieri-malmasi:2018:W18-05</bibkey>
  </paper>
  <paper id='0535'>
    <title>
      OneStopEnglish corpus: A new corpus for automatic readability assessment
      and text simplification
    </title>
    <author>
      <first>Sowmya</first>
      <last>Vajjala</last>
    </author>
    <author>
      <first>Ivana</first>
      <last>Lucic</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>297–304</pages>
    <abstract>
      This paper describes the collection and compilation of the OneStopEnglish
      corpus of texts written at three reading levels, and demonstrates its
      usefulness for through two applications - automatic readability assessment
      and automatic text simplification. The corpus consists of 189 texts, each
      in three versions (567 in total). The corpus is now freely available under
      a CC by-SA 4.0 license and we hope that it would foster further research
      on the topics of readability assessment and text simplification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0535</url>
    <doi>10.18653/v1/W18-0535</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>vajjala-lucic:2018:W18-05</bibkey>
  </paper>
  <paper id='0536'>
    <title>The Effect of Adding Authorship Knowledge in Automated Text Scoring</title>
    <author>
      <first>Meng</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Xie</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Ronan</first>
      <last>Cummins</last>
    </author>
    <author>
      <first>Øistein E.</first>
      <last>Andersen</last>
    </author>
    <author>
      <first>Ted</first>
      <last>Briscoe</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>305–314</pages>
    <abstract>
      Some language exams have multiple writing tasks. When a learner writes
      multiple texts in a language exam, it is not surprising that the quality
      of these texts tends to be similar, and the existing automated text
      scoring (ATS) systems do not explicitly model this similarity. In this
      paper, we suggest that it could be useful to include the other texts
      written by this learner in the same exam as extra references in an ATS
      system. We propose various approaches of fusing information from multiple
      tasks and pass this authorship knowledge into our ATS model on six
      different datasets. We show that this can positively affect the model
      performance at a global level.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0536</url>
    <doi>10.18653/v1/W18-0536</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0537'>
    <title>SB@GU at the Complex Word Identification 2018 Shared Task</title>
    <author>
      <first>David</first>
      <last>Alfter</last>
    </author>
    <author>
      <first>Ildikó</first>
      <last>Pilán</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>315–321</pages>
    <abstract>
      In this paper, we describe our experiments for the Shared Task on Complex
      Word Identification (CWI) 2018 (Yimam et al., 2018), hosted by the 13th
      Workshop on Innovative Use of NLP for Building Educational Applications
      (BEA) at NAACL 2018. Our system for English builds on previous work for
      Swedish concerning the classification of words into proficiency levels. We
      investigate different features for English and compare their usefulness
      using feature selection methods. For the German, Spanish and French data
      we use simple systems based on character n-gram models and show that
      sometimes simple models achieve comparable results to fully
      feature-engineered systems.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0537</url>
    <doi>10.18653/v1/W18-0537</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>alfter-piln:2018:W18-05</bibkey>
  </paper>
  <paper id='0538'>
    <title>
      Complex Word Identification: Convolutional Neural Network vs. Feature
      Engineering
    </title>
    <author>
      <first>Segun Taofeek</first>
      <last>Aroyehun</last>
    </author>
    <author>
      <first>Jason</first>
      <last>Angel</last>
    </author>
    <author>
      <first>Daniel Alejandro</first>
      <last>Pérez Alvarez</last>
    </author>
    <author>
      <first>Alexander</first>
      <last>Gelbukh</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>322–327</pages>
    <abstract>
      We describe the systems of NLP-CIC team that participated in the Complex
      Word Identification (CWI) 2018 shared task. The shared task aimed to
      benchmark approaches for identifying complex words in English and other
      languages from the perspective of non-native speakers. Our goal is to
      compare two approaches: feature engineering and a deep neural network.
      Both approaches achieved comparable performance on the English test set.
      We demonstrated the flexibility of the deep-learning approach by using the
      same deep neural network setup in the Spanish track. Our systems achieved
      competitive results: all our systems were within 0.01 of the system with
      the best macro-F1 score on the test sets except on Wikipedia test set, on
      which our best system is 0.04 below the best macro-F1 score.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0538</url>
    <doi>10.18653/v1/W18-0538</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>aroyehun-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id='0539'>
    <title>Deep Learning Architecture for Complex Word Identification</title>
    <author>
      <first>Dirk</first>
      <last>De Hertog</last>
    </author>
    <author>
      <first>Anaïs</first>
      <last>Tack</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>328–334</pages>
    <abstract>
      We describe a system for the CWI-task that includes information on 5
      aspects of the (complex) lexical item, namely distributional information
      of the item itself, morphological structure, psychological measures,
      corpus-counts and topical information. We constructed a deep learning
      architecture that combines those features and apply it to the
      probabilistic and binary classification task for all English sets and
      Spanish. We achieved reasonable performance on all sets with best
      performances seen on the probabilistic task, particularly on the English
      news set (MAE 0.054 and F1-score of 0.872). An analysis of the results
      shows that reasonable performance can be achieved with a single
      architecture without any domain-specific tweaking of the parameter
      settings and that distributional features capture almost all of the
      information also found in hand-crafted features.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0539</url>
    <doi>10.18653/v1/W18-0539</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>dehertog-tack:2018:W18-05</bibkey>
  </paper>
  <paper id='0540'>
    <title>NILC at CWI 2018: Exploring Feature Engineering and Feature Learning</title>
    <author>
      <first>Nathan</first>
      <last>Hartmann</last>
    </author>
    <author>
      <first>Leandro Borges</first>
      <von>dos</von>
      <last>Santos</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>335–340</pages>
    <abstract>
      This paper describes the results of NILC team at CWI 2018. We developed
      solutions following three approaches: (i) a feature engineering method
      using lexical, n-gram and psycholinguistic features, (ii) a shallow neural
      network method using only word embeddings, and (iii) a Long Short-Term
      Memory (LSTM) language model, which is pre-trained on a large text corpus
      to produce a contextualized word vector. The feature engineering method
      obtained our best results for the classification task and the LSTM model
      achieved the best results for the probabilistic classification task. Our
      results show that deep neural networks are able to perform as well as
      traditional machine learning methods using manually engineered features
      for the task of complex word identification in English.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0540</url>
    <doi>10.18653/v1/W18-0540</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>hartmann-dossantos:2018:W18-05</bibkey>
  </paper>
  <paper id='0541'>
    <title>Complex Word Identification Using Character n-grams</title>
    <author>
      <first>Maja</first>
      <last>Popović</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>341–348</pages>
    <abstract>
      This paper investigates the use of character n-gram frequencies for
      identifying complex words in English, German and Spanish texts. The
      approach is based on the assumption that complex words are likely to
      contain different character sequences than simple words. The multinomial
      Naive Bayes classifier was used with n-grams of different lengths as
      features, and the best results were obtained for the combination of
      2-grams and 4-grams. This variant was submitted to the Complex Word
      Identification Shared Task 2018 for all texts and achieved F-scores
      between 70% and 83%. The system was ranked in the middle range for all
      English texts, as third of fourteen submissions for German, and as tenth
      of seventeen submissions for Spanish. The method is not very convenient
      for the cross-language task, achieving only 59% on the French text.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0541</url>
    <doi>10.18653/v1/W18-0541</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>popovi:2018:W18-05</bibkey>
  </paper>
  <paper id='0542'>
    <title>
      Predicting Second Language Learner Successes and Mistakes by Means of
      Conjunctive Features
    </title>
    <author>
      <first>Yves</first>
      <last>Bestgen</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>349–355</pages>
    <abstract>
      This paper describes the system developed by the Centre for English Corpus
      Linguistics for the 2018 Duolingo SLAM challenge. It aimed at predicting
      the successes and mistakes of second language learners on each of the
      words that compose the exercises they answered. Its main characteristic is
      to include conjunctive features, built by combining word ngrams with
      metadata about the user and the exercise. It achieved a relatively good
      performance, ranking fifth out of 15 systems. Complementary analyses
      carried out to gauge the contribution of the different sets of features to
      the performance confirmed the usefulness of the conjunctive features for
      the SLAM task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0542</url>
    <doi>10.18653/v1/W18-0542</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bestgen:2018:W18-05</bibkey>
  </paper>
  <paper id='0543'>
    <title>Feature Engineering for Second Language Acquisition Modeling</title>
    <author>
      <first>Guanliang</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Claudia</first>
      <last>Hauff</last>
    </author>
    <author>
      <first>Geert-Jan</first>
      <last>Houben</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>356–364</pages>
    <abstract>
      Knowledge tracing serves as a keystone in delivering personalized
      education. However, few works attempted to model students’ knowledge state
      in the setting of Second Language Acquisition. The Duolingo Shared Task on
      Second Language Acquisition Modeling provides students’ trace data that we
      extensively analyze and engineer features from for the task of predicting
      whether a student will correctly solve a vocabulary exercise. Our analyses
      of students’ learning traces reveal that factors like exercise format and
      engagement impact their exercise performance to a large extent. Overall,
      we extracted 23 different features as input to a Gradient Tree Boosting
      framework, which resulted in an AUC score of between 0.80 and 0.82 on the
      official test set.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0543</url>
    <doi>10.18653/v1/W18-0543</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen-hauff-houben:2018:W18-05</bibkey>
  </paper>
  <paper id='0544'>
    <title>TMU System for SLAM-2018</title>
    <author>
      <first>Masahiro</first>
      <last>Kaneko</last>
    </author>
    <author>
      <first>Tomoyuki</first>
      <last>Kajiwara</last>
    </author>
    <author>
      <first>Mamoru</first>
      <last>Komachi</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>365–369</pages>
    <abstract>
      We introduce the TMU systems for the second language acquisition modeling
      shared task 2018 (Settles et al., 2018). To model learner error patterns,
      it is necessary to maintain a considerable amount of information regarding
      the type of exercises learners have been learning in the past and the
      manner in which they answered them. Tracking an enormous learner’s
      learning history and their correct and mistaken answers is essential to
      predict the learner’s future mistakes. Therefore, we propose a model which
      tracks the learner’s learning history efficiently. Our systems ranked
      fourth in the English and Spanish subtasks, and fifth in the French
      subtask.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0544</url>
    <doi>10.18653/v1/W18-0544</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kaneko-kajiwara-komachi:2018:W18-05</bibkey>
  </paper>
  <paper id='0545'>
    <title>Deep Factorization Machines for Knowledge Tracing</title>
    <author>
      <first>Jill-Jênn</first>
      <last>Vie</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>370–373</pages>
    <abstract>
      This paper introduces our solution to the 2018 Duolingo Shared Task on
      Second Language Acquisition Modeling (SLAM). We used deep factorization
      machines, a wide and deep learning model of pairwise relationships between
      users, items, skills, and other entities considered. Our solution (AUC
      0.815) hopefully managed to beat the logistic regression baseline (AUC
      0.774) but not the top performing model (AUC 0.861) and reveals
      interesting strategies to build upon item response theory models.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0545</url>
    <doi>10.18653/v1/W18-0545</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>vie:2018:W18-05</bibkey>
  </paper>
  <paper id='0546'>
    <title>CLUF: a Neural Model for Second Language Acquisition Modeling</title>
    <author>
      <first>Shuyao</first>
      <last>Xu</last>
    </author>
    <author>
      <first>Jin</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Long</first>
      <last>Qin</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>374–380</pages>
    <abstract>
      Second Language Acquisition Modeling is the task to predict whether a
      second language learner would respond correctly in future exercises based
      on their learning history. In this paper, we propose a neural network
      based system to utilize rich contextual, linguistic and user information.
      Our neural model consists of a Context encoder, a Linguistic feature
      encoder, a User information encoder and a Format information encoder
      (CLUF). Furthermore, a decoder is introduced to combine such encoded
      features and make final predictions. Our system ranked in first place in
      the English track and second place in the Spanish and French track with an
      AUROC score of 0.861, 0.835 and 0.854 respectively.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0546</url>
    <doi>10.18653/v1/W18-0546</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>xu-chen-qin:2018:W18-05</bibkey>
  </paper>
  <paper id='0547'>
    <title>Neural sequence modelling for learner error prediction</title>
    <author>
      <first>Zheng</first>
      <last>Yuan</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>381–388</pages>
    <abstract>
      This paper describes our use of two recurrent neural network sequence
      models: sequence labelling and sequence-to-sequence models, for the
      prediction of future learner errors in our submission to the 2018 Duolingo
      Shared Task on Second Language Acquisition Modeling (SLAM). We show that
      these two models capture complementary information as combining them
      improves performance. Furthermore, the same network architecture and group
      of features can be used directly to build competitive prediction models in
      all three language tracks, demonstrating that our approach generalises
      well across languages.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0547</url>
    <doi>10.18653/v1/W18-0547</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>yuan:2018:W18-05</bibkey>
  </paper>
  <paper id='0548'>
    <title>
      Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept
      Embeddings and Information Retrieval
    </title>
    <author>
      <first>Le An</first>
      <last>Ha</last>
    </author>
    <author>
      <first>Victoria</first>
      <last>Yaneva</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>389–398</pages>
    <abstract>
      Developing plausible distractors (wrong answer options) when writing
      multiple-choice questions has been described as one of the most
      challenging and time-consuming parts of the item-writing process. In this
      paper we propose a fully automatic method for generating distractor
      suggestions for multiple-choice questions used in high-stakes medical
      exams. The system uses a question stem and the correct answer as an input
      and produces a list of suggested distractors ranked based on their
      similarity to the stem and the correct answer. To do this we use a novel
      approach of combining concept embeddings with information retrieval
      methods. We frame the evaluation as a prediction task where we aim to
      “predict” the human-produced distractors used in large sets of medical
      questions, i.e. if a distractor generated by our system is good enough it
      is likely to feature among the list of distractors produced by the human
      item-writers. The results reveal that combining concept embeddings with
      information retrieval approaches significantly improves the generation of
      plausible distractors and enables us to match around 1 in 5 of the
      human-produced distractors. The approach proposed in this paper is
      generalisable to all scenarios where the distractors refer to concepts.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0548</url>
    <doi>10.18653/v1/W18-0548</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ha-yaneva:2018:W18-05</bibkey>
  </paper>
  <paper id='0549'>
    <title>Co-Attention Based Neural Network for Source-Dependent Essay Scoring</title>
    <author>
      <first>Haoran</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Diane</first>
      <last>Litman</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>399–409</pages>
    <abstract>
      This paper presents an investigation of using a co-attention based neural
      network for source-dependent essay scoring. We use a co-attention
      mechanism to help the model learn the importance of each part of the essay
      more accurately. Also, this paper shows that the co-attention based neural
      network model provides reliable score prediction of source-dependent
      responses. We evaluate our model on two source-dependent response corpora.
      Results show that our model outperforms the baseline on both corpora. We
      also show that the attention of the model is similar to the expert
      opinions with examples.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0549</url>
    <doi>10.18653/v1/W18-0549</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-litman:2018:W18-05</bibkey>
  </paper>
  <paper id='0550'>
    <title>Cross-Lingual Content Scoring</title>
    <author>
      <first>Andrea</first>
      <last>Horbach</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Stennmanns</last>
    </author>
    <author>
      <first>Torsten</first>
      <last>Zesch</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>410–419</pages>
    <abstract>
      We investigate the feasibility of cross-lingual content scoring, a
      scenario where training and test data in an automatic scoring task are
      from two different languages. Cross-lingual scoring can contribute to
      educational equality by allowing answers in multiple languages. Training a
      model in one language and applying it to another language might also help
      to overcome data sparsity issues by re-using trained models from other
      languages. As there is no suitable dataset available for this new task, we
      create a comparable bi-lingual corpus by extending the English ASAP
      dataset with German answers. Our experiments with cross-lingual scoring
      based on machine-translating either training or test data show a
      considerable drop in scoring quality.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0550</url>
    <doi>10.18653/v1/W18-0550</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>horbach-stennmanns-zesch:2018:W18-05</bibkey>
  </paper>
  <paper id='0600'>
    <title>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </title>
    <editor>
      <first>Kate</first>
      <last>Loveys</last>
    </editor>
    <editor>
      <first>Kate</first>
      <last>Niederhoffer</last>
    </editor>
    <editor>
      <first>Emily</first>
      <last>Prud’hommeaux</last>
    </editor>
    <editor>
      <first>Rebecca</first>
      <last>Resnik</last>
    </editor>
    <editor>
      <first>Philip</first>
      <last>Resnik</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-06</url>
    <doi>10.18653/v1/W18-06</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-06:2018</bibkey>
  </paper>
  <paper id='0601'>
    <title>
      What type of happiness are you looking for? - A closer look at detecting
      mental health from language
    </title>
    <author>
      <first>Alina</first>
      <last>Arseniev-Koehler</last>
    </author>
    <author>
      <first>Sharon</first>
      <last>Mozgai</last>
    </author>
    <author>
      <first>Stefan</first>
      <last>Scherer</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–12</pages>
    <abstract>
      Computational models to detect mental illnesses from text and speech could
      enhance our understanding of mental health while offering opportunities
      for early detection and intervention. However, these models are often
      disconnected from the lived experience of depression and the larger
      diagnostic debates in mental health. This article investigates these
      disconnects, primarily focusing on the labels used to diagnose depression,
      how these labels are computationally represented, and the performance
      metrics used to evaluate computational models. We also consider how
      medical instruments used to measure depression, such as the Patient Health
      Questionnaire (PHQ), contribute to these disconnects. To illustrate our
      points, we incorporate mixed-methods analyses of 698 interviews on
      emotional health, which are coupled with self-report PHQ screens for
      depression. We propose possible strategies to bridge these gaps between
      modern psychiatric understandings of depression, lay experience of
      depression, and computational representation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0601</url>
    <doi>10.18653/v1/W18-0601</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>arsenievkoehler-mozgai-scherer:2018:W18-06</bibkey>
  </paper>
  <paper id='0602'>
    <title>A Linguistically-Informed Fusion Approach for Multimodal Depression Detection</title>
    <author>
      <first>Michelle</first>
      <last>Morales</last>
    </author>
    <author>
      <first>Stefan</first>
      <last>Scherer</last>
    </author>
    <author>
      <first>Rivka</first>
      <last>Levitan</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–24</pages>
    <abstract>
      Automated depression detection is inherently a multimodal problem.
      Therefore, it is critical that researchers investigate fusion techniques
      for multimodal design. This paper presents the first-ever comprehensive
      study of fusion techniques for depression detection. In addition, we
      present novel linguistically-motivated fusion techniques, which we find
      outperform existing approaches.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0602</url>
    <doi>10.18653/v1/W18-0602</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>morales-scherer-levitan:2018:W18-06</bibkey>
  </paper>
  <paper id='0603'>
    <title>
      Expert, Crowdsourced, and Machine Assessment of Suicide Risk via Online
      Postings
    </title>
    <author>
      <first>Han-Chin</first>
      <last>Shing</last>
    </author>
    <author>
      <first>Suraj</first>
      <last>Nair</last>
    </author>
    <author>
      <first>Ayah</first>
      <last>Zirikly</last>
    </author>
    <author>
      <first>Meir</first>
      <last>Friedenberg</last>
    </author>
    <author>
      <first>Hal</first>
      <last>Daumé III</last>
    </author>
    <author>
      <first>Philip</first>
      <last>Resnik</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–36</pages>
    <abstract>
      We report on the creation of a dataset for studying assessment of suicide
      risk via online postings in Reddit. Evaluation of risk-level annotations
      by experts yields what is, to our knowledge, the first demonstration of
      reliability in risk assessment by clinicians based on social media
      postings. We also introduce and demonstrate the value of a new, detailed
      rubric for assessing suicide risk, compare crowdsourced with expert
      performance, and present baseline predictive modeling experiments using
      the new dataset, which will be made available to researchers through the
      American Association of Suicidology.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0603</url>
    <doi>10.18653/v1/W18-0603</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>shing-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0604'>
    <title>
      CLPsych 2018 Shared Task: Predicting Current and Future Psychological
      Health from Childhood Essays
    </title>
    <author>
      <first>Veronica</first>
      <last>Lynn</last>
    </author>
    <author>
      <first>Alissa</first>
      <last>Goodman</last>
    </author>
    <author>
      <first>Kate</first>
      <last>Niederhoffer</last>
    </author>
    <author>
      <first>Kate</first>
      <last>Loveys</last>
    </author>
    <author>
      <first>Philip</first>
      <last>Resnik</last>
    </author>
    <author>
      <first>H. Andrew</first>
      <last>Schwartz</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–46</pages>
    <abstract>
      We describe the shared task for the CLPsych 2018 workshop, which focused
      on predicting current and future psychological health from an essay
      authored in childhood. Language-based predictions of a person’s current
      health have the potential to supplement traditional psychological
      assessment such as questionnaires, improving intake risk measurement and
      monitoring. Predictions of future psychological health can aid with both
      early detection and the development of preventative care. Research into
      the mental health trajectory of people, beginning from their childhood,
      has thus far been an area of little work within the NLP community. This
      shared task represents one of the first attempts to evaluate the use of
      early language to predict future health; this has the potential to support
      a wide variety of clinical health care tasks, from early assessment of
      lifetime risk for mental health problems, to optimal timing for targeted
      interventions aimed at both prevention and treatment.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0604</url>
    <doi>10.18653/v1/W18-0604</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>lynn-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0605'>
    <title>
      An Approach to the CLPsych 2018 Shared Task Using Top-Down Text
      Representation and Simple Bottom-Up Model Selection
    </title>
    <author>
      <first>Micah</first>
      <last>Iserman</last>
    </author>
    <author>
      <first>Molly</first>
      <last>Ireland</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Littlefield</last>
    </author>
    <author>
      <first>Tyler</first>
      <last>Davis</last>
    </author>
    <author>
      <first>Sage</first>
      <last>Maliepaard</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–56</pages>
    <abstract>
      The Computational Linguistics and Clinical Psychology (CLPsych) 2018
      Shared Task asked teams to predict cross-sectional indices of anxiety and
      distress, and longitudinal indices of psychological distress from a
      subsample of the National Child Development Study, started in the United
      Kingdom in 1958. Teams aimed to predict mental health outcomes from essays
      written by 11-year-olds about what they believed their lives would be like
      at age 25. In the hopes of producing results that could be easily
      disseminated and applied, we used largely theory-based dictionaries to
      process the texts, and a simple data-driven approach to model selection.
      This approach yielded only modest results in terms of out-of-sample
      accuracy, but most of the category-level findings are interpretable and
      consistent with existing literature on psychological distress, anxiety,
      and depression.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0605</url>
    <doi>10.18653/v1/W18-0605</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>iserman-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0606'>
    <title>
      Using contextual information for automatic triage of posts in a
      peer-support forum
    </title>
    <author>
      <first>Edgar</first>
      <last>Altszyler</last>
    </author>
    <author>
      <first>Ariel J.</first>
      <last>Berenstein</last>
    </author>
    <author>
      <first>David</first>
      <last>Milne</last>
    </author>
    <author>
      <first>Rafael A.</first>
      <last>Calvo</last>
    </author>
    <author>
      <first>Diego</first>
      <last>Fernandez Slezak</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–68</pages>
    <abstract>
      Mental health forums are online spaces where people can share their
      experiences anonymously and get peer support. These forums, require the
      supervision of moderators to provide support in delicate cases, such as
      posts expressing suicide ideation. The large increase in the number of
      forum users makes the task of the moderators unmanageable without the help
      of automatic triage systems. In the present paper, we present a Machine
      Learning approach for the triage of posts. Most approaches in the
      literature focus on the content of the posts, but only a few authors take
      advantage of features extracted from the context in which they appear. Our
      approach consists of the development and implementation of a large variety
      of new features from both, the content and the context of posts, such as
      previous messages, interaction with other users and author’s history. Our
      method has competed in the CLPsych 2017 Shared Task, obtaining the first
      place for several of the subtasks. Moreover, we also found that models
      that take advantage of post context improve significantly its performance
      in the detection of flagged posts (posts that require moderators
      attention), as well as those that focus on post content outperforms in the
      detection of most urgent events.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0606</url>
    <doi>10.18653/v1/W18-0606</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>altszyler-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0607'>
    <title>
      Hierarchical neural model with attention mechanisms for the classification
      of social media text related to mental health
    </title>
    <author>
      <first>Julia</first>
      <last>Ive</last>
    </author>
    <author>
      <first>George</first>
      <last>Gkotsis</last>
    </author>
    <author>
      <first>Rina</first>
      <last>Dutta</last>
    </author>
    <author>
      <first>Robert</first>
      <last>Stewart</last>
    </author>
    <author>
      <first>Sumithra</first>
      <last>Velupillai</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>69–77</pages>
    <abstract>
      Mental health problems represent a major public health challenge.
      Automated analysis of text related to mental health is aimed to help
      medical decision-making, public health policies and to improve health
      care. Such analysis may involve text classification. Traditionally,
      automated classification has been performed mainly using machine learning
      methods involving costly feature engineering. Recently, the performance of
      those methods has been dramatically improved by neural methods. However,
      mainly Convolutional neural networks (CNNs) have been explored. In this
      paper, we apply a hierarchical Recurrent neural network (RNN) architecture
      with an attention mechanism on social media data related to mental health.
      We show that this architecture improves overall classification results as
      compared to previously reported results on the same data. Benefitting from
      the attention mechanism, it can also efficiently select text elements
      crucial for classification decisions, which can also be used for in-depth
      analysis.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0607</url>
    <doi>10.18653/v1/W18-0607</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ive-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0608'>
    <title>Cross-cultural differences in language markers of depression online</title>
    <author>
      <first>Kate</first>
      <last>Loveys</last>
    </author>
    <author>
      <first>Jonathan</first>
      <last>Torrez</last>
    </author>
    <author>
      <first>Alex</first>
      <last>Fine</last>
    </author>
    <author>
      <first>Glen</first>
      <last>Moriarty</last>
    </author>
    <author>
      <first>Glen</first>
      <last>Coppersmith</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>78–87</pages>
    <abstract>
      Depression is a global mental health condition that affects all cultures.
      Despite this, the way depression is expressed varies by culture. Uptake of
      machine learning technology for diagnosing mental health conditions means
      that increasingly more depression classifiers are created from online
      language data. Yet, culture is rarely considered as a factor affecting
      online language in this literature. This study explores cultural
      differences in online language data of users with depression. Written
      language data from 1,593 users with self-reported depression from the
      online peer support community 7 Cups of Tea was analyzed using the
      Linguistic Inquiry and Word Count (LIWC), topic modeling, data
      visualization, and other techniques. We compared the language of users
      identifying as White, Black or African American, Hispanic or Latino, and
      Asian or Pacific Islander. Exploratory analyses revealed cross-cultural
      differences in depression expression in online language data, particularly
      in relation to emotion expression, cognition, and functioning. The results
      have important implications for avoiding depression misclassification from
      machine-driven assessments when used in a clinical setting, and for
      avoiding inadvertent cultural biases in this line of research more
      broadly.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0608</url>
    <doi>10.18653/v1/W18-0608</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>loveys-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0609'>
    <title>Deep Learning for Depression Detection of Twitter Users</title>
    <author>
      <first>Ahmed</first>
      <last>Husseini Orabi</last>
    </author>
    <author>
      <first>Prasadith</first>
      <last>Buddhitha</last>
    </author>
    <author>
      <first>Mahmoud</first>
      <last>Husseini Orabi</last>
    </author>
    <author>
      <first>Diana</first>
      <last>Inkpen</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>88–97</pages>
    <abstract>
      Mental illness detection in social media can be considered a complex task,
      mainly due to the complicated nature of mental disorders. In recent years,
      this research area has started to evolve with the continuous increase in
      popularity of social media platforms that became an integral part of
      people’s life. This close relationship between social media platforms and
      their users has made these platforms to reflect the users’ personal life
      with different limitations. In such an environment, researchers are
      presented with a wealth of information regarding one’s life. In addition
      to the level of complexity in identifying mental illnesses through social
      media platforms, adopting supervised machine learning approaches such as
      deep neural networks have not been widely accepted due to the difficulties
      in obtaining sufficient amounts of annotated training data. Due to these
      reasons, we try to identify the most effective deep neural network
      architecture among a few of selected architectures that were successfully
      used in natural language processing tasks. The chosen architectures are
      used to detect users with signs of mental illnesses (depression in our
      case) given limited unstructured text data extracted from the Twitter
      social media platform.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0609</url>
    <doi>10.18653/v1/W18-0609</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>husseiniorabi-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0610'>
    <title>
      Current and Future Psychological Health Prediction using Language and
      Socio-Demographics of Children for the CLPysch 2018 Shared Task
    </title>
    <author>
      <first>Sharath Chandra</first>
      <last>Guntuku</last>
    </author>
    <author>
      <first>Salvatore</first>
      <last>Giorgi</last>
    </author>
    <author>
      <first>Lyle</first>
      <last>Ungar</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>98–106</pages>
    <abstract>
      This article is a system description and report on the submission of a
      team from the University of Pennsylvania in the ’CLPsych 2018’ shared
      task. The goal of the shared task was to use childhood language as a
      marker for both current and future psychological health over individual
      lifetimes. Our system employs multiple textual features derived from the
      essays written and individuals’ socio-demographic variables at the age of
      11. We considered several word clustering approaches, and explore the use
      of linear regression based on different feature sets. Our approach showed
      best results for predicting distress at the age of 42 and for predicting
      current anxiety on Disattenuated Pearson Correlation, and ranked fourth in
      the future health prediction task. In addition to the subtasks presented,
      we attempted to provide insight into mental health aspects at different
      ages. Our findings indicate that misspellings, words with illegible
      letters and increased use of personal pronouns are correlated with poor
      mental health at age 11, while descriptions about future physical
      activity, family and friends are correlated with good mental health.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0610</url>
    <doi>10.18653/v1/W18-0610</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>guntuku-giorgi-ungar:2018:W18-06</bibkey>
  </paper>
  <paper id='0611'>
    <title>
      Predicting Psychological Health from Childhood Essays with Convolutional
      Neural Networks for the CLPsych 2018 Shared Task (Team UKNLP)
    </title>
    <author>
      <first>Anthony</first>
      <last>Rios</last>
    </author>
    <author>
      <first>Tung</first>
      <last>Tran</last>
    </author>
    <author>
      <first>Ramakanth</first>
      <last>Kavuluru</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>107–112</pages>
    <abstract>
      This paper describes the systems we developed for tasks A and B of the
      2018 CLPsych shared task. The first task (task A) focuses on predicting
      behavioral health scores at age 11 using childhood essays. The second task
      (task B) asks participants to predict future psychological distress at
      ages 23, 33, 42, and 50 using the age 11 essays. We propose two
      convolutional neural network based methods that map each task to a
      regression problem. Among seven teams we ranked third on task A with
      disattenuated Pearson correlation (DPC) score of 0.5587. Likewise, we
      ranked third on task B with an average DPC score of 0.3062.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0611</url>
    <doi>10.18653/v1/W18-0611</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rios-tran-kavuluru:2018:W18-06</bibkey>
  </paper>
  <paper id='0612'>
    <title>A Psychologically Informed Approach to CLPsych Shared Task 2018</title>
    <author>
      <first>Almog</first>
      <last>Simchon</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Gilead</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>113–118</pages>
    <abstract>
      This paper describes our approach to the CLPsych 2018 Shared Task, in
      which we attempted to predict cross-sectional psychological health at age
      11 and future psychological distress based on childhood essays. We
      attempted several modeling approaches and observed best cross-validated
      prediction accuracy with relatively simple models based on psychological
      theory. The models provided reasonable predictions in most outcomes.
      Notably, our model was especially successful in predicting out-of-sample
      psychological distress (across people and across time) at age 50.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0612</url>
    <doi>10.18653/v1/W18-0612</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>simchon-gilead:2018:W18-06</bibkey>
  </paper>
  <paper id='0613'>
    <title>
      Predicting Psychological Health from Childhood Essays. The UGent-IDLab
      CLPsych 2018 Shared Task System.
    </title>
    <author>
      <first>Klim</first>
      <last>Zaporojets</last>
    </author>
    <author>
      <first>Lucas</first>
      <last>Sterckx</last>
    </author>
    <author>
      <first>Johannes</first>
      <last>Deleu</last>
    </author>
    <author>
      <first>Thomas</first>
      <last>Demeester</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Develder</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119–125</pages>
    <abstract>
      This paper describes the IDLab system submitted to Task A of the CLPsych
      2018 shared task. The goal of this task is predicting psychological health
      of children based on language used in hand-written essays and
      socio-demographic control variables. Our entry uses word- and
      character-based features as well as lexicon-based features and features
      derived from the essays such as the quality of the language. We apply
      linear models, gradient boosting as well as neural-network based
      regressors (feed-forward, CNNs and RNNs) to predict scores. We then make
      ensembles of our best performing models using a weighted average.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0613</url>
    <doi>10.18653/v1/W18-0613</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zaporojets-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0614'>
    <title>
      Can adult mental health be predicted by childhood future-self narratives?
      Insights from the CLPsych 2018 Shared Task
    </title>
    <author>
      <first>Kylie</first>
      <last>Radford</last>
    </author>
    <author>
      <first>Louise</first>
      <last>Lavrencic</last>
    </author>
    <author>
      <first>Ruth</first>
      <last>Peters</last>
    </author>
    <author>
      <first>Kim</first>
      <last>Kiely</last>
    </author>
    <author>
      <first>Ben</first>
      <last>Hachey</last>
    </author>
    <author>
      <first>Scott</first>
      <last>Nowson</last>
    </author>
    <author>
      <first>Will</first>
      <last>Radford</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>126–135</pages>
    <abstract>
      The CLPsych 2018 Shared Task B explores how childhood essays can predict
      psychological distress throughout the author’s life. Our main aim was to
      build tools to help our psychologists understand the data, propose
      features and interpret predictions. We submitted two linear regression
      models: ModelA uses simple demographic and word- count features, while
      ModelB uses linguistic, entity, typographic, expert-gazetteer, and
      readability features. Our models perform best at younger prediction ages,
      with our best unofficial score at 23 of 0.426 disattenuated Pearson
      correlation. This task is challenging and although predictive performance
      is limited, we propose that tight integration of expertise across
      computational linguistics and clinical psychology is a productive
      direction.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0614</url>
    <doi>10.18653/v1/W18-0614</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>radford-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0615'>
    <title>Automatic Detection of Incoherent Speech for Diagnosing Schizophrenia</title>
    <author>
      <first>Dan</first>
      <last>Iter</last>
    </author>
    <author>
      <first>Jong</first>
      <last>Yoon</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Jurafsky</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>136–146</pages>
    <abstract>
      Schizophrenia is a mental disorder which afflicts an estimated 0.7\% of
      adults world wide. It affects many areas of mental function, often evident
      from incoherent speech. Diagnosing schizophrenia relies on subjective
      judgments resulting in disagreements even among trained clinicians. Recent
      studies have proposed the use of natural language processing for diagnosis
      by drawing on automatically-extracted linguistic features like discourse
      coherence and lexicon. Here, we present the first benchmark comparison of
      previously proposed coherence models for detecting symptoms of
      schizophrenia and evaluate their performance on a new dataset of recorded
      interviews between subjects and clinicians. We also present two
      alternative coherence metrics based on modern sentence embedding
      techniques that outperform the previous methods on our dataset. Lastly, we
      propose a novel computational model for reference incoherence based on
      ambiguous pronoun usage and show that it is a highly predictive feature on
      our data. While the number of subjects is limited in this pilot study, our
      results suggest new directions for diagnosing common symptoms of
      schizophrenia.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0615</url>
    <doi>10.18653/v1/W18-0615</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>iter-yoon-jurafsky:2018:W18-06</bibkey>
  </paper>
  <paper id='0616'>
    <title>
      Oral-Motor and Lexical Diversity During Naturalistic Conversations in
      Adults with Autism Spectrum Disorder
    </title>
    <author>
      <first>Julia</first>
      <last>Parish-Morris</last>
    </author>
    <author>
      <first>Evangelos</first>
      <last>Sariyanidi</last>
    </author>
    <author>
      <first>Casey</first>
      <last>Zampella</last>
    </author>
    <author>
      <first>G. Keith</first>
      <last>Bartley</last>
    </author>
    <author>
      <first>Emily</first>
      <last>Ferguson</last>
    </author>
    <author>
      <first>Ashley A.</first>
      <last>Pallathra</last>
    </author>
    <author>
      <first>Leila</first>
      <last>Bateman</last>
    </author>
    <author>
      <first>Samantha</first>
      <last>Plate</last>
    </author>
    <author>
      <first>Meredith</first>
      <last>Cola</last>
    </author>
    <author>
      <first>Juhi</first>
      <last>Pandey</last>
    </author>
    <author>
      <first>Edward S.</first>
      <last>Brodkin</last>
    </author>
    <author>
      <first>Robert T.</first>
      <last>Schultz</last>
    </author>
    <author>
      <first>Birkan</first>
      <last>Tunc</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>147–157</pages>
    <abstract>
      Autism spectrum disorder (ASD) is a neurodevelopmental condition
      characterized by impaired social communication and the presence of
      restricted, repetitive patterns of behaviors and interests. Prior research
      suggests that restricted patterns of behavior in ASD may be cross-domain
      phenomena that are evident in a variety of modalities. Computational
      studies of language in ASD provide support for the existence of an
      underlying dimension of restriction that emerges during a conversation.
      Similar evidence exists for restricted patterns of facial movement. Using
      tools from computational linguistics, computer vision, and information
      theory, this study tests whether cognitive-motor restriction can be
      detected across multiple behavioral domains in adults with ASD during a
      naturalistic conversation. Our methods identify restricted behavioral
      patterns, as measured by entropy in word use and mouth movement. Results
      suggest that adults with ASD produce significantly less diverse mouth
      movements and words than neurotypical adults, with an increased reliance
      on repeated patterns in both domains. The diversity values of the two
      domains are not significantly correlated, suggesting that they provide
      complementary information.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0616</url>
    <doi>10.18653/v1/W18-0616</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>parishmorris-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0617'>
    <title>Dynamics of an idiostyle of a Russian suicidal blogger</title>
    <author>
      <first>Tatiana</first>
      <last>Litvinova</last>
    </author>
    <author>
      <first>Olga</first>
      <last>Litvinova</last>
    </author>
    <author>
      <first>Pavel</first>
      <last>Seredin</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>158–167</pages>
    <abstract>
      Over 800000 people die of suicide each year. It is es-timated that by the
      year 2020, this figure will have in-creased to 1.5 million. It is
      considered to be one of the major causes of mortality during adolescence.
      Thus there is a growing need for methods of identifying su-icidal
      individuals. Language analysis is known to be a valuable psychodiagnostic
      tool, however the material for such an analysis is not easy to obtain.
      Currently as the Internet communications are developing, there is an
      opportunity to study texts of suicidal individuals. Such an analysis can
      provide a useful insight into the peculiarities of suicidal thinking,
      which can be used to further develop methods for diagnosing the risk of
      suicidal behavior. The paper analyzes the dynamics of a number of
      linguistic parameters of an idiostyle of a Russian-language blogger who
      died by suicide. For the first time such an analysis has been conducted
      using the material of Russian online texts. For text processing, the LIWC
      program is used. A correlation analysis was performed to identify the
      relationship between LIWC variables and number of days prior to suicide.
      Data visualization, as well as comparison with the results of related
      studies was performed.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0617</url>
    <doi>10.18653/v1/W18-0617</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>litvinova-litvinova-seredin:2018:W18-06</bibkey>
  </paper>
  <paper id='0618'>
    <title>RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses</title>
    <author>
      <first>Sean</first>
      <last>MacAvaney</last>
    </author>
    <author>
      <first>Bart</first>
      <last>Desmet</last>
    </author>
    <author>
      <first>Arman</first>
      <last>Cohan</last>
    </author>
    <author>
      <first>Luca</first>
      <last>Soldaini</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Yates</last>
    </author>
    <author>
      <first>Ayah</first>
      <last>Zirikly</last>
    </author>
    <author>
      <first>Nazli</first>
      <last>Goharian</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>168–173</pages>
    <abstract>
      Self-reported diagnosis statements have been widely employed in studying
      language related to mental health in social media. However, existing
      research has largely ignored the temporality of mental health diagnoses.
      In this work, we introduce RSDD-Time: a new dataset of 598 manually
      annotated self-reported depression diagnosis posts from Reddit that
      include temporal information about the diagnosis. Annotations include
      whether a mental health con- dition is present and how recently the
      diagnosis happened. Furthermore, we include exact temporal spans that
      relate to the date of diagnosis. This information is valuable for various
      computational methods to examine mental health through social media
      because one’s mental health state is not static. We also test several
      baseline classification and extraction approaches, which suggest that
      extracting temporal information from self-reported diagnosis statements is
      challenging.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0618</url>
    <doi>10.18653/v1/W18-0618</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>macavaney-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0619'>
    <title>Predicting Human Trustfulness from Facebook Language</title>
    <author>
      <first>Mohammadzaman</first>
      <last>Zamani</last>
    </author>
    <author>
      <first>Anneke</first>
      <last>Buffone</last>
    </author>
    <author>
      <first>H. Andrew</first>
      <last>Schwartz</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>174–181</pages>
    <abstract>
      Trustfulness — one’s general tendency to have confidence in unknown people
      or situations — predicts many important real-world outcomes such as mental
      health and likelihood to cooperate with others such as clinicians. While
      data-driven measures of interpersonal trust have previously been
      introduced, here, we develop the first language-based assessment of the
      personality trait of trustfulness by fitting one’s language to an accepted
      questionnaire-based trust score. Further, using trustfulness as a type of
      case study, we explore the role of questionnaire size as well as word
      count in developing language-based predictive models of users’
      psychological traits. We find that leveraging a longer questionnaire can
      yield greater test set accuracy, while, for training, we find it
      beneficial to include users who took smaller questionnaires which offers
      more observations for training. Similarly, after noting a decrease in
      individual prediction error as word count increased, we found a word
      count-weighted training scheme was helpful when there were very few users
      in the first place.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0619</url>
    <doi>10.18653/v1/W18-0619</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zamani-buffone-schwartz:2018:W18-06</bibkey>
  </paper>
  <paper id='0620'>
    <title>
      Within and Between-Person Differences in Language Used Across Anxiety
      Support and Neutral Reddit Communities
    </title>
    <author>
      <first>Molly</first>
      <last>Ireland</last>
    </author>
    <author>
      <first>Micah</first>
      <last>Iserman</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>182–193</pages>
    <abstract>
      Although many studies have distinguished between the social media language
      use of people who do and do not have a mental health condition,
      within-person context-sensitive comparisons (for example, analyzing
      individuals’ language use when seeking support or discussing neutral
      topics) are less common. Two dictionary-based analyses of Reddit
      communities compared (1) anxious individuals’ comments in anxiety support
      communities (e.g., /r/PanicParty) with the same users’ comments in neutral
      communities (e.g., /r/todayilearned), and, (2) within popular neutral
      communities, comments by members of anxiety subreddits with comments by
      other users. Each comparison yielded theory-consistent effects as well as
      unexpected results that suggest novel hypotheses to be tested in the
      future. Results have relevance for improving researchers’ and
      practitioners’ ability to unobtrusively assess anxiety symptoms in
      conversations that are not explicitly about mental health.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0620</url>
    <doi>10.18653/v1/W18-0620</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ireland-iserman:2018:W18-06</bibkey>
  </paper>
  <paper id='0621'>
    <title>
      Helping or Hurting? Predicting Changes in Users’ Risk of Self-Harm Through
      Online Community Interactions
    </title>
    <author>
      <first>Luca</first>
      <last>Soldaini</last>
    </author>
    <author>
      <first>Timothy</first>
      <last>Walsh</last>
    </author>
    <author>
      <first>Arman</first>
      <last>Cohan</last>
    </author>
    <author>
      <first>Julien</first>
      <last>Han</last>
    </author>
    <author>
      <first>Nazli</first>
      <last>Goharian</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>194–203</pages>
    <abstract>
      In recent years, online communities have formed around suicide and
      self-harm prevention. While these communities offer support in moment of
      crisis, they can also normalize harmful behavior, discourage professional
      treatment, and instigate suicidal ideation. In this work, we focus on how
      interaction with others in such a community affects the mental state of
      users who are seeking support. We first build a dataset of conversation
      threads between users in a distressed state and community members offering
      support. We then show how to construct a classifier to predict whether
      distressed users are helped or harmed by the interactions in the thread,
      and we achieve a macro-F1 score of up to 0.69.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0621</url>
    <doi>10.18653/v1/W18-0621</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>soldaini-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id='0700'>
    <title>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </title>
    <editor>
      <first>Massimo</first>
      <last>Poesio</last>
    </editor>
    <editor>
      <first>Vincent</first>
      <last>Ng</last>
    </editor>
    <editor>
      <first>Maciej</first>
      <last>Ogrodniczuk</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-07</url>
    <doi>10.18653/v1/W18-07</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-07:2018</bibkey>
  </paper>
  <paper id='0701'>
    <title>Anaphora Resolution for Twitter Conversations: An Exploratory Study</title>
    <author>
      <first>Berfin</first>
      <last>Aktaş</last>
    </author>
    <author>
      <first>Tatjana</first>
      <last>Scheffler</last>
    </author>
    <author>
      <first>Manfred</first>
      <last>Stede</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <abstract>
      We present a corpus study of pronominal anaphora on Twitter conversations.
      After outlining the specific features of this genre, with respect to
      reference resolution, we explain the construction of our corpus and the
      annotation steps. From this we derive a list of phenomena that need to be
      considered when performing anaphora resolution on this type of data.
      Finally, we test the performance of an off-the-shelf resolution system,
      and provide some qualitative error analysis.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0701</url>
    <doi>10.18653/v1/W18-0701</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>akta-scheffler-stede:2018:W18-07</bibkey>
  </paper>
  <paper id='0702'>
    <title>Anaphora Resolution with the ARRAU Corpus</title>
    <author>
      <first>Massimo</first>
      <last>Poesio</last>
    </author>
    <author>
      <first>Yulia</first>
      <last>Grishina</last>
    </author>
    <author>
      <first>Varada</first>
      <last>Kolhatkar</last>
    </author>
    <author>
      <first>Nafise</first>
      <last>Moosavi</last>
    </author>
    <author>
      <first>Ina</first>
      <last>Roesiger</last>
    </author>
    <author>
      <first>Adam</first>
      <last>Roussel</last>
    </author>
    <author>
      <first>Fabian</first>
      <last>Simonjetz</last>
    </author>
    <author>
      <first>Alexandra</first>
      <last>Uma</last>
    </author>
    <author>
      <first>Olga</first>
      <last>Uryupina</last>
    </author>
    <author>
      <first>Juntao</first>
      <last>Yu</last>
    </author>
    <author>
      <first>Heike</first>
      <last>Zinsmeister</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–22</pages>
    <abstract>
      The ARRAU corpus is an anaphorically annotated corpus of English providing
      rich linguistic information about anaphora resolution. The most
      distinctive feature of the corpus is the annotation of a wide range of
      anaphoric relations, including bridging references and discourse deixis in
      addition to identity (coreference). Other distinctive features include
      treating all NPs as markables, including non-referring NPs; and the
      annotation of a variety of morphosyntactic and semantic mention and entity
      attributes, including the genericity status of the entities referred to by
      markables. The corpus however has not been extensively used for anaphora
      resolution research so far. In this paper, we discuss three datasets
      extracted from the ARRAU corpus to support the three subtasks of the CRAC
      2018 Shared Task–identity anaphora resolution over ARRAU-style markables,
      bridging references resolution, and discourse deixis; the evaluation
      scripts assessing system performance on those datasets; and preliminary
      results on these three tasks that may serve as baseline for subsequent
      research in these phenomena.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0702</url>
    <doi>10.18653/v1/W18-0702</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>poesio-EtAl:2018:W18-07</bibkey>
  </paper>
  <paper id='0703'>
    <title>Rule- and Learning-based Methods for Bridging Resolution in the ARRAU Corpus</title>
    <author>
      <first>Ina</first>
      <last>Roesiger</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>23–33</pages>
    <abstract>
      We present two systems for bridging resolution, which we submitted to the
      CRAC shared task on bridging anaphora resolution in the ARRAU corpus
      (track 2): a rule-based approach following Hou et al. 2014 and a
      learning-based approach. The re-implementation of Hou et al. 2014 achieves
      very poor performance when being applied to ARRAU. We found that the
      reasons for this lie in the different bridging annotations: whereas the
      rule-based system suggests many referential bridging pairs, ARRAU contains
      mostly lexical bridging. We describe the differences between these two
      types of bridging and adapt the rule-based approach to be able to handle
      lexical bridging. The modified rule-based approach achieves reasonable
      performance on all (sub)-tasks and outperforms a simple learning-based
      approach.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0703</url>
    <doi>10.18653/v1/W18-0703</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>roesiger:2018:W18-07</bibkey>
  </paper>
  <paper id='0704'>
    <title>A Predictive Model for Notional Anaphora in English</title>
    <author>
      <first>Amir</first>
      <last>Zeldes</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34–43</pages>
    <abstract>
      Notional anaphors are pronouns which disagree with their antecedents’
      grammatical categories for notional reasons, such as plural to singular
      agreement in: "the government ... they". Since such cases are rare and
      conflict with evidence from strictly agreeing cases ("the government ...
      it"), they present a substantial challenge to both coreference resolution
      and referring expression generation. Using the OntoNotes corpus, this
      paper takes an ensemble approach to predicting English notional anaphora
      in context on the basis of the largest empirical data to date. In addition
      to state of the art prediction accuracy, the results suggest that
      theoretical approaches positing a plural construal at the antecedent’s
      utterance are insufficient, and that circumstances at the anaphor’s
      utterance location, as well as global factors such as genre, have a strong
      effect on the choice of referring expression.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0704</url>
    <doi>10.18653/v1/W18-0704</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zeldes:2018:W18-07</bibkey>
  </paper>
  <paper id='0705'>
    <title>
      Integrating Predictions from Neural-Network Relation Classifiers into
      Coreference and Bridging Resolution
    </title>
    <author>
      <first>Ina</first>
      <last>Roesiger</last>
    </author>
    <author>
      <first>Maximilian</first>
      <last>Köper</last>
    </author>
    <author>
      <first>Kim Anh</first>
      <last>Nguyen</last>
    </author>
    <author>
      <first>Sabine</first>
      <last>Schulte im Walde</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>44–49</pages>
    <abstract>
      Cases of coreference and bridging resolution often require knowledge about
      semantic relations between anaphors and antecedents. We suggest
      state-of-the-art neural-network classifiers trained on relation benchmarks
      to predict and integrate likelihoods for relations. Two experiments with
      representations differing in noise and complexity improve our bridging but
      not our coreference resolver.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0705</url>
    <doi>10.18653/v1/W18-0705</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>roesiger-EtAl:2018:W18-07</bibkey>
  </paper>
  <paper id='0706'>
    <title>Towards Bridging Resolution in German: Data Analysis and Rule-based Experiments</title>
    <author>
      <first>Janis</first>
      <last>Pagel</last>
    </author>
    <author>
      <first>Ina</first>
      <last>Roesiger</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>50–60</pages>
    <abstract>
      Bridging resolution is the task of recognising bridging anaphors and
      linking them to their antecedents. While there is some work on bridging
      resolution for English, there is only little work for German. We present
      two datasets which contain bridging annotations, namely DIRNDL and GRAIN,
      and compare the performance of a rule-based system with a simple baseline
      approach on these two corpora. The performance for full bridging
      resolution ranges between an F1 score of 13.6% for DIRNDL and 11.8% for
      GRAIN. An analysis using oracle lists suggests that the system could, to a
      certain extent, benefit from ranking and re-ranking antecedent candidates.
      Furthermore, we investigate the importance of single features and show
      that the features used in our work seem promising for future bridging
      resolution approaches.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0706</url>
    <doi>10.18653/v1/W18-0706</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>pagel-roesiger:2018:W18-07</bibkey>
  </paper>
  <paper id='0707'>
    <title>Detecting and Resolving Shell Nouns in German</title>
    <author>
      <first>Adam</first>
      <last>Roussel</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>61–67</pages>
    <abstract>
      This paper describes the design and evaluation of a system for the
      automatic detection and resolution of shell nouns in German. Shell nouns
      are general nouns, such as fact, question, or problem, whose full
      interpretation relies on a content phrase located elsewhere in a text,
      which these nouns simultaneously serve to characterize and encapsulate. To
      accomplish this, the system uses a series of lexico-syntactic patterns in
      order to extract shell noun candidates and their content in parallel. Each
      pattern has its own classifier, which makes the final decision as to
      whether or not a link is to be established and the shell noun resolved.
      Overall, about 26.2\% of the annotated shell noun instances were correctly
      identified by the system, and of these cases, about 72.5\% are assigned
      the correct content phrase. Though it remains difficult to identify shell
      noun instances reliably (recall is accordingly low in this regard), this
      system usually assigns the right content to correctly classified cases.
      cases.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0707</url>
    <doi>10.18653/v1/W18-0707</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>roussel:2018:W18-07</bibkey>
  </paper>
  <paper id='0708'>
    <title>PAWS: A Multi-lingual Parallel Treebank with Anaphoric Relations</title>
    <author>
      <first>Anna</first>
      <last>Nedoluzhko</last>
    </author>
    <author>
      <first>Michal</first>
      <last>Novák</last>
    </author>
    <author>
      <first>Maciej</first>
      <last>Ogrodniczuk</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>68–76</pages>
    <abstract>
      We present PAWS, a multi-lingual parallel treebank with coreference
      annotation. It consists of English texts from the Wall Street Journal
      translated into Czech, Russian and Polish. In addition, the texts are
      syntactically parsed and word-aligned. PAWS is based on PCEDT 2.0 and
      continues the tradition of multilingual treebanks with coreference
      annotation. The paper focuses on the coreference annotation in PAWS and
      its language-specific differences. PAWS offers linguistic material that
      can be further leveraged in cross-lingual studies, especially on
      coreference.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0708</url>
    <doi>10.18653/v1/W18-0708</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>nedoluzhko-novk-ogrodniczuk:2018:W18-07</bibkey>
  </paper>
  <paper id='0709'>
    <title>A Fine-grained Large-scale Analysis of Coreference Projection</title>
    <author>
      <first>Michal</first>
      <last>Novák</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>77–86</pages>
    <abstract>
      We perform a fine-grained large-scale analysis of coreference projection.
      By projecting gold coreference from Czech to English and vice versa on
      Prague Czech-English Dependency Treebank 2.0 Coref, we set an upper bound
      of a proposed projection approach for these two languages. We undertake a
      detailed thorough analysis that combines the analysis of projection’s
      subtasks with analysis of performance on individual mention types. The
      findings are accompanied with examples from the corpus.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0709</url>
    <doi>10.18653/v1/W18-0709</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>novk:2018:W18-07</bibkey>
  </paper>
  <paper id='0710'>
    <title>
      Modeling Brain Activity Associated with Pronoun Resolution in English and
      Chinese
    </title>
    <author>
      <first>Jixing</first>
      <last>Li</last>
    </author>
    <author>
      <first>Murielle</first>
      <last>Fabre</last>
    </author>
    <author>
      <first>Wen-Ming</first>
      <last>Luh</last>
    </author>
    <author>
      <first>John</first>
      <last>Hale</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–96</pages>
    <abstract>
      Typological differences between English and Chinese suggest stronger
      reliance on salience of the antecedent during pronoun resolution in
      Chinese. We examined this hypothesis by correlating a difficulty measure
      of pronoun resolution derived by the activation-based ACT-R model with the
      brain activity of English and Chinese participants listening to a same
      audiobook during fMRI recording. The ACT-R model predicts higher overall
      difficulty for English speakers, which is supported at the brain level in
      left Broca’s area. More generally, it confirms that computational modeling
      approach is able to dissociate different dimensions that are involved in
      the complex process of pronoun resolution in the brain.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0710</url>
    <doi>10.18653/v1/W18-0710</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>li-EtAl:2018:W18-07</bibkey>
  </paper>
  <paper id='0711'>
    <title>
      Event versus entity co-reference: Effects of context and form of referring
      expression
    </title>
    <author>
      <first>Sharid</first>
      <last>Loáiciga</last>
    </author>
    <author>
      <first>Luca</first>
      <last>Bevacqua</last>
    </author>
    <author>
      <first>Hannah</first>
      <last>Rohde</last>
    </author>
    <author>
      <first>Christian</first>
      <last>Hardmeier</last>
    </author>
    <booktitle>
      Proceedings of the First Workshop on Computational Models of Reference,
      Anaphora and Coreference
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>97–103</pages>
    <abstract>
      Anaphora resolution systems require both an enumeration of possible
      candidate antecedents and an identification process of the antecedent.
      This paper focuses on (i) the impact of the form of referring expression
      on entity-vs-event preferences and (ii) how properties of the passage
      interact with referential form. Two crowd-sourced story-continuation
      experiments were conducted, using constructed and naturally-occurring
      passages, to see how participants interpret 
      <i>
        It and This pronouns following a context sentence that makes available
        event and entity referents. Our participants show a strong, but not
        categorical, bias to use This to refer to events and It
      </i>
       to refer to entities. However, these preferences vary with passage
      characteristics such as verb class (a proxy in our constructed examples
      for the number of explicit and implicit entities) and more subtle author
      intentions regarding subsequent re-mention (the original event-vs-entity
      re-mention of our corpus items).
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0711</url>
    <doi>10.18653/v1/W18-0711</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>loiciga-EtAl:2018:W18-07</bibkey>
  </paper>
  <paper id='0800'>
    <title>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</title>
    <editor>
      <first>Mark</first>
      <last>Alfano</last>
    </editor>
    <editor>
      <first>Dirk</first>
      <last>Hovy</last>
    </editor>
    <editor>
      <first>Margaret</first>
      <last>Mitchell</last>
    </editor>
    <editor>
      <first>Michael</first>
      <last>Strube</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-08</url>
    <doi>10.18653/v1/W18-08</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-08:2018</bibkey>
  </paper>
  <paper id='0801'>
    <title>
      On the Utility of Lay Summaries and AI Safety Disclosures: Toward Robust,
      Open Research Oversight
    </title>
    <author>
      <first>Allen</first>
      <last>Schmaltz</last>
    </author>
    <booktitle>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–6</pages>
    <abstract>
      In this position paper, we propose that the community consider encouraging
      researchers to include two riders, a ”Lay Summary” and an ”AI Safety
      Disclosure”, as part of future NLP papers published in ACL forums that
      present user-facing systems. The goal is to encourage researchers–via a
      relatively non-intrusive mechanism–to consider the societal implications
      of technologies carrying (un)known and/or (un)knowable long-term risks, to
      highlight failure cases, and to provide a mechanism by which the general
      public (and scientists in other disciplines) can more readily engage in
      the discussion in an informed manner. This simple proposal requires
      minimal additional up-front costs for researchers; the lay summary, at
      least, has significant precedence in the medical literature and other
      areas of science; and the proposal is aimed to supplement, rather than
      replace, existing approaches for encouraging researchers to consider the
      ethical implications of their work, such as those of the Collaborative
      Institutional Training Initiative (CITI) Program and institutional review
      boards (IRBs).
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0801</url>
    <doi>10.18653/v1/W18-0801</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>schmaltz:2018:W18-08</bibkey>
  </paper>
  <paper id='0802'>
    <title>#MeToo Alexa: How Conversational Systems Respond to Sexual Harassment</title>
    <author>
      <first>Amanda</first>
      <last>Cercas Curry</last>
    </author>
    <author>
      <first>Verena</first>
      <last>Rieser</last>
    </author>
    <booktitle>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>7–14</pages>
    <abstract>
      Conversational AI systems, such as Amazon’s Alexa, are rapidly developing
      from purely transactional systems to social chatbots, which can respond to
      a wide variety of user requests. In this article, we establish how current
      state-of-the-art conversational systems react to inappropriate requests,
      such as bullying and sexual harassment on the part of the user, by
      collecting and analysing the novel #MeTooAlexa corpus. Our results show
      that commercial systems mainly avoid answering, while rule-based chatbots
      show a variety of behaviours and often deflect. Data-driven systems, on
      the other hand, are often non-coherent, but also run the risk of being
      interpreted as flirtatious and sometimes react with counter-aggression.
      This includes our own system, trained on “clean” data, which suggests that
      inappropriate system behaviour is not caused by data bias.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0802</url>
    <doi>10.18653/v1/W18-0802</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>cercascurry-rieser:2018:W18-08</bibkey>
  </paper>
  <paper id='0900'>
    <title>Proceedings of the Workshop on Figurative Language Processing</title>
    <editor>
      <first>Beata Beigman</first>
      <last>Klebanov</last>
    </editor>
    <editor>
      <first>Ekaterina</first>
      <last>Shutova</last>
    </editor>
    <editor>
      <first>Patricia</first>
      <last>Lichtenstein</last>
    </editor>
    <editor>
      <first>Smaranda</first>
      <last>Muresan</last>
    </editor>
    <editor>
      <first>Chee</first>
      <last>Wee</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-09</url>
    <doi>10.18653/v1/W18-09</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-09:2018</bibkey>
  </paper>
  <paper id='0901'>
    <title>Challenges in Finding Metaphorical Connections</title>
    <author>
      <first>Katy</first>
      <last>Gero</last>
    </author>
    <author>
      <first>Lydia</first>
      <last>Chilton</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–6</pages>
    <abstract>
      Poetry is known for its novel expression using figurative language. We
      introduce a writing task that contains the essential challenges of
      generating meaningful figurative language and can be evaluated. We
      investigate how to find metaphorical connections between abstract themes
      and concrete domains by asking people to write four-line poems on a given
      metaphor, such as "death is a rose" or "anger is wood". We find that only
      21% of poems successfully make a metaphorical connection. We present five
      alternate ways people respond to the prompt and release our dataset of 100
      categorized poems. We suggest opportunities for computational approaches.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0901</url>
    <doi>10.18653/v1/W18-0901</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gero-chilton:2018:W18-09</bibkey>
  </paper>
  <paper id='0902'>
    <title>Linguistic Features of Sarcasm and Metaphor Production Quality</title>
    <author>
      <first>Stephen</first>
      <last>Skalicky</last>
    </author>
    <author>
      <first>Scott</first>
      <last>Crossley</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>7–16</pages>
    <abstract>
      Using linguistic features to detect figurative language has provided a
      deeper in-sight into figurative language. The purpose of this study is to
      assess whether linguistic features can help explain differences in quality
      of figurative language. In this study a large corpus of metaphors and
      sarcastic responses are collected from human subjects and rated for
      figurative language quality based on theoretical components of metaphor,
      sarcasm, and creativity. Using natural language processing tools, specific
      linguistic features related to lexical sophistication and semantic
      cohesion were used to predict the human ratings of figurative language
      quality. Results demonstrate linguistic features were able to predict
      small amounts of variance in metaphor and sarcasm production quality.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0902</url>
    <doi>10.18653/v1/W18-0902</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>skalicky-crossley:2018:W18-09</bibkey>
  </paper>
  <paper id='0903'>
    <title>Leveraging Syntactic Constructions for Metaphor Identification</title>
    <author>
      <first>Kevin</first>
      <last>Stowe</last>
    </author>
    <author>
      <first>Martha</first>
      <last>Palmer</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–26</pages>
    <abstract>
      Identification of metaphoric language in text is critical for generating
      effective semantic representations for natural language understanding.
      Computational approaches to metaphor identification have largely relied on
      heuristic based models or feature-based machine learning, using
      hand-crafted lexical resources coupled with basic syntactic information.
      However, recent work has shown the predictive power of syntactic
      constructions in determining metaphoric source and target domains
      (Sullivan 2013). Our work intends to explore syntactic constructions and
      their relation to metaphoric language. We undertake a corpus-based
      analysis of predicate-argument constructions and their metaphoric
      properties, and attempt to effectively represent syntactic constructions
      as features for metaphor processing, both in identifying source and target
      domains and in distinguishing metaphoric words from non-metaphoric.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0903</url>
    <doi>10.18653/v1/W18-0903</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>stowe-palmer:2018:W18-09</bibkey>
  </paper>
  <paper id='0904'>
    <title>
      Literal, Metphorical or Both? Detecting Metaphoricity in Isolated
      Adjective-Noun Phrases
    </title>
    <author>
      <first>Agnieszka</first>
      <last>Mykowiecka</last>
    </author>
    <author>
      <first>Malgorzata</first>
      <last>Marciniak</last>
    </author>
    <author>
      <first>Aleksander</first>
      <last>Wawer</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–33</pages>
    <abstract>
      The paper addresses the classification of isolated Polish adjective-noun
      phrases according to their metaphoricity. We tested neural networks to
      predict if a phrase has a literal or metaphorical sense or can have both
      senses depending on usage. The input to the neural network consists of
      word embeddings, but we also tested the impact of information about the
      domain of the adjective and about the abstractness of the noun. We applied
      our solution to English data available on the Internet and compared it to
      results published in papers. We found that the solution based on word
      embeddings only can achieve results comparable with complex solutions
      requiring additional information.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0904</url>
    <doi>10.18653/v1/W18-0904</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>mykowiecka-marciniak-wawer:2018:W18-09</bibkey>
  </paper>
  <paper id='0905'>
    <title>Catching Idiomatic Expressions in EFL Essays</title>
    <author>
      <first>Michael</first>
      <last>Flor</last>
    </author>
    <author>
      <first>Beata</first>
      <last>Beigman Klebanov</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34–44</pages>
    <abstract>
      This paper presents an exploratory study on large-scale detection of
      idiomatic expressions in essays written by non-native speakers of English.
      We describe a computational search procedure for automatic detection of
      idiom-candidate phrases in essay texts. The study used a corpus of essays
      written during a standardized examination of English language proficiency.
      Automatically-flagged candidate expressions were manually annotated for
      idiomaticity. The study found that idioms are widely used in EFL essays.
      The study also showed that a search algorithm that accommodates the
      syntactic and lexical exibility of idioms can increase the recall of idiom
      instances by 30%, but it also increases the amount of false positives.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0905</url>
    <doi>10.18653/v1/W18-0905</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>flor-beigmanklebanov:2018:W18-09</bibkey>
  </paper>
  <paper id='0906'>
    <title>Predicting Human Metaphor Paraphrase Judgments with Deep Neural Networks</title>
    <author>
      <first>Yuri</first>
      <last>Bizzoni</last>
    </author>
    <author>
      <first>Shalom</first>
      <last>Lappin</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45–55</pages>
    <abstract>
      We propose a new annotated corpus for metaphor interpretation by
      paraphrase, and a novel DNN model for performing this task. Our corpus
      consists of 200 sets of 5 sentences, with each set containing one
      reference metaphorical sentence, and four ranked candidate paraphrases.
      Our model is trained for a binary classification of paraphrase candidates,
      and then used to predict graded paraphrase acceptability. It reaches an
      encouraging 75\% accuracy on the binary classification task, and high
      Pearson (.75) and Spearman (.68) correlations on the gradient judgment
      prediction task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0906</url>
    <doi>10.18653/v1/W18-0906</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bizzoni-lappin:2018:W18-09</bibkey>
  </paper>
  <paper id='0907'>
    <title>A Report on the 2018 VUA Metaphor Detection Shared Task</title>
    <author>
      <first>Chee Wee (Ben)</first>
      <last>Leong</last>
    </author>
    <author>
      <first>Beata</first>
      <last>Beigman Klebanov</last>
    </author>
    <author>
      <first>Ekaterina</first>
      <last>Shutova</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–66</pages>
    <abstract>
      As the community working on computational approaches to figurative
      language is growing and as methods and data become increasingly diverse,
      it is important to create widely shared empirical knowledge of the level
      of system performance in a range of contexts, thus facilitating progress
      in this area. One way of creating such shared knowledge is through
      benchmarking multiple systems on a common dataset. We report on the shared
      task on metaphor identification on the VU Amsterdam Metaphor Corpus
      conducted at the NAACL 2018 Workshop on Figurative Language Processing.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0907</url>
    <doi>10.18653/v1/W18-0907</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>leong-beigmanklebanov-shutova:2018:W18-09</bibkey>
  </paper>
  <paper id='0908'>
    <title>An LSTM-CRF Based Approach to Token-Level Metaphor Detection</title>
    <author>
      <first>Malay</first>
      <last>Pramanick</last>
    </author>
    <author>
      <first>Ashim</first>
      <last>Gupta</last>
    </author>
    <author>
      <first>Pabitra</first>
      <last>Mitra</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–75</pages>
    <abstract>
      Automatic processing of figurative languages is gaining popularity in NLP
      community for their ubiquitous nature and increasing volume. In this era
      of web 2.0, automatic analysis of sarcasm and metaphors is important for
      their extensive usage. Metaphors are a part of figurative language that
      compares different concepts, often on a cognitive level. Many approaches
      have been proposed for automatic detection of metaphors, even using
      sequential models or neural networks. In this paper, we propose a method
      for detection of metaphors at the token level using a hybrid model of
      Bidirectional-LSTM and CRF. We used fewer features, as compared to the
      previous state-of-the-art sequential model. On experimentation with VUAMC,
      our method obtained an F-score of 0.674.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0908</url>
    <doi>10.18653/v1/W18-0908</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>pramanick-gupta-mitra:2018:W18-09</bibkey>
  </paper>
  <paper id='0909'>
    <title>Unsupervised Detection of Metaphorical Adjective-Noun Pairs</title>
    <author>
      <first>Malay</first>
      <last>Pramanick</last>
    </author>
    <author>
      <first>Pabitra</first>
      <last>Mitra</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–80</pages>
    <abstract>
      Metaphor is a popular figure of speech. Popularity of metaphors calls for
      their automatic identification and interpretation. Most of the
      unsupervised methods directed at detection of metaphors use some
      hand-coded knowledge. We propose an unsupervised framework for metaphor
      detection that does not require any hand-coded knowledge. We applied
      clustering on features derived from Adjective-Noun pairs for classifying
      them into two disjoint classes. We experimented with adjective-noun pairs
      of a popular dataset annotated for metaphors and obtained an accuracy of
      72.87% with k-means clustering algorithm.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0909</url>
    <doi>10.18653/v1/W18-0909</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>pramanick-mitra:2018:W18-09</bibkey>
  </paper>
  <paper id='0910'>
    <title>
      Phrase-Level Metaphor Identification Using Distributed Representations of
      Word Meaning
    </title>
    <author>
      <first>Omnia</first>
      <last>Zayed</last>
    </author>
    <author>
      <first>John Philip</first>
      <last>McCrae</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Buitelaar</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>81–90</pages>
    <abstract>
      Metaphor is an essential element of human cognition which is often used to
      express ideas and emotions that might be difficult to express using
      literal language. Processing metaphoric language is a challenging task for
      a wide range of applications ranging from text simplification to
      psychotherapy. Despite the variety of approaches that are trying to
      process metaphor, there is still a need for better models that mimic the
      human cognition while exploiting fewer resources. In this paper, we
      present an approach based on distributional semantics to identify
      metaphors on the phrase-level. We investigated the use of different word
      embeddings models to identify verb-noun pairs where the verb is used
      metaphorically. Several experiments are conducted to show the performance
      of the proposed approach on benchmark datasets.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0910</url>
    <doi>10.18653/v1/W18-0910</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zayed-mccrae-buitelaar:2018:W18-09</bibkey>
  </paper>
  <paper id='0911'>
    <title>Bigrams and BiLSTMs Two Neural Networks for Sequential Metaphor Detection</title>
    <author>
      <first>Yuri</first>
      <last>Bizzoni</last>
    </author>
    <author>
      <first>Mehdi</first>
      <last>Ghanimifard</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–101</pages>
    <abstract>
      We present and compare two alternative deep neural architectures to
      perform word-level metaphor detection on text: a bi-LSTM model and a new
      structure based on recursive feed-forward concatenation of the input. We
      discuss different versions of such models and the effect that input
      manipulation - specifically, reducing the length of sentences and
      introducing concreteness scores for words - have on their performance.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0911</url>
    <doi>10.18653/v1/W18-0911</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bizzoni-ghanimifard:2018:W18-09</bibkey>
  </paper>
  <paper id='0912'>
    <title>
      Computationally Constructed Concepts: A Machine Learning Approach to
      Metaphor Interpretation Using Usage-Based Construction Grammatical Cues
    </title>
    <author>
      <first>Zachary</first>
      <last>Rosen</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102–109</pages>
    <abstract>
      The current study seeks to implement a deep learning classification
      algorithm us- ing argument-structure level representation of metaphoric
      constructions, for the identifica- tion of source domain mappings in
      metaphoric utterances. It thus builds on previous work in computational
      metaphor interpretation (Mohler et al. 2014; Shutova 2010; Bolle-gala
      & Shutova 2013; Hong 2016; Su et al. 2017) while implementing a
      theoretical frame- work based off of work in the interface of metaphor and
      construction grammar (Sullivan 2006, 2007, 2013). The results indicate
      that it is possible to achieve an accuracy of approx- imately 80.4% using
      the proposed method, combining construction grammatical features with a
      simple deep learning NN. I attribute this increase in accuracy to the use
      of con- structional cues, extracted from the raw text of metaphoric
      instances.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0912</url>
    <doi>10.18653/v1/W18-0912</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rosen:2018:W18-09</bibkey>
  </paper>
  <paper id='0913'>
    <title>Neural Metaphor Detecting with CNN-LSTM Model</title>
    <author>
      <first>Chuhan</first>
      <last>Wu</last>
    </author>
    <author>
      <first>Fangzhao</first>
      <last>Wu</last>
    </author>
    <author>
      <first>Yubo</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Sixing</first>
      <last>Wu</last>
    </author>
    <author>
      <first>Zhigang</first>
      <last>Yuan</last>
    </author>
    <author>
      <first>Yongfeng</first>
      <last>Huang</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110–114</pages>
    <abstract>
      Metaphors are figurative languages widely used in daily life and
      literatures. It’s an important task to detect the metaphors evoked by
      texts. Thus, the metaphor shared task is aimed to extract metaphors from
      plain texts at word level. We propose to use a CNN-LSTM model for this
      task. Our model combines CNN and LSTM layers to utilize both local and
      long-range contextual information for identifying metaphorical
      information. In addition, we compare the performance of the softmax
      classifier and conditional random field (CRF) for sequential labeling in
      this task. We also incorporated some additional features such as part of
      speech (POS) tags and word cluster to improve the performance of model.
      Our best model achieved 65.06% F-score in the all POS testing subtask and
      67.15% in the verbs testing subtask.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0913</url>
    <doi>10.18653/v1/W18-0913</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wu-EtAl:2018:W18-09</bibkey>
  </paper>
  <paper id='0914'>
    <title>Di-LSTM Contrast : A Deep Neural Network for Metaphor Detection</title>
    <author>
      <first>Krishnkant</first>
      <last>Swarnkar</last>
    </author>
    <author>
      <first>Anil Kumar</first>
      <last>Singh</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115–120</pages>
    <abstract>
      The contrast between the contextual and general meaning of a word serves
      as an important clue for detecting its metaphoricity. In this paper, we
      present a deep neural architecture for metaphor detection which exploits
      this contrast. Additionally, we also use cost-sensitive learning by
      re-weighting examples, and baseline features like concreteness ratings,
      POS and WordNet-based features. The best performing system of ours
      achieves an overall F1 score of 0.570 on All POS category and 0.605 on the
      Verbs category at the Metaphor Shared Task 2018.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0914</url>
    <doi>10.18653/v1/W18-0914</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>swarnkar-singh:2018:W18-09</bibkey>
  </paper>
  <paper id='0915'>
    <title>Conditional Random Fields for Metaphor Detection</title>
    <author>
      <first>Anna</first>
      <last>Mosolova</last>
    </author>
    <author>
      <first>Ivan</first>
      <last>Bondarenko</last>
    </author>
    <author>
      <first>Vadim</first>
      <last>Fomin</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>121–123</pages>
    <abstract>
      We present an algorithm for detecting metaphor in sentences which was used
      in Shared Task on Metaphor Detection by First Workshop on Figurative
      Language Processing. The algorithm is based on different features and
      Conditional Random Fields.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0915</url>
    <doi>10.18653/v1/W18-0915</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>mosolova-bondarenko-fomin:2018:W18-09</bibkey>
  </paper>
  <paper id='0916'>
    <title>Detecting Figurative Word Occurrences Using Recurrent Neural Networks</title>
    <author>
      <first>Agnieszka</first>
      <last>Mykowiecka</last>
    </author>
    <author>
      <first>Aleksander</first>
      <last>Wawer</last>
    </author>
    <author>
      <first>Malgorzata</first>
      <last>Marciniak</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>124–127</pages>
    <abstract>
      The paper addresses detection of figurative usage of words in English
      text. The chosen method was to use neural nets fed by pretrained word
      embeddings. The obtained results show that simple solutions, based on
      words embeddings only, are comparable to complex solutions, using many
      sources of information which are not available for languages less-studied
      than English.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0916</url>
    <doi>10.18653/v1/W18-0916</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>mykowiecka-wawer-marciniak:2018:W18-09</bibkey>
  </paper>
  <paper id='0917'>
    <title>Multi-Module Recurrent Neural Networks with Transfer Learning</title>
    <author>
      <first>Filip</first>
      <last>Skurniak</last>
    </author>
    <author>
      <first>Maria</first>
      <last>Janicka</last>
    </author>
    <author>
      <first>Aleksander</first>
      <last>Wawer</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>128–132</pages>
    <abstract>
      This paper describes multiple solutions designed and tested for the
      problem of word-level metaphor detection. The proposed systems are all
      based on variants of recurrent neural network architectures. Specifically,
      we explore multiple sources of information: pre-trained word embeddings
      (Glove), a dictionary of language concreteness and a transfer learning
      scenario based on the states of an encoder network from neural network
      machine translation system. One of the architectures is based on combining
      all three systems: (1) Neural CRF (Conditional Random Fields), trained
      directly on the metaphor data set; (2) Neural Machine Translation encoder
      of a transfer learning scenario; (3) a neural network used to predict
      final labels, trained directly on the metaphor data set. Our results vary
      between test sets: Neural CRF standalone is the best one on submission
      data, while combined system scores the highest on a test subset randomly
      selected from training data.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0917</url>
    <doi>10.18653/v1/W18-0917</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>skurniak-janicka-wawer:2018:W18-09</bibkey>
  </paper>
  <paper id='0918'>
    <title>Using Language Learner Data for Metaphor Detection</title>
    <author>
      <first>Egon</first>
      <last>Stemle</last>
    </author>
    <author>
      <first>Alexander</first>
      <last>Onysko</last>
    </author>
    <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>133–138</pages>
    <abstract>
      This article describes the system that participated in the shared task on
      metaphor detection on the Vrije University Amsterdam Metaphor Corpus
      (VUA). The ST was part of the workshop on processing figurative language
      at the 16th annual conference of the North American Chapter of the
      Association for Computational Linguistics (NAACL2018). The system combines
      a small assertion of trending techniques, which implement matured methods
      from NLP and ML; in particular, the system uses word embeddings from
      standard corpora and from corpora representing different proficiency
      levels of language learners in a LSTM BiRNN architecture. The system is
      available under the APLv2 open-source license.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0918</url>
    <doi>10.18653/v1/W18-0918</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>stemle-onysko:2018:W18-09</bibkey>
  </paper>
  <paper id='1000'>
    <title>Proceedings of the Workshop on Generalization in the Age of Deep Learning</title>
    <editor>
      <first>Yonatan</first>
      <last>Bisk</last>
    </editor>
    <editor>
      <first>Omer</first>
      <last>Levy</last>
    </editor>
    <editor>
      <first>Mark</first>
      <last>Yatskar</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-10</url>
    <doi>10.18653/v1/W18-10</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-10:2018</bibkey>
  </paper>
  <paper id='1001'>
    <title>Towards Inference-Oriented Reading Comprehension: ParallelQA</title>
    <author>
      <first>Soumya</first>
      <last>Wadhwa</last>
    </author>
    <author>
      <first>Varsha</first>
      <last>Embar</last>
    </author>
    <author>
      <first>Matthias</first>
      <last>Grabmair</last>
    </author>
    <author>
      <first>Eric</first>
      <last>Nyberg</last>
    </author>
    <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–7</pages>
    <abstract>
      In this paper, we investigate the tendency of end-to-end neural Machine
      Reading Comprehension (MRC) models to match shallow patterns rather than
      perform inference-oriented reasoning on RC benchmarks. We aim to test the
      ability of these systems to answer questions which focus on referential
      inference. We propose ParallelQA, a strategy to formulate such questions
      using parallel passages. We also demonstrate that existing neural models
      fail to generalize well to this setting.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1001</url>
    <doi>10.18653/v1/W18-1001</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wadhwa-EtAl:2018:W18-10</bibkey>
  </paper>
  <paper id='1002'>
    <title>
      Commonsense mining as knowledge base completion? A study on the impact of
      novelty
    </title>
    <author>
      <first>Stanislaw</first>
      <last>Jastrzebski</last>
    </author>
    <author>
      <first>Dzmitry</first>
      <last>Bahdanau</last>
    </author>
    <author>
      <first>Seyedarian</first>
      <last>Hosseini</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Noukhovitch</last>
    </author>
    <author>
      <first>Yoshua</first>
      <last>Bengio</last>
    </author>
    <author>
      <first>Jackie</first>
      <last>Cheung</last>
    </author>
    <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8–16</pages>
    <abstract>
      Commonsense knowledge bases such as ConceptNet represent knowledge in the
      form of relational triples. Inspired by recent work by Li et al., we
      analyse if knowledge base completion models can be used to mine
      commonsense knowledge from raw text. We propose novelty of predicted
      triples with respect to the training set as an important factor in
      interpreting results. We critically analyse the difficulty of mining novel
      commonsense knowledge, and show that a simple baseline method that
      outperforms the previous state of the art on predicting more novel
      triples.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1002</url>
    <doi>10.18653/v1/W18-1002</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jastrzebski-EtAl:2018:W18-10</bibkey>
  </paper>
  <paper id='1003'>
    <title>Deep learning evaluation using deep linguistic processing</title>
    <author>
      <first>Alexander</first>
      <last>Kuhnle</last>
    </author>
    <author>
      <first>Ann</first>
      <last>Copestake</last>
    </author>
    <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–23</pages>
    <abstract>
      We discuss problems with the standard approaches to evaluation for tasks
      like visual question answering, and argue that artificial data can be used
      to address these as a complement to current practice. We demonstrate that
      with the help of existing ‘deep’ linguistic processing technology we are
      able to create challenging abstract datasets, which enable us to
      investigate the language understanding abilities of multimodal deep
      learning models in detail, as compared to a single performance value on a
      static and monolithic dataset.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1003</url>
    <doi>10.18653/v1/W18-1003</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kuhnle-copestake:2018:W18-10</bibkey>
  </paper>
  <paper id='1004'>
    <title>
      The Fine Line between Linguistic Generalization and Failure in
      Seq2Seq-Attention Models
    </title>
    <author>
      <first>Noah</first>
      <last>Weber</last>
    </author>
    <author>
      <first>Leena</first>
      <last>Shekhar</last>
    </author>
    <author>
      <first>Niranjan</first>
      <last>Balasubramanian</last>
    </author>
    <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–27</pages>
    <abstract>
      Seq2Seq based neural architectures have become the go-to architecture to
      apply to sequence to sequence language tasks. Despite their excellent
      performance on these tasks, recent work has noted that these models
      typically do not fully capture the linguistic structure required to
      generalize beyond the dense sections of the data distribution
      ettinger2017towards, and as such, are likely to fail on examples from the
      tail end of the distribution (such as inputs that are noisy
      belkinovnmtbreak, or of different length bentivoglinmtlength). In this
      paper we look at a model’s ability to generalize on a simple symbol
      rewriting task with a clearly defined structure. We find that the model’s
      ability to generalize this structure beyond the training distribution
      depends greatly on the chosen random seed, even when performance on the
      test set remains the same. This finding suggests that model’s ability to
      capture generalizable structure is highly sensitive, and more so, this
      sensitivity may not be apparent when evaluating the model on standard test
      sets.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1004</url>
    <doi>10.18653/v1/W18-1004</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>weber-shekhar-balasubramanian:2018:W18-10</bibkey>
  </paper>
  <paper id='1005'>
    <title>Extrapolation in NLP</title>
    <author>
      <first>Jeff</first>
      <last>Mitchell</last>
    </author>
    <author>
      <first>Pontus</first>
      <last>Stenetorp</last>
    </author>
    <author>
      <first>Pasquale</first>
      <last>Minervini</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Riedel</last>
    </author>
    <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>28–33</pages>
    <abstract>
      We argue that extrapolation to unseen data will often be easier for models
      that capture global structures, rather than just maximise their local fit
      to the training data. We show that this is true for two popular models:
      the Decomposable Attention Model and word2vec.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1005</url>
    <doi>10.18653/v1/W18-1005</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>mitchell-EtAl:2018:W18-10</bibkey>
  </paper>
  <paper id='1100'>
    <title>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </title>
    <editor>
      <first>Malvina</first>
      <last>Nissim</last>
    </editor>
    <editor>
      <first>Viviana</first>
      <last>Patti</last>
    </editor>
    <editor>
      <first>Barbara</first>
      <last>Plank</last>
    </editor>
    <editor>
      <first>Claudia</first>
      <last>Wagner</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-11</url>
    <doi>10.18653/v1/W18-11</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-11:2018</bibkey>
  </paper>
  <paper id='1101'>
    <title>What makes us laugh? Investigations into Automatic Humor Classification</title>
    <author>
      <first>Vikram</first>
      <last>Ahuja</last>
    </author>
    <author>
      <first>Taradheesh</first>
      <last>Bali</last>
    </author>
    <author>
      <first>Navjyoti</first>
      <last>Singh</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <abstract>
      Most scholarly works in the field of computational detection of humour
      derive their inspiration from the incongruity theory. Incongruity is an
      indispensable facet in drawing a line between humorous and non-humorous
      occurrences but is immensely inadequate in shedding light on what actually
      made the particular occurrence a funny one. Classical theories like
      Script-based Semantic Theory of Humour and General Verbal Theory of Humour
      try and achieve this feat to an adequate extent. In this paper we adhere
      to a more holistic approach towards classification of humour based on
      these classical theories with a few improvements and revisions. Through
      experiments based on our linear approach and performed on large data-sets
      of jokes, we are able to demonstrate the adaptability and show
      componentizability of our model, and that a host of classification
      techniques can be used to overcome the challenging problem of
      distinguishing between various categories and sub-categories of jokes.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1101</url>
    <doi>10.18653/v1/W18-1101</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ahuja-bali-singh:2018:W18-11</bibkey>
  </paper>
  <paper id='1102'>
    <title>Social and Emotional Correlates of Capitalization on Twitter</title>
    <author>
      <first>Sophia</first>
      <last>Chan</last>
    </author>
    <author>
      <first>Alona</first>
      <last>Fyshe</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–15</pages>
    <abstract>
      Social media text is replete with unusual capitalization patterns. We
      posit that capitalizing a token like THIS performs two expressive
      functions: it marks a person socially, and marks certain parts of an
      utterance as more salient than others. Focusing on gender and sentiment,
      we illustrate using a corpus of tweets that capitalization appears in more
      negative than positive contexts, and is used more by females compared to
      males. Yet we find that both genders use capitalization in a similar way
      when expressing sentiment.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1102</url>
    <doi>10.18653/v1/W18-1102</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chan-fyshe:2018:W18-11</bibkey>
  </paper>
  <paper id='1103'>
    <title>
      Building an annotated dataset of app store reviews with Appraisal features
      in English and Spanish
    </title>
    <author>
      <first>Natalia</first>
      <last>Mora</last>
    </author>
    <author>
      <first>Julia</first>
      <last>Lavid-López</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>16–24</pages>
    <abstract>
      This paper describes the creation and annotation of a dataset consisting
      of 250 English and Spanish app store reviews from Google’s Play Store with
      Appraisal features. This is one of the most influential linguistic
      frameworks for the analysis of evaluation and opinion in discourse due to
      its insightful descriptive features. However, it has not been extensively
      applied in NLP in spite of its potential for the classification of the
      subjective content of these reviews. We describe the dataset, the
      annotation scheme and guidelines, the agreement studies, the annotation
      results and their impact on the characterisation of this genre.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1103</url>
    <doi>10.18653/v1/W18-1103</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>mora-lavidlpez:2018:W18-11</bibkey>
  </paper>
  <paper id='1104'>
    <title>Enabling Deep Learning of Emotion With First-Person Seed Expressions</title>
    <author>
      <first>Hassan</first>
      <last>Alhuzali</last>
    </author>
    <author>
      <first>Muhammad</first>
      <last>Abdul-Mageed</last>
    </author>
    <author>
      <first>Lyle</first>
      <last>Ungar</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–35</pages>
    <abstract>
      The computational treatment of emotion in natural language text remains
      relatively limited, and Arabic is no exception. This is partly due to lack
      of labeled data. In this work, we describe and manually validate a method
      for the automatic acquisition of emotion labeled data and introduce a
      newly developed data set for Modern Standard and Dialectal Arabic emotion
      detection focused at Robert Plutchik’s 8 basic emotion types. Using a
      hybrid supervision method that exploits first person emotion seeds, we
      show how we can acquire promising results with a deep gated recurrent
      neural network. Our best model reaches 70% F-score, significantly (i.e.,
      11%, p 0:05) outperforming a competitive baseline. Applying our method and
      data on an external dataset of 4 emotions released around the same time we
      finalized our work, we acquire 7% absolute gain in F-score over a linear
      SVM classifier trained on gold data, thus validating our approach.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1104</url>
    <doi>10.18653/v1/W18-1104</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>alhuzali-abdulmageed-ungar:2018:W18-11</bibkey>
  </paper>
  <paper id='1105'>
    <title>
      A Dataset of Hindi-English Code-Mixed Social Media Text for Hate Speech
      Detection
    </title>
    <author>
      <first>Aditya</first>
      <last>Bohra</last>
    </author>
    <author>
      <first>Deepanshu</first>
      <last>Vijay</last>
    </author>
    <author>
      <first>Vinay</first>
      <last>Singh</last>
    </author>
    <author>
      <first>Syed Sarfaraz</first>
      <last>Akhtar</last>
    </author>
    <author>
      <first>Manish</first>
      <last>Shrivastava</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36–41</pages>
    <abstract>
      Hate speech detection in social media texts is an important Natural
      language Processing task, which has several crucial applications like
      sentiment analysis, investigating cyberbullying and examining
      socio-political controversies. While relevant research has been done
      independently on code-mixed social media texts and hate speech detection,
      our work is the first attempt in detecting hate speech in Hindi-English
      code-mixed social media text. In this paper, we analyze the problem of
      hate speech detection in code-mixed texts and present a Hindi-English
      code-mixed dataset consisting of tweets posted online on Twitter. The
      tweets are annotated with the language at word level and the class they
      belong to (Hate Speech or Normal Speech). We also propose a supervised
      classification system for detecting hate speech in the text using various
      character level, word level, and lexicon based features.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1105</url>
    <doi>10.18653/v1/W18-1105</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bohra-EtAl:2018:W18-11</bibkey>
  </paper>
  <paper id='1106'>
    <title>
      The Social and the Neural Network: How to Make Natural Language Processing
      about People again
    </title>
    <author>
      <first>Dirk</first>
      <last>Hovy</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>42–49</pages>
    <abstract>
      Over the years, natural language processing has increasingly focused on
      tasks that can be solved by statistical models, but ignored the social
      aspects of language. These limitations are in large part due to
      historically available data and the limitations of the models, but have
      narrowed our focus and biased the tools demographically. However, with the
      increased availability of data sets including socio-demographic
      information and more expressive (neural) models, we have the opportunity
      to address both issues. I argue that this combination can broaden the
      focus of NLP to solve a whole new range of tasks, enable us to generate
      novel linguistic insights, and provide fairer tools for everyone.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1106</url>
    <doi>10.18653/v1/W18-1106</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>hovy:2018:W18-11</bibkey>
  </paper>
  <paper id='1107'>
    <title>Observational Comparison of Geo-tagged and Randomly-drawn Tweets</title>
    <author>
      <first>Tom</first>
      <last>Lippincott</last>
    </author>
    <author>
      <first>Annabelle</first>
      <last>Carrell</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>50–55</pages>
    <abstract>
      Twitter is a ubiquitous source of micro-blog social media data, providing
      the academic, industrial, and public sectors real-time access to
      actionable information. A particularly attractive property of some tweets
      is *geo-tagging*, where a user account has opted-in to attaching their
      current location to each message. Unfortunately (from a researcher’s
      perspective) only a fraction of Twitter accounts agree to this, and these
      accounts are likely to have systematic diffences with the general
      population. This work is an exploratory study of these differences across
      the full range of Twitter content, and complements previous studies that
      focus on the English-language subset. Additionally, we compare methods for
      querying users by self-identified properties, finding that the constrained
      semantics of the "description" field provides cleaner, higher-volume
      results than more complex regular expressions.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1107</url>
    <doi>10.18653/v1/W18-1107</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>lippincott-carrell:2018:W18-11</bibkey>
  </paper>
  <paper id='1108'>
    <title>
      Johns Hopkins or johnny-hopkins: Classifying Individuals versus
      Organizations on Twitter
    </title>
    <author>
      <first>Zach</first>
      <last>Wood-Doughty</last>
    </author>
    <author>
      <first>Praateek</first>
      <last>Mahajan</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Dredze</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–61</pages>
    <abstract>
      Twitter user accounts include a range of different user types. While many
      individuals use Twitter, organizations also have Twitter accounts.
      Identifying opinions and trends from Twitter requires the accurate
      differentiation of these two groups. Previous work (McCorriston et al.,
      2015) presented a method for determining if an account was an individual
      or organization based on account profile and a collection of tweets. We
      present a method that relies solely on the account profile, allowing for
      the classification of individuals versus organizations based on a single
      tweet. Our method obtains accuracies comparable to methods that rely on
      much more information by leveraging two improvements: a character-based
      Convolutional Neural Network, and an automatically derived labeled corpus
      an order of magnitude larger than the previously available dataset. We
      make both the dataset and the resulting tool available.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1108</url>
    <doi>10.18653/v1/W18-1108</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wooddoughty-mahajan-dredze:2018:W18-11</bibkey>
  </paper>
  <paper id='1109'>
    <title>
      The Potential of the Computational Linguistic Analysis of Social Media for
      Population Studies
    </title>
    <author>
      <first>Letizia</first>
      <last>Mencarini</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>62–68</pages>
    <abstract>
      The paper provides an outline of the scope for synergy between
      computational linguistic analysis and population stud-ies. It first
      reviews where population studies stand in terms of using social media
      data. Demographers are entering the realm of big data in force. But, this
      paper argues, population studies have much to gain from computational
      linguis-tic analysis, especially in terms of ex-plaining the drivers
      behind population processes. The paper gives two examples of how the
      method can be applied, and concludes with a fundamental caveat. Yes,
      computational linguistic analysis provides a possible key for integrating
      micro theory into any demographic analysis of social media data. But
      results may be of little value in as much as knowledge about fundamental
      sample characteristics are unknown.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1109</url>
    <doi>10.18653/v1/W18-1109</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>mencarini:2018:W18-11</bibkey>
  </paper>
  <paper id='1110'>
    <title>
      Understanding the Effect of Gender and Stance in Opinion Expression in
      Debates on "Abortion"
    </title>
    <author>
      <first>Esin</first>
      <last>Durmus</last>
    </author>
    <author>
      <first>Claire</first>
      <last>Cardie</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>69–75</pages>
    <abstract>
      In this paper, we focus on understanding linguistic differences across
      groups with different self-identified gender and stance in expressing
      opinions about ABORTION. We provide a new dataset consisting of users’
      gender, stance on ABORTION as well as the debates in ABOR- TION drawn from
      debate.org. We use the gender and stance information to identify
      significant linguistic differences across individuals with different
      gender and stance. We show the importance of considering the stance
      information along with the gender since we observe significant linguistic
      differences across individuals with different stance even within the same
      gender group.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1110</url>
    <doi>10.18653/v1/W18-1110</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>durmus-cardie:2018:W18-11</bibkey>
  </paper>
  <paper id='1111'>
    <title>Frustrated, Polite, or Formal: Quantifying Feelings and Tone in Email</title>
    <author>
      <first>Niyati</first>
      <last>Chhaya</last>
    </author>
    <author>
      <first>Kushal</first>
      <last>Chawla</last>
    </author>
    <author>
      <first>Tanya</first>
      <last>Goyal</last>
    </author>
    <author>
      <first>Projjal</first>
      <last>Chanda</last>
    </author>
    <author>
      <first>Jaya</first>
      <last>Singh</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–86</pages>
    <abstract>
      Email conversations are the primary mode of communication in enterprises.
      The email content expresses an individual’s needs, requirements and
      intentions. Affective information in the email text can be used to get an
      insight into the sender’s mood or emotion. We present a novel approach to
      model human frustration in text. We identify linguistic features that
      influence human perception of frustration and model it as a supervised
      learning task. The paper provides a detailed comparison across traditional
      regression and word distribution-based models. We report a mean-squared
      error (MSE) of 0.018 against human-annotated frustration for the best
      performing model. The approach establishes the importance of affect
      features in frustration prediction for email data. We further evaluate the
      efficacy of the proposed feature set and model in predicting other tone or
      affects in text, namely formality and politeness; results demonstrate a
      comparable performance against the state-of-the-art baselines.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1111</url>
    <doi>10.18653/v1/W18-1111</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chhaya-EtAl:2018:W18-11</bibkey>
  </paper>
  <paper id='1112'>
    <title>Reddit: A Gold Mine for Personality Prediction</title>
    <author>
      <first>Matej</first>
      <last>Gjurković</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Šnajder</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–97</pages>
    <abstract>
      Automated personality prediction from social media is gaining increasing
      attention in natural language processing and social sciences communities.
      However, due to high labeling costs and privacy issues, the few publicly
      available datasets are of limited size and low topic diversity. We address
      this problem by introducing a large-scale dataset derived from Reddit, a
      source so far overlooked for personality prediction. The dataset is
      labeled with Myers-Briggs Type Indicators (MBTI) and comes with a rich set
      of features for more than 9k users. We carry out a preliminary feature
      analysis, revealing marked differences between the MBTI dimensions and
      poles. Furthermore, we use the dataset to train and evaluate benchmark
      personality prediction models, achieving macro F1-scores between 67% and
      82% on the individual dimensions and 82% accuracy for exact or one-off
      accurate type prediction. These results are encouraging and comparable
      with the reliability of standardized tests.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1112</url>
    <doi>10.18653/v1/W18-1112</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gjurkovi-najder:2018:W18-11</bibkey>
  </paper>
  <paper id='1113'>
    <title>Predicting Authorship and Author Traits from Keystroke Dynamics</title>
    <author>
      <first>Barbara</first>
      <last>Plank</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>98–104</pages>
    <abstract>
      Written text transmits a good deal of nonverbal information related to the
      author’s identity and social factors, such as age, gender and personality.
      However, it is less known to what extent behavioral biometric traces
      transmit such information. We use typist data to study the predictiveness
      of authorship, and present first experiments on predicting both age and
      gender from keystroke dynamics. Our results show that the model based on
      keystroke features, while being two orders of magnitude smaller, leads to
      significantly higher accuracies for authorship than the text-based system.
      For user attribute prediction, the best approach is to combine the two,
      suggesting that extralinguistic factors are disclosed to a larger degree
      in written text, while author identity is better transmitted in typing
      behavior.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1113</url>
    <doi>10.18653/v1/W18-1113</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>plank:2018:W18-11</bibkey>
  </paper>
  <paper id='1114'>
    <title>Predicting Twitter User Demographics from Names Alone</title>
    <author>
      <first>Zach</first>
      <last>Wood-Doughty</last>
    </author>
    <author>
      <first>Nicholas</first>
      <last>Andrews</last>
    </author>
    <author>
      <first>Rebecca</first>
      <last>Marvin</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Dredze</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>105–111</pages>
    <abstract>
      Social media analysis frequently requires tools that can automatically
      infer demographics to contextualize trends. These tools often require
      hundreds of user-authored messages for each user, which may be prohibitive
      to obtain when analyzing millions of users. We explore character-level
      neural models that learn a representation of a user’s name and screen name
      to predict gender and ethnicity, allowing for demographic inference with
      minimal data. We release trained models1 which may enable new demographic
      analyses that would otherwise require enormous amounts of data collection
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1114</url>
    <doi>10.18653/v1/W18-1114</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wooddoughty-EtAl:2018:W18-11</bibkey>
  </paper>
  <paper id='1115'>
    <title>Modeling Personality Traits of Filipino Twitter Users</title>
    <author>
      <first>Edward</first>
      <last>Tighe</last>
    </author>
    <author>
      <first>Charibeth</first>
      <last>Cheng</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>112–122</pages>
    <abstract>
      Recent studies in the field of text-based personality recognition
      experiment with different languages, feature extraction techniques, and
      machine learning algorithms to create better and more accurate models;
      however, little focus is placed on exploring the language use of a group
      of individuals defined by nationality. Individuals of the same nationality
      share certain practices and communicate certain ideas that can become
      embedded into their natural language. Many nationals are also not limited
      to speaking just one language, such as how Filipinos speak Filipino and
      English, the two national languages of the Philippines. The addition of
      several regional/indigenous languages, along with the commonness of
      code-switching, allow for a Filipino to have a rich vocabulary. This
      presents an opportunity to create a text-based personality model based on
      how Filipinos speak, regardless of the language they use. To do so, data
      was collected from 250 Filipino Twitter users. Different combinations of
      data processing techniques were experimented upon to create personality
      models for each of the Big Five. The results for both regression and
      classification show that Conscientiousness is consistently the easiest
      trait to model, followed by Extraversion. Classification models for
      Agreeableness and Neuroticism had subpar performances, but performed
      better than those of Openness. An analysis on personality trait score
      representation showed that classifying extreme outliers generally produce
      better results for all traits except for Neuroticism and Openness.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1115</url>
    <doi>10.18653/v1/W18-1115</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>tighe-cheng:2018:W18-11</bibkey>
  </paper>
  <paper id='1116'>
    <title>Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter</title>
    <author>
      <first>David</first>
      <last>Vilares</last>
    </author>
    <author>
      <first>Carlos</first>
      <last>Gómez-Rodríguez</last>
    </author>
    <booktitle>
      Proceedings of the Second Workshop on Computational Modeling of People’s
      Opinions, Personality, and Emotions in Social Media
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>123–128</pages>
    <abstract>
      The usage of part-of-day nouns, such as ’night’, and their time-specific
      greetings (’good night’), varies across languages and cultures. We show
      the possibilities that Twitter offers for studying the semantics of these
      terms and its variability between countries. We mine a worldwide sample of
      multilingual tweets with temporal greetings, and study how their
      frequencies vary in relation with local time. The results provide insights
      into the semantics of these temporal expressions and the cultural and
      sociological factors influencing their usage.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1116</url>
    <doi>10.18653/v1/W18-1116</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>vilares-gmezrodrguez:2018:W18-11</bibkey>
  </paper>
  <paper id='1200'>
    <title>Proceedings of the Second Workshop on Subword/Character LEvel Models</title>
    <editor>
      <first>Manaal</first>
      <last>Faruqui</last>
    </editor>
    <editor>
      <first>Hinrich</first>
      <last>Schütze</last>
    </editor>
    <editor>
      <first>Isabel</first>
      <last>Trancoso</last>
    </editor>
    <editor>
      <first>Yulia</first>
      <last>Tsvetkov</last>
    </editor>
    <editor>
      <first>Yadollah</first>
      <last>Yaghoobzadeh</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-12</url>
    <doi>10.18653/v1/W18-12</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-12:2018</bibkey>
  </paper>
  <paper id='1201'>
    <title>
      Morphological Word Embeddings for Arabic Neural Machine Translation in
      Low-Resource Settings
    </title>
    <author>
      <first>Pamela</first>
      <last>Shapiro</last>
    </author>
    <author>
      <first>Kevin</first>
      <last>Duh</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–11</pages>
    <abstract>
      Neural machine translation has achieved impressive results in the last few
      years, but its success has been limited to settings with large amounts of
      parallel data. One way to improve NMT for lower-resource settings is to
      initialize a word-based NMT model with pretrained word embeddings.
      However, rare words still suffer from lower quality word embeddings when
      trained with standard word-level objectives. We introduce word embeddings
      that utilize morphological resources, and compare to purely unsupervised
      alternatives. We work with Arabic, a morphologically rich language with
      available linguistic resources, and perform Ar-to-En MT experiments on a
      small corpus of TED subtitles. We find that word embeddings utilizing
      subword information consistently outperform standard word embeddings on a
      word similarity task and as initialization of the source word embeddings
      in a low-resource NMT system.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1201</url>
    <doi>10.18653/v1/W18-1201</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>shapiro-duh:2018:W18-12</bibkey>
  </paper>
  <paper id='1202'>
    <title>Entropy-Based Subword Mining with an Application to Word Embeddings</title>
    <author>
      <first>Ahmed</first>
      <last>El-Kishky</last>
    </author>
    <author>
      <first>Frank</first>
      <last>Xu</last>
    </author>
    <author>
      <first>Aston</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Stephen</first>
      <last>Macke</last>
    </author>
    <author>
      <first>Jiawei</first>
      <last>Han</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–21</pages>
    <abstract>
      Recent literature has shown a wide variety of benefits to mapping
      traditional one-hot representations of words and phrases to
      lower-dimensional real-valued vectors known as word embeddings.
      Traditionally, most word embedding algorithms treat each word as the
      finest meaningful semantic granularity and perform embedding by learning
      distinct embedding vectors for each word. Contrary to this line of
      thought, technical domains such as scientific and medical literature
      compose words from subword structures such as prefixes, suffixes, and
      root-words as well as compound words. Treating individual words as the
      finest-granularity unit discards meaningful shared semantic structure
      between words sharing substructures. This not only leads to poor
      embeddings for text corpora that have long-tail distributions, but also
      heuristic methods for handling out-of-vocabulary words. In this paper we
      propose SubwordMine, an entropy-based subword mining algorithm that is
      fast, unsupervised, and fully data-driven. We show that this allows for
      great cross-domain performance in identifying semantically meaningful
      subwords. We then investigate utilizing the mined subwords within the
      FastText embedding model and compare performance of the learned
      representations in a downstream language modeling task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1202</url>
    <doi>10.18653/v1/W18-1202</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>elkishky-EtAl:2018:W18-12</bibkey>
  </paper>
  <paper id='1203'>
    <title>
      A Comparison of Character Neural Language Model and Bootstrapping for
      Language Identification in Multilingual Noisy Texts
    </title>
    <author>
      <first>Wafia</first>
      <last>Adouane</last>
    </author>
    <author>
      <first>Simon</first>
      <last>Dobnik</last>
    </author>
    <author>
      <first>Jean-Philippe</first>
      <last>Bernardy</last>
    </author>
    <author>
      <first>Nasredine</first>
      <last>Semmar</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–31</pages>
    <abstract>
      This paper seeks to examine the effect of including background knowledge
      in the form of character pre-trained neural language model (LM), and data
      bootstrapping to overcome the problem of unbalanced limited resources. As
      a test, we explore the task of language identification in mixed-language
      short non-edited texts with an under-resourced language, namely the case
      of Algerian Arabic for which both labelled and unlabelled data are
      limited. We compare the performance of two traditional machine learning
      methods and a deep neural networks (DNNs) model. The results show that
      overall DNNs perform better on labelled data for the majority categories
      and struggle with the minority ones. While the effect of the untokenised
      and unlabelled data encoded as LM differs for each category,
      bootstrapping, however, improves the performance of all systems and all
      categories. These methods are language independent and could be
      generalised to other under-resourced languages for which a small labelled
      data and a larger unlabelled data are available.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1203</url>
    <doi>10.18653/v1/W18-1203</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>adouane-EtAl:2018:W18-12</bibkey>
  </paper>
  <paper id='1204'>
    <title>Addressing Low-Resource Scenarios with Character-aware Embeddings</title>
    <author>
      <first>Sean</first>
      <last>Papay</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Padó</last>
    </author>
    <author>
      <first>Ngoc Thang</first>
      <last>Vu</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32–37</pages>
    <abstract>
      Most modern approaches to computing word embeddings assume the
      availability of text corpora with billions of words. In this paper, we
      explore a setup where only corpora with millions of words are available,
      and many words in any new text are out of vocabulary. This setup is both
      of practical interests – modeling the situation for specific domains and
      low-resource languages – and of psycholinguistic interest, since it
      corresponds much more closely to the actual experiences and challenges of
      human language learning and use. We compare standard skip-gram word
      embeddings with character-based embeddings on word relatedness prediction.
      Skip-grams excel on large corpora, while character-based embeddings do
      well on small corpora generally and rare and complex words specifically.
      The models can be combined easily.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1204</url>
    <doi>10.18653/v1/W18-1204</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>papay-pad-vu:2018:W18-12</bibkey>
  </paper>
  <paper id='1205'>
    <title>Subword-level Composition Functions for Learning Word Embeddings</title>
    <author>
      <first>Bofang</first>
      <last>Li</last>
    </author>
    <author>
      <first>Aleksandr</first>
      <last>Drozd</last>
    </author>
    <author>
      <first>Tao</first>
      <last>Liu</last>
    </author>
    <author>
      <first>Xiaoyong</first>
      <last>Du</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38–48</pages>
    <abstract>
      Subword-level information is crucial for capturing the meaning and
      morphology of words, especially for out-of-vocabulary entries. We propose
      CNN- and RNN-based subword-level composition functions for learning word
      embeddings, and systematically compare them with popular word-level and
      subword-level models (Skip-Gram and FastText). Additionally, we propose a
      hybrid training scheme in which a pure subword-level model is trained
      jointly with a conventional word-level embedding model based on
      lookup-tables. This increases the fitness of all types of subword-level
      word embeddings; the word-level embeddings can be discarded after
      training, leaving only compact subword-level representation with much
      smaller data volume. We evaluate these embeddings on a set of intrinsic
      and extrinsic tasks, showing that subword-level models have advantage on
      tasks related to morphology and datasets with high OOV rate, and can be
      combined with other types of embeddings.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1205</url>
    <doi>10.18653/v1/W18-1205</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>li-EtAl:2018:W18-12</bibkey>
  </paper>
  <paper id='1206'>
    <title>Discovering Phonesthemes with Sparse Regularization</title>
    <author>
      <first>Nelson F.</first>
      <last>Liu</last>
    </author>
    <author>
      <first>Gina-Anne</first>
      <last>Levow</last>
    </author>
    <author>
      <first>Noah A.</first>
      <last>Smith</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>49–54</pages>
    <abstract>
      We introduce a simple method for extracting non-arbitrary form-meaning
      representations from a collection of semantic vectors. We treat the
      problem as one of feature selection for a model trained to predict word
      vectors from subword features. We apply this model to the problem of
      automatically discovering phonesthemes, which are submorphemic sound
      clusters that appear in words with similar meaning. Many of our
      model-predicted phonesthemes overlap with those proposed in the
      linguistics literature, and we validate our approach with human judgments.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1206</url>
    <doi>10.18653/v1/W18-1206</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>liu-levow-smith:2018:W18-12</bibkey>
  </paper>
  <paper id='1207'>
    <title>Meaningless yet meaningful: Morphology grounded subword-level NMT</title>
    <author>
      <first>Tamali</first>
      <last>Banerjee</last>
    </author>
    <author>
      <first>Pushpak</first>
      <last>Bhattacharyya</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–60</pages>
    <abstract>
      We explore the use of two independent subsystems Byte Pair Encoding (BPE)
      and Morfessor as basic units for subword-level neural machine translation
      (NMT). We show that, for linguistically distant language-pairs
      Morfessor-based segmentation algorithm produces significantly better
      quality translation than BPE. However, for close language-pairs BPE-based
      subword-NMT may translate better than Morfessor-based subword-NMT. We
      propose a combined approach of these two segmentation algorithms
      Morfessor-BPE (M-BPE) which outperforms these two baseline systems in
      terms of BLEU score. Our results are supported by experiments on three
      language-pairs: English-Hindi, Bengali-Hindi and English-Bengali.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1207</url>
    <doi>10.18653/v1/W18-1207</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>banerjee-bhattacharyya:2018:W18-12</bibkey>
  </paper>
  <paper id='1208'>
    <title>Fast Query Expansion on an Accounting Corpus using Sub-Word Embeddings</title>
    <author>
      <first>Hrishikesh</first>
      <last>Ganu</last>
    </author>
    <author>
      <first>Viswa Datha</first>
      <last>P</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>61–65</pages>
    <abstract>
      We present early results from a system under development which uses
      sub-word embeddings for query expansion in presence of mis-spelled words
      and other aberrations. We work for a company which creates accounting
      software and the end goal is to improve customer experience when they
      search for help on our "Customer Care" portal. Our customers use
      colloquial language, non-standard acronyms and sometimes mis-spell words
      when they use our Search portal or interact over other channels. However,
      our Knowledge Base has curated content which leverages technical terms and
      is in language which is quite formal. This results in the answer not being
      retrieved even though the answer might actually be present in the
      documentation (as assessed by a human). We address this problem by
      creating equivalence classes of words with similar meanings (with the
      additional property that the mappings to these equivalence classes are
      robust to mis-spellings) using sub-word embeddings and then use them to
      fine tune an Elasticsearch index to improve recall. We demonstrate through
      an end-end system that using sub-word embeddings leads to a significant
      lift in correct answers retrieved for an accounting corpus available in
      the public domain.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1208</url>
    <doi>10.18653/v1/W18-1208</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ganu-p:2018:W18-12</bibkey>
  </paper>
  <paper id='1209'>
    <title>Incorporating Subword Information into Matrix Factorization Word Embeddings</title>
    <author>
      <first>Alexandre</first>
      <last>Salle</last>
    </author>
    <author>
      <first>Aline</first>
      <last>Villavicencio</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–71</pages>
    <abstract>
      The positive effect of adding subword information to word embeddings has
      been demonstrated for predictive models. In this paper we investigate
      whether similar benefits can also be derived from incorporating subwords
      into counting models. We evaluate the impact of different types of
      subwords (n-grams and unsupervised morphemes), with results confirming the
      importance of subword information in learning representations of rare and
      out-of-vocabulary words.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1209</url>
    <doi>10.18653/v1/W18-1209</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>salle-villavicencio:2018:W18-12</bibkey>
  </paper>
  <paper id='1210'>
    <title>A Multi-Context Character Prediction Model for a Brain-Computer Interface</title>
    <author>
      <first>Shiran</first>
      <last>Dudy</last>
    </author>
    <author>
      <first>Shaobin</first>
      <last>Xu</last>
    </author>
    <author>
      <first>Steven</first>
      <last>Bedrick</last>
    </author>
    <author>
      <first>David</first>
      <last>Smith</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>72–77</pages>
    <abstract>
      Brain-computer interfaces and other augmentative and alternative
      communication devices introduce language-modeing challenges distinct from
      other character-entry methods. In particular, the acquired signal of the
      EEG (electroencephalogram) signal is noisier, which, in turn, makes the
      user intent harder to decipher. In order to adapt to this condition, we
      propose to maintain ambiguous history for every time step, and to employ,
      apart from the character language model, word information to produce a
      more robust prediction system. We present preliminary results that compare
      this proposed Online-Context Language Model (OCLM) to current algorithms
      that are used in this type of setting. Evaluation on both perplexity and
      predictive accuracy demonstrates promising results when dealing with
      ambiguous histories in order to provide to the front end a distribution of
      the next character the user might type.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1210</url>
    <doi>10.18653/v1/W18-1210</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>dudy-EtAl:2018:W18-12</bibkey>
  </paper>
  <paper id='1300'>
    <title>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</title>
    <editor>
      <first>Eduardo</first>
      <last>Blanco</last>
    </editor>
    <editor>
      <first>Roser</first>
      <last>Morante</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-13</url>
    <doi>10.18653/v1/W18-13</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-13:2018</bibkey>
  </paper>
  <paper id='1301'>
    <title>Using Hedge Detection to Improve Committed Belief Tagging</title>
    <author>
      <first>Morgan</first>
      <last>Ulinski</last>
    </author>
    <author>
      <first>Seth</first>
      <last>Benjamin</last>
    </author>
    <author>
      <first>Julia</first>
      <last>Hirschberg</last>
    </author>
    <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–5</pages>
    <abstract>
      We describe a novel method for identifying hedge terms using a set of
      manually constructed rules. We present experiments adding hedge features
      to a committed belief system to improve classification. We compare
      performance of this system (a) without hedging features, (b) with
      dictionary-based features, and (c) with rule-based features. We find that
      using hedge features improves performance of the committed belief system,
      particularly in identifying instances of non-committed belief and reported
      belief.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1301</url>
    <doi>10.18653/v1/W18-1301</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ulinski-benjamin-hirschberg:2018:W18-13</bibkey>
  </paper>
  <paper id='1302'>
    <title>
      Paths for uncertainty: Exploring the intricacies of uncertainty
      identification for news
    </title>
    <author>
      <first>Chrysoula</first>
      <last>Zerva</last>
    </author>
    <author>
      <first>Sophia</first>
      <last>Ananiadou</last>
    </author>
    <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>6–20</pages>
    <abstract>
      Currently, news articles are produced, shared and consumed at an extremely
      rapid rate. Although their quantity is increasing, at the same time, their
      quality and trustworthiness is becoming fuzzier. Hence, it is important
      not only to automate information extraction but also to quantify the
      certainty of this information. Automated identification of certainty has
      been studied both in the scientific and newswire domains, but performance
      is considerably higher in tasks focusing on scientific text. We compare
      the differences in the definition and expression of uncertainty between a
      scientific domain, i.e., biomedicine, and newswire. We delve into the
      different aspects that affect the certainty of an extracted event in a
      news article and examine whether they can be easily identified by
      techniques already validated in the biomedical domain. Finally, we present
      a comparison of the syntactic and lexical differences between the the
      expression of certainty in the biomedical and newswire domains, using two
      annotated corpora.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1302</url>
    <doi>10.18653/v1/W18-1302</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zerva-ananiadou:2018:W18-13</bibkey>
  </paper>
  <paper id='1303'>
    <title>Detecting Sarcasm is Extremely Easy ;-)</title>
    <author>
      <first>Natalie</first>
      <last>Parde</last>
    </author>
    <author>
      <first>Rodney</first>
      <last>Nielsen</last>
    </author>
    <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–26</pages>
    <abstract>
      Detecting sarcasm in text is a particularly challenging problem in
      computational semantics, and its solution may vary across different types
      of text. We analyze the performance of a domain-general sarcasm detection
      system on datasets from two very different domains: Twitter, and Amazon
      product reviews. We categorize the errors that we identify with each, and
      make recommendations for addressing these issues in NLP systems in the
      future.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1303</url>
    <doi>10.18653/v1/W18-1303</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>parde-nielsen:2018:W18-13</bibkey>
  </paper>
  <paper id='1304'>
    <title>GKR: the Graphical Knowledge Representation for semantic parsing</title>
    <author>
      <first>Aikaterini-Lida</first>
      <last>Kalouli</last>
    </author>
    <author>
      <first>Richard</first>
      <last>Crouch</last>
    </author>
    <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–37</pages>
    <abstract>
      This paper describes the first version of an open-source semantic parser
      that creates graphical representations of sentences to be used for further
      semantic processing, e.g. for natural language inference, reasoning and
      semantic similarity. The Graphical Knowledge Representation which is
      output by the parser is inspired by the Abstract Knowledge Representation,
      which separates out conceptual and contextual levels of representation
      that deal respectively with the subject matter of a sentence and its
      existential commitments. Our representation is a layered graph with each
      sub-graph holding different kinds of information, including one sub-graph
      for concepts and one for contexts. Our first evaluation of the system
      shows an F-score of 85% in accurately representing sentences as semantic
      graphs.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1304</url>
    <doi>10.18653/v1/W18-1304</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kalouli-crouch:2018:W18-13</bibkey>
  </paper>
  <paper id='1305'>
    <title>
      Computational Argumentation: A Journey Beyond Semantics, Logic, Opinions,
      and Easy Tasks
    </title>
    <author>
      <first>Ivan</first>
      <last>Habernal</last>
    </author>
    <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38</pages>
    <abstract>
      The classical view on argumentation, such that arguments are logical
      structures consisting of different distinguishable parts and that parties
      exchange arguments in a rational way, is prevalent in textbooks but
      nonexistent in the real world. Instead, argumentation is a multifaceted
      communication tool built upon humans’ capabilities to easily use common
      sense, emotions, and social context. As humans, we are pretty good at it.
      Computational Argumentation tries to tackle these phenomena but has a long
      and not so easy way to go. In this talk, I would like to shed a light on
      several recent attempts to deal with argumentation computationally, such
      as addressing argument quality, understanding argument reasoning, dealing
      with fallacies, and how should we never ever argue online.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1305</url>
    <doi>10.18653/v1/W18-1305</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>habernal:2018:W18-13</bibkey>
  </paper>
  <paper id='1400'>
    <title>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </title>
    <editor>
      <first>Parisa</first>
      <last>Kordjamshidi</last>
    </editor>
    <editor>
      <first>Archna</first>
      <last>Bhatia</last>
    </editor>
    <editor>
      <first>James</first>
      <last>Pustejovsky</last>
    </editor>
    <editor>
      <first>Marie-Francine</first>
      <last>Moens</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-14</url>
    <doi>10.18653/v1/W18-14</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-14:2018</bibkey>
  </paper>
  <paper id='1401'>
    <title>
      Exploring the Functional and Geometric Bias of Spatial Relations Using
      Neural Language Models
    </title>
    <author>
      <first>Simon</first>
      <last>Dobnik</last>
    </author>
    <author>
      <first>Mehdi</first>
      <last>Ghanimifard</last>
    </author>
    <author>
      <first>John</first>
      <last>Kelleher</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–11</pages>
    <abstract>
      The challenge for computational models of spatial descriptions for
      situated dialogue systems is the integration of information from different
      modalities. The semantics of spatial descriptions are grounded in at least
      two sources of information: (i) a geometric representation of space and
      (ii) the functional interaction of related objects that. We train several
      neural language models on descriptions of scenes from a dataset of image
      captions and examine whether the functional or geometric bias of spatial
      descriptions reported in the literature is reflected in the estimated
      perplexity of these models. The results of these experiments have
      implications for the creation of models of spatial lexical semantics for
      human-robot dialogue systems. Furthermore, they also provide an insight
      into the kinds of the semantic knowledge captured by neural language
      models trained on spatial descriptions, which has implications for image
      captioning systems.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1401</url>
    <doi>10.18653/v1/W18-1401</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>dobnik-ghanimifard-kelleher:2018:W18-14</bibkey>
  </paper>
  <paper id='1402'>
    <title>
      Building and Learning Structures in a Situated Blocks World Through Deep
      Language Understanding
    </title>
    <author>
      <first>Ian</first>
      <last>Perera</last>
    </author>
    <author>
      <first>James</first>
      <last>Allen</last>
    </author>
    <author>
      <first>Choh Man</first>
      <last>Teng</last>
    </author>
    <author>
      <first>Lucian</first>
      <last>Galescu</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–20</pages>
    <abstract>
      We demonstrate a system for understanding natural language utterances for
      structure description and placement in a situated blocks world context. By
      relying on a rich, domain-specific adaptation of a generic ontology and a
      logical form structure produced by a semantic parser, we obviate the need
      for an intermediate, domain-specific representation and can produce a
      reasoner that grounds and reasons over concepts and constraints with
      real-valued data. This linguistic base enables more flexibility in
      interpreting natural language expressions invoking intrinsic concepts and
      features of structures and space. We demonstrate some of the capabilities
      of a system grounded in deep language understanding and present initial
      results in a structure learning task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1402</url>
    <doi>10.18653/v1/W18-1402</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>perera-EtAl:2018:W18-14</bibkey>
  </paper>
  <paper id='1403'>
    <title>Computational Models for Spatial Prepositions</title>
    <author>
      <first>Georgiy</first>
      <last>Platonov</last>
    </author>
    <author>
      <first>Lenhart</first>
      <last>Schubert</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–30</pages>
    <abstract>
      Developing computational models of spatial prepositions (such as on, in,
      above, etc.) is crucial for such tasks as human-machine collaboration,
      story understanding, and 3D model generation from descriptions. However,
      these prepositions are notoriously vague and ambiguous, with meanings
      depending on the types, shapes and sizes of entities in the argument
      positions, the physical and task context, and other factors. As a result
      truth value judgments for prepositional relations are often uncertain and
      variable. In this paper we treat the modeling task as calling for
      assignment of probabilities to such relations as a function of multiple
      factors, where such probabilities can be viewed as estimates of whether
      humans would judge the relations to hold in given circumstances. We
      implemented our models in a 3D blocks world and a room world in a computer
      graphics setting, and found that true/false judgments based on these
      models do not differ much more from human judgments that the latter differ
      from one another. However, what really matters pragmatically is not the
      accuracy of truth value judgments but whether, for instance, the computer
      models suffice for identifying objects described in terms of prepositional
      relations, (e.g., "the box to the left of the table", where there are
      multiple boxes). For such tasks, our models achieved accuracies above 90%
      for most relations.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1403</url>
    <doi>10.18653/v1/W18-1403</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>platonov-schubert:2018:W18-14</bibkey>
  </paper>
  <paper id='1404'>
    <title>
      Lexical Conceptual Structure of Literal and Metaphorical Spatial Language:
      A Case Study of "Push"
    </title>
    <author>
      <first>Bonnie</first>
      <last>Dorr</last>
    </author>
    <author>
      <first>Mari</first>
      <last>Olsen</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–40</pages>
    <abstract>
      Prior methodologies for understanding spatial language have treated
      literal expressions such as "Mary pushed the car over the edge"
      differently from metaphorical extensions such as "Mary’s job pushed her
      over the edge". We demonstrate a methodology for standardizing literal and
      metaphorical meanings, by building on work in Lexical Conceptual Structure
      (LCS), a general-purpose representational component used in machine
      translation. We argue that spatial predicates naturally extend into other
      fields (e.g., circumstantial or temporal), and that LCS provides both a
      framework for distinguishing spatial from non-spatial, and a system for
      finding metaphorical meaning extensions. We start with MetaNet (MN), a
      large repository of conceptual metaphors, condensing 197 spatial entries
      into sixteen top-level categories of motion frames. Using naturally
      occurring instances of English push , and expansions of MN frames, we
      demonstrate that literal and metaphorical extensions exhibit patterns
      predicted and represented by the LCS model.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1404</url>
    <doi>10.18653/v1/W18-1404</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>dorr-olsen:2018:W18-14</bibkey>
  </paper>
  <paper id='1405'>
    <title>Representing Spatial Relations in FrameNet</title>
    <author>
      <first>Miriam R L</first>
      <last>Petruck</last>
    </author>
    <author>
      <first>Michael J</first>
      <last>Ellsworth</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–45</pages>
    <abstract>
      While humans use natural language to express spatial relations between and
      across entities in the world with great facility, natural language systems
      have a facility that depends on that human facility. This position paper
      presents approach to representing spatial relations in language, and
      advocates its adoption for representing the meaning of spatial language.
      This work shows the importance of axis-orientation systems for capturing
      the complexity of spatial relations, which FrameNet encodes with semantic
      types.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1405</url>
    <doi>10.18653/v1/W18-1405</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>petruck-ellsworth:2018:W18-14</bibkey>
  </paper>
  <paper id='1406'>
    <title>
      Points, Paths, and Playscapes: Large-scale Spatial Language Understanding
      Tasks Set in the Real World
    </title>
    <author>
      <first>Jason</first>
      <last>Baldridge</last>
    </author>
    <author>
      <first>Tania</first>
      <last>Bedrax-Weiss</last>
    </author>
    <author>
      <first>Daphne</first>
      <last>Luong</last>
    </author>
    <author>
      <first>Srini</first>
      <last>Narayanan</last>
    </author>
    <author>
      <first>Bo</first>
      <last>Pang</last>
    </author>
    <author>
      <first>Fernando</first>
      <last>Pereira</last>
    </author>
    <author>
      <first>Radu</first>
      <last>Soricut</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Tseng</last>
    </author>
    <author>
      <first>Yuan</first>
      <last>Zhang</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–52</pages>
    <abstract>
      Spatial language understanding is important for practical applications and
      as a building block for better abstract language understanding. Much
      progress has been made through work on understanding spatial relations and
      values in images and texts as well as on giving and following navigation
      instructions in restricted domains. We argue that the next big advances in
      spatial language understanding can be best supported by creating
      large-scale datasets that focus on points and paths based in the real
      world, and then extending these to create online, persistent playscapes
      that mix human and bot players, where the bot players must learn, evolve,
      and survive according to their depth of understanding of scenes,
      navigation, and interactions.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1406</url>
    <doi>10.18653/v1/W18-1406</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>baldridge-EtAl:2018:W18-14</bibkey>
  </paper>
  <paper id='1407'>
    <title>Anaphora Resolution for Improving Spatial Relation Extraction from Text</title>
    <author>
      <first>Umar</first>
      <last>Manzoor</last>
    </author>
    <author>
      <first>Parisa</first>
      <last>Kordjamshidi</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53–62</pages>
    <abstract>
      Spatial relation extraction from generic text is a challenging problem due
      to the ambiguity of the prepositions spatial meaning as well as the
      nesting structure of the spatial descriptions. In this work, we highlight
      the difficulties that the anaphora can make in the extraction of spatial
      relations. We use external multi-modal (here visual) resources to find the
      most probable candidates for resolving the anaphoras that refer to the
      landmarks of the spatial relations. We then use global inference to decide
      jointly on resolving the anaphora and extraction of the spatial relations.
      Our preliminary results show that resolving anaphora improves the
      state-of-the-art results on spatial relation extraction.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1407</url>
    <doi>10.18653/v1/W18-1407</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>manzoor-kordjamshidi:2018:W18-14</bibkey>
  </paper>
  <paper id='1408'>
    <title>The Case for Systematically Derived Spatial Language Usage</title>
    <author>
      <first>Bonnie</first>
      <last>Dorr</last>
    </author>
    <author>
      <first>Clare</first>
      <last>Voss</last>
    </author>
    <booktitle>
      Proceedings of the First International Workshop on Spatial Language
      Understanding
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>63–70</pages>
    <abstract>
      This position paper argues that, while prior work in spatial language
      understanding for tasks such as robot navigation focuses on mapping
      natural language into deep conceptual or non-linguistic representations,
      it is possible to systematically derive regular patterns of spatial
      language usage from existing lexical-semantic resources. Furthermore, even
      with access to such resources, effective solutions to many application
      areas such as robot navigation and narrative generation also require
      additional knowledge at the syntax-semantics interface to cover the wide
      range of spatial expressions observed and available to natural language
      speakers. We ground our insights in, and present our extensions to, an
      existing lexico-semantic resource, covering 500 semantic classes of verbs,
      of which 219 fall within a spatial subset. We demonstrate that these
      extensions enable systematic derivation of regular patterns of spatial
      language without requiring manual annotation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1408</url>
    <doi>10.18653/v1/W18-1408</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>dorr-voss:2018:W18-14</bibkey>
  </paper>
  <paper id='1500'>
    <title>Proceedings of the First Workshop on Storytelling</title>
    <editor>
      <first>Margaret</first>
      <last>Mitchell</last>
    </editor>
    <editor>
      <first>Ting-Hao ‘Kenneth’</first>
      <last>Huang</last>
    </editor>
    <editor>
      <first>Francis</first>
      <last>Ferraro</last>
    </editor>
    <editor>
      <first>Ishan</first>
      <last>Misra</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-15</url>
    <doi>10.18653/v1/W18-15</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-15:2018</bibkey>
  </paper>
  <paper id='1501'>
    <title>
      Learning to Listen: Critically Considering the Role of AI in Human
      Storytelling and Character Creation
    </title>
    <author>
      <first>Anna</first>
      <last>Kasunic</last>
    </author>
    <author>
      <first>Geoff</first>
      <last>Kaufman</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–13</pages>
    <abstract>
      In this opinion piece, we argue that there is a need for alternative
      design directions to complement existing AI efforts in narrative and
      character generation and algorithm development. To make our argument, we
      a) outline the predominant roles and goals of AI research in storytelling;
      b) present existing discourse on the benefits and harms of narratives; and
      c) highlight the pain points in character creation revealed by
      semi-structured interviews we conducted with 14 individuals deeply
      involved in some form of character creation. We conclude by proffering
      several specific design avenues that we believe can seed fruitful research
      collaborations. In our vision, AI collaborates with humans during creative
      processes and narrative generation, helps amplify voices and perspectives
      that are currently marginalized or misrepresented, and engenders
      experiences of narrative that support spectatorship and listening roles.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1501</url>
    <doi>10.18653/v1/W18-1501</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kasunic-kaufman:2018:W18-15</bibkey>
  </paper>
  <paper id='1502'>
    <title>Linguistic Features of Helpfulness in Automated Support for Creative Writing</title>
    <author>
      <first>Melissa</first>
      <last>Roemmele</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Gordon</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>14–19</pages>
    <abstract>
      We examine an emerging NLP application that supports creative writing by
      automatically suggesting continuing sentences in a story. The application
      tracks users’ modifications to generated sentences, which can be used to
      quantify their "helpfulness" in advancing the story. We explore the task
      of predicting helpfulness based on automatically detected linguistic
      features of the suggestions. We illustrate this analysis on a set of user
      interactions with the application using an initial selection of features
      relevant to story generation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1502</url>
    <doi>10.18653/v1/W18-1502</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>roemmele-gordon:2018:W18-151</bibkey>
  </paper>
  <paper id='1503'>
    <title>A Pipeline for Creative Visual Storytelling</title>
    <author>
      <first>Stephanie</first>
      <last>Lukin</last>
    </author>
    <author>
      <first>Reginald</first>
      <last>Hobbs</last>
    </author>
    <author>
      <first>Clare</first>
      <last>Voss</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20–32</pages>
    <abstract>
      Computational visual storytelling produces a textual description of events
      and interpretations depicted in a sequence of images. These texts are made
      possible by advances and cross-disciplinary approaches in natural language
      processing, generation, and computer vision. We define a computational
      creative visual storytelling as one with the ability to alter the telling
      of a story along three aspects: to speak about different environments, to
      produce variations based on narrative goals, and to adapt the narrative to
      the audience. These aspects of creative storytelling and their effect on
      the narrative have yet to be explored in visual storytelling. This paper
      presents a pipeline of task-modules, Object Identification, Single-Image
      Inferencing, and Multi-Image Narration, that serve as a preliminary design
      for building a creative visual storyteller. We have piloted this design
      for a sequence of images in an annotation task. We present and analyze the
      collected corpus and describe plans towards automation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1503</url>
    <doi>10.18653/v1/W18-1503</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>lukin-hobbs-voss:2018:W18-15</bibkey>
  </paper>
  <paper id='1504'>
    <title>Telling Stories with Soundtracks: An Empirical Analysis of Music in Film</title>
    <author>
      <first>Jon</first>
      <last>Gillick</last>
    </author>
    <author>
      <first>David</first>
      <last>Bamman</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33–42</pages>
    <abstract>
      Soundtracks play an important role in carrying the story of a film. In
      this work, we collect a corpus of movies and television shows matched with
      subtitles and soundtracks and analyze the relationship between story,
      song, and audience reception. We look at the content of a film through the
      lens of its latent topics and at the content of a song through descriptors
      of its musical attributes. In two experiments, we find first that
      individual topics are strongly associated with musical attributes, and
      second, that musical attributes of soundtracks are predictive of film
      ratings, even after controlling for topic and genre.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1504</url>
    <doi>10.18653/v1/W18-1504</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gillick-bamman:2018:W18-15</bibkey>
  </paper>
  <paper id='1505'>
    <title>Towards Controllable Story Generation</title>
    <author>
      <first>Nanyun</first>
      <last>Peng</last>
    </author>
    <author>
      <first>Marjan</first>
      <last>Ghazvininejad</last>
    </author>
    <author>
      <first>Jonathan</first>
      <last>May</last>
    </author>
    <author>
      <first>Kevin</first>
      <last>Knight</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–49</pages>
    <abstract>
      We present a general framework of analyzing existing story corpora to
      generate controllable and creative new stories. The proposed framework
      needs little manual annotation to achieve controllable story generation.
      It creates a new interface for humans to interact with computers to
      generate personalized stories. We apply the framework to build recurrent
      neural network (RNN)-based generation models to control story ending
      valence and storyline. Experiments show that our methods successfully
      achieve the control and enhance the coherence of stories through
      introducing storylines. with additional control factors, the generation
      model gets lower perplexity, and yields more coherent stories that are
      faithful to the control factors according to human evaluation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1505</url>
    <doi>10.18653/v1/W18-1505</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>peng-EtAl:2018:W18-15</bibkey>
  </paper>
  <paper id='1506'>
    <title>An Encoder-decoder Approach to Predicting Causal Relations in Stories</title>
    <author>
      <first>Melissa</first>
      <last>Roemmele</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Gordon</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>50–59</pages>
    <abstract>
      We address the task of predicting causally related events in stories
      according to a standard evaluation framework, the Choice of Plausible
      Alternatives (COPA). We present a neural encoder-decoder model that learns
      to predict relations between adjacent sequences in stories as a means of
      modeling causality. We explore this approach using different methods for
      extracting and representing sequence pairs as well as different model
      architectures. We also compare the impact of different training datasets
      on our model. In particular, we demonstrate the usefulness of a corpus not
      previously applied to COPA, the ROCStories corpus. While not
      state-of-the-art, our results establish a new reference point for systems
      evaluated on COPA, and one that is particularly informative for future
      neural-based approaches.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1506</url>
    <doi>10.18653/v1/W18-1506</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>roemmele-gordon:2018:W18-152</bibkey>
  </paper>
  <paper id='1507'>
    <title>Neural Event Extraction from Movies Description</title>
    <author>
      <first>Alex</first>
      <last>Tozzo</last>
    </author>
    <author>
      <first>Dejan</first>
      <last>Jovanovic</last>
    </author>
    <author>
      <first>Mohamed</first>
      <last>Amer</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60–66</pages>
    <abstract>
      We present a novel approach for event extraction and abstraction from
      movie descriptions. Our event frame consists of ‘who”, “did what” “to
      whom”, “where”, and “when”. We formulate our problem using a recurrent
      neural network, enhanced with structural features extracted from syntactic
      parser, and trained using curriculum learning by progressively increasing
      the difficulty of the sentences. Our model serves as an intermediate step
      towards question answering systems, visual storytelling, and story
      completion tasks. We evaluate our approach on MovieQA dataset.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1507</url>
    <doi>10.18653/v1/W18-1507</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>tozzo-jovanovic-amer:2018:W18-15</bibkey>
  </paper>
  <paper id='1600'>
    <title>Proceedings of the Second Workshop on Stylistic Variation</title>
    <editor>
      <first>Julian</first>
      <last>Brooke</last>
    </editor>
    <editor>
      <first>Lucie</first>
      <last>Flekova</last>
    </editor>
    <editor>
      <first>Moshe</first>
      <last>Koppel</last>
    </editor>
    <editor>
      <first>Thamar</first>
      <last>Solorio</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-16</url>
    <doi>10.18653/v1/W18-16</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-16:2018</bibkey>
  </paper>
  <paper id='1601'>
    <title>
      Stylistic variation over 200 years of court proceedings according to
      gender and social class
    </title>
    <author>
      <first>Stefania</first>
      <last>Degaetano-Ortlieb</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <abstract>
      We present an approach to detect stylistic variation across social
      variables (here: gender and social class), considering also diachronic
      change in language use. For detection of stylistic variation, we use
      relative entropy, measuring the difference between probability
      distributions at different linguistic levels (here: lexis and grammar). In
      addition, by relative entropy, we can determine which linguistic units are
      related to stylistic variation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1601</url>
    <doi>10.18653/v1/W18-1601</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>degaetanoortlieb:2018:W18-16</bibkey>
  </paper>
  <paper id='1602'>
    <title>Stylistic Variation in Social Media Part-of-Speech Tagging</title>
    <author>
      <first>Murali Raghu Babu</first>
      <last>Balusu</last>
    </author>
    <author>
      <first>Taha</first>
      <last>Merghani</last>
    </author>
    <author>
      <first>Jacob</first>
      <last>Eisenstein</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–19</pages>
    <abstract>
      Social media features substantial stylistic variation, raising new
      challenges for syntactic analysis of online writing. However, this
      variation is often aligned with author attributes such as age, gender, and
      geography, as well as more readily-available social network metadata. In
      this paper, we report new evidence on the link between language and social
      networks in the task of part-of-speech tagging. We find that tagger error
      rates are correlated with network structure, with high accuracy in some
      parts of the network, and lower accuracy elsewhere. As a result, tagger
      accuracy depends on training from a balanced sample of the network, rather
      than training on texts from a narrow subcommunity. We also describe our
      attempts to add robustness to stylistic variation, by building a
      mixture-of-experts model in which each expert is associated with a region
      of the social network. While prior work found that similar approaches
      yield performance improvements in sentiment analysis and entity linking,
      we were unable to obtain performance improvements in part-of-speech
      tagging, despite strong evidence for the link between part-of-speech error
      rates and social network structure.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1602</url>
    <doi>10.18653/v1/W18-1602</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>balusu-merghani-eisenstein:2018:W18-16</bibkey>
  </paper>
  <paper id='1603'>
    <title>Detecting Syntactic Features of Translated Chinese</title>
    <author>
      <first>Hai</first>
      <last>Hu</last>
    </author>
    <author>
      <first>Wen</first>
      <last>Li</last>
    </author>
    <author>
      <first>Sandra</first>
      <last>Kübler</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20–28</pages>
    <abstract>
      We present a machine learning approach to distinguish texts translated to
      Chinese (by humans) from texts originally written in Chinese, with a focus
      on a wide range of syntactic features. Using Support Vector Machines
      (SVMs) as classifier on a genre-balanced corpus in translation studies of
      Chinese, we find that constituent parse trees and dependency triples as
      features without lexical information perform very well on the task, with
      an F-measure above 90%, close to the results of lexical n-gram features,
      without the risk of learning topic information rather than translation
      features. Thus, we claim syntactic features alone can accurately
      distinguish translated from original Chinese. Translated Chinese exhibits
      an increased use of determiners, subject position pronouns, NP + "的" as NP
      modifiers, multiple NPs or VPs conjoined by "、", among other structures.
      We also interpret the syntactic features with reference to previous
      translation studies in Chinese, particularly the usage of pronouns.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1603</url>
    <doi>10.18653/v1/W18-1603</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>hu-li-kbler:2018:W18-16</bibkey>
  </paper>
  <paper id='1604'>
    <title>Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting</title>
    <author>
      <first>Peter</first>
      <last>Potash</last>
    </author>
    <author>
      <first>Alexey</first>
      <last>Romanov</last>
    </author>
    <author>
      <first>Anna</first>
      <last>Rumshisky</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29–38</pages>
    <abstract>
      Language generation tasks that seek to mimic human ability to use language
      creatively are difficult to evaluate, since one must consider creativity,
      style, and other non-trivial aspects of the generated text. The goal of
      this paper is to develop evaluations methods for one such task,
      ghostwriting of rap lyrics, and to provide an explicit, quantifiable
      foundation for the goals and future directions for this task. Ghostwriting
      must produce text that is similar in style to the emulated artist, yet
      distinct in content. We develop a novel evaluation methodology that
      addresses several complementary aspects of this task, and illustrate how
      such evaluation can be used to meaning fully analyze system performance.
      We provide a corpus of lyrics for 13 rap artists, annotated for stylistic
      similarity, which allows us to assess the feasibility of manual evaluation
      for generated verse.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1604</url>
    <doi>10.18653/v1/W18-1604</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>potash-romanov-rumshisky:2018:W18-16</bibkey>
  </paper>
  <paper id='1605'>
    <title>Cross-corpus Native Language Identification via Statistical Embedding</title>
    <author>
      <first>Francisco</first>
      <last>Rangel</last>
    </author>
    <author>
      <first>Paolo</first>
      <last>Rosso</last>
    </author>
    <author>
      <first>Julian</first>
      <last>Brooke</last>
    </author>
    <author>
      <first>Alexandra</first>
      <last>Uitdenbogerd</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–43</pages>
    <abstract>
      In this paper, we approach the task of native language identification in a
      realistic cross-corpus scenario where a model is trained with available
      data and has to predict the native language from data of a different
      corpus. The motivation behind this study is to investigate native language
      identification in the Australian academic scenario where a majority of
      students come from China, Indonesia, and Arabic-speaking nations. We have
      proposed a statistical embedding representation reporting a significant
      improvement over common single-layer approaches of the state of the art,
      identifying Chinese, Arabic, and Indonesian in a cross-corpus scenario.
      The proposed approach was shown to be competitive even when the data is
      scarce and imbalanced.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1605</url>
    <doi>10.18653/v1/W18-1605</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rangel-EtAl:2018:W18-16</bibkey>
  </paper>
  <paper id='1700'>
    <title>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </title>
    <editor>
      <first>Goran</first>
      <last>Glavaš</last>
    </editor>
    <editor>
      <first>Swapna</first>
      <last>Somasundaran</last>
    </editor>
    <editor>
      <first>Martin</first>
      <last>Riedl</last>
    </editor>
    <editor>
      <first>Eduard</first>
      <last>Hovy</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-17</url>
    <doi>10.18653/v1/W18-17</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-17:2018</bibkey>
  </paper>
  <paper id='1701'>
    <title>Scientific Discovery as Link Prediction in Influence and Citation Graphs</title>
    <author>
      <first>Fan</first>
      <last>Luo</last>
    </author>
    <author>
      <first>Marco A.</first>
      <last>Valenzuela-Escárcega</last>
    </author>
    <author>
      <first>Gus</first>
      <last>Hahn-Powell</last>
    </author>
    <author>
      <first>Mihai</first>
      <last>Surdeanu</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–6</pages>
    <abstract>
      We introduce a machine learning approach for the identification of “white
      spaces” in scientific knowledge. Our approach addresses this task as link
      prediction over a graph that contains over 2M influence statements such as
      “CTCF activates FOXA1”, which were automatically extracted using
      open-domain machine reading. We model this prediction task using
      graph-based features extracted from the above influence graph, as well as
      from a citation graph that captures scientific communities. We evaluated
      the proposed approach through backtesting. Although the data is heavily
      unbalanced (50 times more negative examples than positives), our approach
      predicts which influence links will be discovered in the “near future”
      with a F1 score of 27 points, and a mean average precision of 68%.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1701</url>
    <doi>10.18653/v1/W18-1701</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>luo-EtAl:2018:W18-17</bibkey>
  </paper>
  <paper id='1702'>
    <title>
      Efficient Generation and Processing of Word Co-occurrence Networks Using
      corpus2graph
    </title>
    <author>
      <first>Zheng</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Pierre</first>
      <last>Zweigenbaum</last>
    </author>
    <author>
      <first>Ruiqing</first>
      <last>Yin</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>7–11</pages>
    <abstract>
      Corpus2graph is an open-source NLP-application-oriented tool that
      generates a word co-occurrence network from a large corpus. It not only
      contains different built-in methods to preprocess words, analyze
      sentences, extract word pairs and define edge weights, but also supports
      user-customized functions. By using parallelization techniques, it can
      generate a large word co-occurrence network of the whole English Wikipedia
      data within hours. And thanks to its nodes-edges-weight three-level
      progressive calculation design, rebuilding networks with different
      configurations is even faster as it does not need to start all over again.
      This tool also works with other graph libraries such as igraph, NetworkX
      and graph-tool as a front end providing data to boost network generation
      speed.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1702</url>
    <doi>10.18653/v1/W18-1702</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-zweigenbaum-yin:2018:W18-17</bibkey>
  </paper>
  <paper id='1703'>
    <title>
      Multi-hop Inference for Sentence-level TextGraphs: How Challenging is
      Meaningfully Combining Information for Science Question Answering?
    </title>
    <author>
      <first>Peter</first>
      <last>Jansen</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–17</pages>
    <abstract>
      Question Answering for complex questions is often modelled as a graph
      construction or traversal task, where a solver must build or traverse a
      graph of facts that answer and explain a given question. This "multi-hop"
      inference has been shown to be extremely challenging, with few models able
      to aggregate more than two facts before being overwhelmed by "semantic
      drift", or the tendency for long chains of facts to quickly drift off
      topic. This is a major barrier to current inference models, as even
      elementary science questions require an average of 4 to 6 facts to answer
      and explain. In this work we empirically characterize the difficulty of
      building or traversing a graph of sentences connected by lexical overlap,
      by evaluating chance sentence aggregation quality through 9,784
      manually-annotated judgements across knowledge graphs built from three
      free-text corpora (including study guides and Simple Wikipedia). We
      demonstrate semantic drift tends to be high and aggregation quality low,
      at between 0.04 and 3, and highlight scenarios that maximize the
      likelihood of meaningfully combining information.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1703</url>
    <doi>10.18653/v1/W18-1703</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jansen:2018:W18-17</bibkey>
  </paper>
  <paper id='1704'>
    <title>
      Multi-Sentence Compression with Word Vertex-Labeled Graphs and Integer
      Linear Programming
    </title>
    <author>
      <first>Elvys</first>
      <last>Linhares Pontes</last>
    </author>
    <author>
      <first>Stéphane</first>
      <last>Huet</last>
    </author>
    <author>
      <first>Thiago</first>
      <last>Gouveia da Silva</last>
    </author>
    <author>
      <first>Andréa carneiro</first>
      <last>Linhares</last>
    </author>
    <author>
      <first>Juan-Manuel</first>
      <last>Torres-Moreno</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18–27</pages>
    <abstract>
      Multi-Sentence Compression (MSC) aims to generate a short sentence with
      key information from a cluster of closely related sentences. MSC enables
      summarization and question-answering systems to generate outputs combining
      fully formed sentences from one or several documents. This paper describes
      a new Integer Linear Programming method for MSC using a vertex-labeled
      graph to select different keywords, and novel 3-gram scores to generate
      more informative sentences while maintaining their grammaticality. Our
      system is of good quality and outperforms the state-of-the-art for
      evaluations led on news dataset. We led both automatic and manual
      evaluations to determine the informativeness and the grammaticality of
      compressions for each dataset. Additional tests, which take advantage of
      the fact that the length of compressions can be modulated, still improve
      ROUGE scores with shorter output sentences. Author4Affiliation
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1704</url>
    <doi>10.18653/v1/W18-1704</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>linharespontes-EtAl:2018:W18-17</bibkey>
  </paper>
  <paper id='1705'>
    <title>
      Large-scale spectral clustering using diffusion coordinates on
      landmark-based bipartite graphs
    </title>
    <author>
      <first>Khiem</first>
      <last>Pham</last>
    </author>
    <author>
      <first>Guangliang</first>
      <last>Chen</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>28–37</pages>
    <abstract>
      Spectral clustering has received a lot of attention due to its ability to
      separate nonconvex, non-intersecting manifolds, but its high computational
      complexity has significantly limited its applicability. Motivated by the
      document-term co-clustering framework by Dhillon (2001), we propose a
      landmark-based scalable spectral clustering approach in which we first use
      the selected landmark set and the given data to form a bipartite graph and
      then run a diffusion process on it to obtain a family of diffusion
      coordinates for clustering. We show that our proposed algorithm can be
      implemented based on very efficient operations on the affinity matrix
      between the given data and selected landmarks, thus capable of handling
      large data. Finally, we demonstrate the excellent performance of our
      method by comparing with the state-of-the-art scalable algorithms on
      several benchmark data sets.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1705</url>
    <doi>10.18653/v1/W18-1705</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>pham-chen:2018:W18-17</bibkey>
  </paper>
  <paper id='1706'>
    <title>
      Efficient Graph-based Word Sense Induction by Distributional Inclusion
      Vector Embeddings
    </title>
    <author>
      <first>Haw-Shiuan</first>
      <last>Chang</last>
    </author>
    <author>
      <first>Amol</first>
      <last>Agrawal</last>
    </author>
    <author>
      <first>Ananya</first>
      <last>Ganesh</last>
    </author>
    <author>
      <first>Anirudha</first>
      <last>Desai</last>
    </author>
    <author>
      <first>Vinayak</first>
      <last>Mathur</last>
    </author>
    <author>
      <first>Alfred</first>
      <last>Hough</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>McCallum</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38–48</pages>
    <abstract>
      Word sense induction (WSI), which addresses polysemy by unsupervised
      discovery of multiple word senses, resolves ambiguities for downstream NLP
      tasks and also makes word representations more interpretable. This paper
      proposes an accurate and efficient graph-based method for WSI that builds
      a global non-negative vector embedding basis (which are interpretable like
      topics) and clusters the basis indexes in the ego network of each
      polysemous word. By adopting distributional inclusion vector embeddings as
      our basis formation model, we avoid the expensive step of nearest neighbor
      search that plagues other graph-based methods without sacrificing the
      quality of sense clusters. Experiments on three datasets show that our
      proposed method produces similar or better sense clusters and embeddings
      compared with previous state-of-the-art methods while being significantly
      more efficient.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1706</url>
    <doi>10.18653/v1/W18-1706</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chang-EtAl:2018:W18-17</bibkey>
  </paper>
  <paper id='1707'>
    <title>
      Fusing Document, Collection and Label Graph-based Representations with
      Word Embeddings for Text Classification
    </title>
    <author>
      <first>Konstantinos</first>
      <last>Skianis</last>
    </author>
    <author>
      <first>Fragkiskos</first>
      <last>Malliaros</last>
    </author>
    <author>
      <first>Michalis</first>
      <last>Vazirgiannis</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>49–58</pages>
    <abstract>
      Contrary to the traditional Bag-of-Words approach, we consider the
      Graph-of-Words(GoW) model in which each document is represented by a graph
      that encodes relationships between the different terms. Based on this
      formulation, the importance of a term is determined by weighting the
      corresponding node in the document, collection and label graphs, using
      node centrality criteria. We also introduce novel graph-based weighting
      schemes by enriching graphs with word-embedding similarities, in order to
      reward or penalize semantic relationships. Our methods produce more
      discriminative feature weights for text categorization, outperforming
      existing frequency-based criteria.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1707</url>
    <doi>10.18653/v1/W18-1707</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>skianis-malliaros-vazirgiannis:2018:W18-17</bibkey>
  </paper>
  <paper id='1708'>
    <title>Embedding Text in Hyperbolic Spaces</title>
    <author>
      <first>Bhuwan</first>
      <last>Dhingra</last>
    </author>
    <author>
      <first>Christopher</first>
      <last>Shallue</last>
    </author>
    <author>
      <first>Mohammad</first>
      <last>Norouzi</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Dai</last>
    </author>
    <author>
      <first>George</first>
      <last>Dahl</last>
    </author>
    <booktitle>
      Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
      Language Processing (TextGraphs-12)
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana, USA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–69</pages>
    <abstract>
      Natural language text exhibits hierarchical structure in a variety of
      respects. Ideally, we could incorporate our prior knowledge of this
      hierarchical structure into unsupervised learning algorithms that work on
      text data. Recent work by nickel2017poincar proposed using hyperbolic
      instead of Euclidean embedding spaces to represent hierarchical data and
      demonstrated encouraging results when embedding graphs. In this work, we
      extend their method with a re-parameterization technique that allows us to
      learn hyperbolic embeddings of arbitrarily parameterized objects. We apply
      this framework to learn word and sentence embeddings in hyperbolic space
      in an unsupervised manner from text corpora. The resulting embeddings seem
      to encode certain intuitive notions of hierarchy, such as word-context
      frequency and phrase constituency. However, the implicit continuous
      hierarchy in the learned hyperbolic space makes interrogating the model’s
      learned hierarchies more difficult than for models that learn explicit
      edges between items. The learned hyperbolic embeddings show improvements
      over Euclidean embeddings in some – but not all – downstream tasks,
      suggesting that hierarchical organization is more useful for some tasks
      than others.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-1708</url>
    <doi>10.18653/v1/W18-1708</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>dhingra-EtAl:2018:W18-17</bibkey>
  </paper>
</volume>